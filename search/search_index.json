{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Stash Sense","text":"<p>AI-powered performer identification and library curation for Stash.</p>"},{"location":"#what-it-does","title":"What it does","text":"<ul> <li>Identifies performers in scenes using face recognition against a database of 108,000+ performers from StashDB, FansDB, ThePornDB, PMVStash, and JAVStash</li> <li>Detects duplicate scenes using face fingerprints, stash-box IDs, and metadata overlap \u2014 catches duplicates that phash matching misses</li> <li>Syncs upstream changes from stash-box endpoints with per-field merge controls</li> <li>Runs locally on your GPU \u2014 no cloud dependencies, no data leaves your network</li> </ul>"},{"location":"#how-it-works","title":"How it works","text":"<ol> <li>You click Identify Performers on a scene in Stash</li> <li>Stash Sense extracts frames from the scene's sprite sheet</li> <li>Faces are detected, aligned, and matched against the performer database</li> <li>Results show matched performers with confidence scores, grouped by person</li> <li>One click to add identified performers to the scene</li> </ol>"},{"location":"#get-started","title":"Get Started","text":"<ul> <li>Installation \u2014 Docker setup, database download, plugin install</li> <li>Plugin Usage \u2014 identifying performers, understanding results</li> <li>Features \u2014 full feature overview</li> <li>Database &amp; Updates \u2014 where the data comes from, how to update</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"Component Requirement Stash v0.25+ with sprite sheets generated Docker With <code>nvidia-container-toolkit</code> (for GPU) GPU NVIDIA with 4GB+ VRAM (optional \u2014 CPU fallback available) Disk ~1.5 GB for the face recognition database"},{"location":"architecture/","title":"Stash Sense Architecture","text":"<p>Reference for how the system works and why key decisions were made.</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<p>Stash Sense is a two-component system: a sidecar API (Python/FastAPI) that runs face recognition and analysis, and a plugin (JS/CSS/Python) injected into the Stash web UI.</p> <p>Deployment model: The sidecar runs as a Docker container alongside Stash (default port 6960). The plugin backend proxies all requests from Stash to the sidecar, bypassing browser CSP restrictions. The sidecar URL is configurable in Stash plugin settings.</p> <p>Two-database design: - <code>performers.db</code> \u2014 Read-only, distributable. Contains performer metadata, face references, and stash-box IDs sourced from the private trainer repo. Updated via GitHub Releases. - <code>stash_sense.db</code> \u2014 Read-write, user-local. Contains recommendations, dismissed items, analysis watermarks, upstream snapshots, and scene fingerprints. Persists across face DB updates.</p> <p>This separation means face DB updates never touch user state, and the distributable database contains no user-specific data.</p> <p>Docker image: Two-stage build on <code>nvidia/cuda:12.4.0-runtime-ubuntu22.04</code>. ONNX models (FaceNet512 + ArcFace, ~220MB) are baked into the image. Data files (Voyager indices ~1.1GB, performers.db ~210MB) are volume-mounted from NVMe for independent updates. PyTorch, TensorFlow, and DeepFace are stripped \u2014 only needed by the trainer's offline scripts.</p>"},{"location":"architecture/#face-recognition","title":"Face Recognition","text":"<p>Models: RetinaFace (buffalo_sc, ONNX GPU) for detection, FaceNet512 + ArcFace for embeddings. Both embedding models use flip-averaging (original + horizontally flipped face, averaged) for more stable representations.</p> <p>Pipeline (3-phase batch): 1. Extract \u2014 ffmpeg seeks N frames from scene, 8 concurrent workers 2. Detect \u2014 RetinaFace processes all frames, InsightFace <code>norm_crop</code> aligns faces via 5-point similarity transform 3. Embed + Match \u2014 All faces batched through both ONNX models in 2 calls (1 per model), then searched against Voyager indices (cosine space)</p> <p>This replaced a per-face sequential pipeline: 53.6s \u2192 4.8s on 1080p/60 frames/64 faces (11x speedup, embedding step 138x faster via GPU batching).</p> <p>Matching: <code>clustered_frequency_matching</code> is the default. Faces are clustered by person using cosine distance (threshold 0.6) on concatenated FaceNet+ArcFace embeddings, then frequency-matched within each cluster. Multi-frame appearances boost confidence. Tagged-performer boost (+0.03) applied when scene already has performers tagged.</p> <p>Tuned defaults (from 20-scene stratified benchmark): fusion weights 0.5/0.5 FaceNet/ArcFace, max_distance 0.5, num_frames 60, min_unique_frames 2. Baseline: ~42% scene accuracy, ~33% precision. Biggest improvement levers are num_frames and database coverage.</p>"},{"location":"architecture/#recommendations-engine","title":"Recommendations Engine","text":"<p>BaseAnalyzer pattern: Each recommendation type (duplicate scenes, missing stash-box links, upstream changes, etc.) is a pluggable analyzer with a consistent interface for running, progress tracking, and creating recommendations. The scheduler runs analyzers as background jobs.</p> <p>Incremental watermarking: Most analyzers track a <code>last_watermark</code> timestamp or cursor and only process items modified since the last run. This keeps incremental runs fast (~seconds) versus full scans (~73 min for 7K+ performers at 5 req/s rate limit).</p> <p>Polymorphic recommendations: A single <code>recommendations</code> table handles all entity types (duplicate_performer, duplicate_scene, unidentified_scene, missing_stashbox_link, stashbox_updates) with type-specific JSON payloads. Users review via a dashboard or contextual actions on affected pages.</p>"},{"location":"architecture/#duplicate-scene-detection","title":"Duplicate Scene Detection","text":"<p>Stash's built-in phash matching fails with different intros/outros, trimming, aspect ratio changes, or watermarks. Stash Sense uses multi-signal detection with a confidence hierarchy.</p> <p>Signal hierarchy with caps: - Stash-box ID match = 100% (authoritative) - Face fingerprint similarity = up to 85% - Metadata overlap = up to 60% - No single signal other than stash-box ID reaches 100%</p> <p>Face fingerprints: Per-scene records of which performers appeared, how many times detected, and proportion of total faces. Same performers appearing in the same ratios is robust to trimming and length differences.</p> <p>Diminishing returns scoring: <code>primary + secondary \u00d7 0.3</code> \u2014 the second signal adds less than the first, preventing false confidence inflation.</p> <p>Scalable candidate generation: Direct O(n\u00b2) comparison crashes on 15K+ scenes. Instead, a two-phase approach: 1. Candidate generation \u2014 SQL joins and inverted indices produce O(n) candidate pairs from 3 sources: stash-box ID grouping, face fingerprint self-join (shared performers), and metadata intersection (same studio AND performer). Yields 10K-50K candidates vs 112M brute-force comparisons. 2. Sequential scoring \u2014 Cursor-based pagination (<code>id &gt; last_id LIMIT 100</code>) iterates candidates, scores each, writes recommendations immediately. Constant memory throughout.</p>"},{"location":"architecture/#multi-signal-identification","title":"Multi-Signal Identification","text":"<p>Face recognition alone achieves ~50-60% accuracy. Additional biometric signals improve identification when faces are unclear or absent.</p> <p>Late fusion architecture: Each signal is searched independently; results are combined via multiplicative scoring. Missing signals are handled gracefully (no tattoo visible \u2192 neutral multiplier).</p> <p>Signals: - Face (primary) \u2014 Voyager index search, top-K candidates - Body proportions \u2014 MediaPipe pose estimation extracts shoulder-hip ratio, leg-torso ratio, arm-span-height ratio. Compared against database values with tolerance ~0.15. Returns penalty multiplier (1.0 compatible, 0.3 severe mismatch). - Tattoo presence \u2014 YOLO-based detection. Query shows tattoos but candidate has none \u2192 0.7x penalty. Matching tattoo locations \u2192 1.15x boost.</p> <p>Fusion: <code>final_score = face_score \u00d7 body_multiplier \u00d7 tattoo_multiplier</code></p> <p>Tattoo embedding matching (semantic similarity of tattoo designs via EfficientNet-B0 1280-dim vectors) exists in the trainer but is not yet deployed to the sidecar.</p>"},{"location":"architecture/#upstream-performer-sync","title":"Upstream Performer Sync","text":"<p>Detects stash-box field changes and presents per-field merge controls to sync local Stash against upstream updates.</p> <p>3-way diff engine: Compares upstream (current stash-box state) vs local (current Stash state) vs snapshot (last-seen upstream state stored in <code>upstream_snapshots</code> table). This distinguishes intentional local differences from actual upstream changes.</p> <p>Merge controls: Name fields get 5 options (keep/accept/demote-to-alias/add-as-alias/custom). Aliases use union checkboxes. Simple fields use 3-option radio (keep/accept/custom). Per-field monitoring can be disabled per endpoint via <code>upstream_field_config</code>.</p> <p>Field mapping complexity: Stash-box uses separate fields (cup_size, band_size, waist_size, hip_size, career_start_year, career_end_year) that Stash combines into compound strings (measurements \"38F-24-35\", career_length \"2015-2023\"). Translation handled in <code>recommendations_router.py:update_performer_fields()</code>.</p> <p>Dismissal model: Soft dismiss (permanent=0) resurfaces if upstream changes again. Permanent dismiss skips entirely.</p>"},{"location":"architecture/#upstream-studio-tag-sync","title":"Upstream Studio &amp; Tag Sync","text":"<p>Both follow the same <code>BaseUpstreamAnalyzer</code> pattern as performer sync: 3-way diff with stored snapshots, incremental watermarking, and per-field enable/disable via <code>upstream_field_config</code>.</p> <p>Studios: Compared fields are name, URL, and parent studio. Parent studio comparison resolves the parent's stash-box ID for the current endpoint rather than comparing local numeric IDs, which would always differ from upstream UUIDs. Falls back to local ID only when the parent isn't linked to the endpoint.</p> <p>Tags: Compared fields are name, description, and aliases. Tag comparison is set-based (aliases are compared as unordered sets, not ordered lists).</p>"},{"location":"architecture/#upstream-scene-sync","title":"Upstream Scene Sync","text":"<p>Detects changes in scene metadata, studio assignment, performer lineup, and tags between local Stash and stash-box endpoints. Uses the same <code>BaseUpstreamAnalyzer</code> infrastructure but with relational diffing rather than simple field comparison.</p> <p>Relational diffing: Scene changes go beyond flat field comparison. Performer changes detect added/removed performers with alias detection (performer \"as\" credits). Tag changes are set-based (added/removed). Studio changes resolve stash-box IDs for comparison, same as the studio analyzer. Simple fields (title, date, URL, details) use standard 3-way diff.</p> <p>Change detection: A scene is flagged only when at least one category has actual differences \u2014 simple field changes, a studio change, performer additions/removals/alias changes, or tag additions/removals. All entity IDs (performers, studios, tags) are resolved to stash-box IDs scoped to the current endpoint to avoid cross-endpoint mismatches.</p>"},{"location":"architecture/#logic-versioning","title":"Logic Versioning","text":"<p>Each upstream analyzer has a <code>logic_version</code> class attribute (e.g., performer analyzer v2 removed the favorite field from comparison, studio analyzer v2 switched to stash-box IDs for parent studio). The version is stored in the <code>analysis_watermarks.logic_version</code> column.</p> <p>When the version changes, the next analysis run detects the mismatch and clears all stale snapshots and watermarks for that entity type, forcing a full re-analysis. Recommendations generated under old logic that no longer produce differences are automatically resolved. This acts as a migration mechanism \u2014 when comparison logic changes (new fields, different normalization, ID resolution fixes), bumping <code>logic_version</code> ensures all entities are re-evaluated cleanly without manual intervention.</p>"},{"location":"architecture/#gallery-image-identification","title":"Gallery &amp; Image Identification","text":"<p>Extends face recognition from scenes (frame extraction) to gallery images (typically higher quality, better-framed).</p> <p>Endpoints: <code>/identify/image</code> (single image) and <code>/identify/gallery</code> (all images in a gallery).</p> <p>Aggregation: Unlike scenes, gallery images are independent \u2014 no temporal clustering needed. Results are grouped by performer across images: best distance, average distance, image count. Filtering: 2+ appearances OR single match with distance &lt; 0.4.</p> <p>Fingerprint caching: <code>image_fingerprints</code> and <code>image_fingerprint_faces</code> tables store per-image results, avoiding re-processing on subsequent requests.</p>"},{"location":"architecture/#database-self-update","title":"Database Self-Update","text":"<p>The sidecar checks for new face recognition database releases on GitHub and performs hot-swap updates without container restart.</p> <p>Pipeline: Download \u2192 extract \u2192 verify checksums against <code>manifest.json</code> \u2192 swap \u2192 reload global state. Files are staged in <code>/data/staging/</code>, old files backed up to <code>/data/backup/</code>.</p> <p>Availability during update: Brief 503 responses (5-10s) on <code>/identify/*</code> endpoints only. An <code>update_in_progress</code> flag gates these endpoints. Non-identification endpoints remain available.</p> <p>Reload order: New DatabaseConfig \u2192 new FaceRecognizer (loads Voyager indices) \u2192 load manifest \u2192 rebuild MultiSignalMatcher \u2192 replace global references \u2192 clear flag.</p> <p>Safety: Disk space check (2.5x current DB size) before download. Rollback from backup if reload fails. <code>stash_sense.db</code> (recommendations) is never touched during updates.</p>"},{"location":"architecture/#operation-queue","title":"Operation Queue","text":"<p>Persistent job queue backed by SQLite (<code>job_queue</code> and <code>job_schedules</code> tables) that manages all background analysis and maintenance work.</p> <p>Job lifecycle: PENDING -&gt; QUEUED -&gt; RUNNING -&gt; COMPLETED/FAILED/CANCELLED. Jobs are submitted with a type, priority, and optional cursor. The <code>QueueManager</code> singleton orchestrates dispatch.</p> <p>Resource-aware scheduling: Each job type declares a resource requirement. Slots limit concurrency per resource type: GPU (1), CPU_HEAVY (1), NETWORK (2), LIGHT (3). A job is dispatched only when a slot is available for its resource type, preventing GPU contention or network flooding.</p> <p>Priority levels: CRITICAL (0), HIGH (10), NORMAL (50), LOW (100). Lower values run first. Most analyzers default to NORMAL; database self-update uses HIGH; StashBox queries use LOW.</p> <p>Incremental job support: Jobs can persist a cursor (e.g., last-processed ID or timestamp) so they resume from where they left off on subsequent runs. Combined with the watermark system, this keeps incremental analysis fast.</p> <p>Schedules: <code>job_schedules</code> table stores cron-like schedules per job type, allowing automatic periodic runs without external schedulers.</p>"},{"location":"architecture/#resource-management","title":"Resource Management","text":"<p><code>ResourceManager</code> handles lazy loading and idle unloading of heavy resources to free GPU memory and RAM when not in use.</p> <p>Resources managed: Face recognition data (Voyager indices + metadata), tattoo detection models, body proportion indices. Each is registered with a loader and unloader function.</p> <p>Lifecycle: Resources are loaded on first <code>require()</code> call and cached. A background task periodically calls <code>check_idle()</code>. Resources unused for 1800 seconds (30 minutes) are unloaded \u2014 their unloader runs, data references are cleared, and <code>gc.collect()</code> reclaims memory. Thread-safe via internal locking.</p>"},{"location":"architecture/#model-management","title":"Model Management","text":"<p>Optional ONNX models (tattoo detection) are not baked into the Docker image. They are downloaded on-demand from GitHub Releases.</p> <p>Download pipeline: <code>ModelManager</code> tracks each model's expected SHA256 hash. Downloads are triggered via the <code>/models/download/{model_name}</code> endpoint, run in the background, and validated against the hash before being placed in the models directory. The <code>/models/status</code> endpoint reports per-model installation state.</p> <p>Capabilities API: The <code>/capabilities</code> endpoint inspects which data files and model files are present on disk and reports which features are available (upstream sync, duplicate detection, identification, tattoo signal). The plugin UI uses this to show/hide feature-specific controls and offer model download prompts for missing capabilities.</p>"},{"location":"architecture/#plugin-distribution","title":"Plugin Distribution","text":"<p>The plugin is distributed via a Stash-compatible package index hosted on GitHub Pages.</p> <p>Build: <code>build_plugin_index.sh</code> extracts metadata from <code>plugin/stash-sense.yml</code>, creates a zip of the plugin directory, generates an <code>index.yml</code> with version info (plugin version + git commit hash), and writes both to an output directory.</p> <p>Deployment: A GitHub Actions docs workflow triggers on version tag pushes and plugin file changes. It runs the build script and deploys the output to the <code>gh-pages</code> branch via <code>ghp-import</code>. The resulting index is available at <code>https://carrotwaxr.github.io/stash-sense/plugin/index.yml</code>.</p> <p>Usage: Users add this URL as a plugin source in Stash settings (Settings &gt; Plugins &gt; Available Plugins). Stash checks the index for updates and installs/updates the plugin automatically.</p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"Variable Required Default Description <code>STASH_URL</code> Yes \u2014 URL to your Stash instance (e.g., <code>http://stash:9999</code>) <code>STASH_API_KEY</code> Yes \u2014 Stash API key (Settings &gt; Security &gt; API Key) <code>DATA_DIR</code> No <code>/data</code> Path to database files inside the container <code>LOG_LEVEL</code> No <code>warning</code> Logging verbosity: <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code>"},{"location":"configuration/#stash-box-api-keys-optional","title":"Stash-Box API Keys (Optional)","text":"<p>For upstream sync to detect metadata changes on stash-box endpoints, add API keys for each endpoint you want to monitor:</p> Variable Endpoint <code>STASHDB_API_KEY</code> StashDB <code>FANSDB_API_KEY</code> FansDB <code>THEPORNDB_API_KEY</code> ThePornDB <code>PMVSTASH_API_KEY</code> PMVStash <code>JAVSTASH_API_KEY</code> JAVStash <p>These are only needed for upstream sync. Face recognition works without them.</p> <p>Tip: Get API keys from each stash-box site's user settings page. StashDB requires an account at stashdb.org.</p>"},{"location":"configuration/#volume-mounts","title":"Volume Mounts","text":"Container Path Purpose Mode <code>/data</code> Database files (Voyager indices, performers.db, manifest) Read-only <code>/root/.insightface</code> Cached model weights (~500 MB) Read-write"},{"location":"configuration/#database-files","title":"Database Files","text":"<p>The <code>/data</code> volume should contain the files from a stash-sense-data release:</p> <pre><code>/data/\n\u251c\u2500\u2500 performers.db        # Performer metadata (SQLite)\n\u251c\u2500\u2500 face_facenet.voy     # FaceNet512 embedding index\n\u251c\u2500\u2500 face_arcface.voy     # ArcFace embedding index\n\u251c\u2500\u2500 faces.json           # Face-to-performer mapping\n\u251c\u2500\u2500 performers.json      # Performer lookup data\n\u251c\u2500\u2500 manifest.json        # Version and checksums\n\u2514\u2500\u2500 stash_sense.db       # (auto-created) Your local data \u2014 do not delete\n</code></pre> <p>Note</p> <p><code>stash_sense.db</code> is created automatically by the sidecar and stores your recommendations, settings, and analysis history. It is not part of the database release and should not be deleted during updates.</p>"},{"location":"configuration/#model-cache","title":"Model Cache","text":"<p>The <code>/root/.insightface</code> volume caches the RetinaFace detection model weights. Without this mount, the model is re-downloaded on every container start (~500 MB download).</p>"},{"location":"configuration/#ports","title":"Ports","text":"<p>The sidecar listens on port 5000 inside the container. The recommended host port is 6960:</p> <pre><code>-p 6960:5000\n</code></pre>"},{"location":"configuration/#runtime-settings","title":"Runtime Settings","text":"<p>Most performance and recognition settings are configured via the plugin's Settings tab rather than environment variables. The sidecar auto-detects your hardware and sets sensible defaults.</p> <p>See Settings Reference for the full list of adjustable options.</p>"},{"location":"configuration/#migrated-environment-variables","title":"Migrated Environment Variables","text":"<p>The following environment variables have been replaced by runtime settings. If detected at startup, they are automatically migrated to the settings database:</p> Old Variable New Setting <code>STASH_RATE_LIMIT</code> <code>stash_api_rate</code> <code>ENABLE_BODY_SIGNAL</code> <code>body_signal_enabled</code> <code>ENABLE_TATTOO_SIGNAL</code> <code>tattoo_signal_enabled</code> <code>FACE_CANDIDATES</code> <code>face_candidates</code>"},{"location":"database/","title":"Database &amp; Updates","text":""},{"location":"database/#where-the-data-comes-from","title":"Where the Data Comes From","text":"<p>The face recognition database is built and published in the stash-sense-data repository. It contains performer metadata and face embeddings sourced from multiple stash-box endpoints:</p> Source Type Performers StashDB Primary ~100,000+ FansDB Supplementary Varies ThePornDB Supplementary ~10,000 PMVStash Supplementary ~6,500 JAVStash Supplementary ~21,700 <p>Additional sources include Babepedia, Freeones, IAFD, and other public performer databases via hybrid scraping.</p> <p>Current stats (v2026.02.18): 108,001 performers, 366,794 face embeddings.</p> <p>The database is built using a private training pipeline that:</p> <ol> <li>Downloads performer images from each source</li> <li>Detects and aligns faces using RetinaFace with 5-point similarity transform</li> <li>Generates embeddings using FaceNet512 and ArcFace models with flip-averaging</li> <li>Builds Voyager vector indices for fast cosine-distance search</li> </ol> <p>Database releases are published as zip files on the stash-sense-data releases page.</p>"},{"location":"database/#database-files","title":"Database Files","text":"<p>The data directory contains:</p> File Size Purpose <code>performers.db</code> ~210 MB SQLite database with performer metadata and stash-box IDs <code>face_facenet.voy</code> ~550 MB FaceNet512 embedding Voyager index <code>face_arcface.voy</code> ~550 MB ArcFace embedding Voyager index <code>face_adaface.voy</code> ~550 MB AdaFace IR-101 embedding Voyager index <code>tattoo_embeddings.voy</code> varies Tattoo embedding Voyager index <code>faces.json</code> ~15 MB Face-to-performer mapping <code>performers.json</code> ~10 MB Performer lookup data <code>tattoo_embeddings.json</code> varies Tattoo-to-performer mapping <code>manifest.json</code> &lt;1 KB Version, checksums, and build metadata"},{"location":"database/#two-database-design","title":"Two-Database Design","text":"<p>Stash Sense uses two separate databases:</p> <ul> <li><code>performers.db</code> + Voyager indices (read-only) \u2014 The distributable face recognition data. Updated via stash-sense-data releases. Contains no user-specific information.</li> <li><code>stash_sense.db</code> (read-write, user-local, schema version 9) \u2014 Your recommendation history, dismissed items, analysis watermarks, upstream snapshots, scene fingerprints, operation queue, and settings overrides.</li> </ul> <p>This separation means database updates never touch your personal data, and the distributable database contains nothing specific to your library.</p>"},{"location":"database/#stash_sensedb-tables","title":"stash_sense.db Tables","text":"Table Purpose <code>recommendations</code> Core recommendations (face matches, upstream changes, duplicates) <code>dismissed_recommendations</code> Permanently or temporarily dismissed items <code>analysis_watermarks</code> Tracks last-completed timestamps and cursors for incremental analysis runs. Includes a <code>logic_version</code> column to detect when comparison logic changes and a full re-analysis is needed. <code>upstream_snapshots</code> Cached upstream performer data for 3-way diff engine <code>scene_fingerprints</code> Per-scene face fingerprints for duplicate detection <code>duplicate_candidates</code> Candidate duplicate scene pairs from fingerprint matching <code>job_queue</code> Persistent operation queue with priority, status (<code>queued</code>/<code>running</code>/<code>completed</code>/<code>failed</code>/<code>cancelled</code>), cursor-based resumption, and progress tracking (<code>items_processed</code>/<code>items_total</code>) <code>job_schedules</code> Configurable recurring job schedules with enable/disable toggle, interval (hours), priority, and next-run tracking <code>user_settings</code> User setting overrides (key-value with JSON-encoded values)"},{"location":"database/#checking-for-updates","title":"Checking for Updates","text":""},{"location":"database/#via-the-plugin-ui","title":"Via the Plugin UI","text":"<ol> <li>Open the Settings tab in the Stash Sense plugin</li> <li>The Database section shows your current version and checks for updates automatically</li> <li>If an update is available, click Update</li> <li>Progress is shown in real-time (download, extract, verify, swap, reload)</li> </ol>"},{"location":"database/#via-the-api","title":"Via the API","text":"<p>Check for updates:</p> <pre><code>curl http://localhost:6960/database/check-update\n</code></pre> <pre><code>{\n  \"current_version\": \"2026.02.12\",\n  \"latest_version\": \"2026.02.18\",\n  \"update_available\": true,\n  \"release_name\": \"Database Release v2026.02.18\",\n  \"download_size_mb\": 1478,\n  \"published_at\": \"2026-02-18T01:09:34Z\"\n}\n</code></pre> <p>Start an update:</p> <pre><code>curl -X POST http://localhost:6960/database/update\n</code></pre> <p>Check update progress:</p> <pre><code>curl http://localhost:6960/database/update/status\n</code></pre>"},{"location":"database/#manual-database-update","title":"Manual Database Update","text":"<p>If you prefer to update manually (e.g., on a system without internet access):</p> <ol> <li>Download the latest release zip from stash-sense-data releases</li> <li>Stop the container: <code>docker stop stash-sense</code></li> <li>Extract the zip into your data directory, replacing existing files</li> <li>Start the container: <code>docker start stash-sense</code></li> </ol> <p>The <code>stash_sense.db</code> file in your data directory is your personal data \u2014 do not delete it during a manual update.</p>"},{"location":"database/#update-safety","title":"Update Safety","text":"<ul> <li>A timestamped backup is created before every update</li> <li>SHA-256 checksums are verified against <code>manifest.json</code></li> <li>On failure, the sidecar automatically rolls back to the backup</li> <li>Face identification endpoints return 503 briefly during the swap (~5-10 seconds)</li> <li>All other API endpoints remain available during updates</li> <li><code>stash_sense.db</code> (your recommendations, settings, history) is never touched</li> </ul>"},{"location":"features/","title":"Features","text":""},{"location":"features/#face-recognition","title":"Face Recognition","text":"<p>Stash Sense identifies performers in your scenes by analyzing sprite sheets \u2014 the thumbnail strips that Stash generates for timeline scrubbing. No video decoding required.</p> <p>How it works:</p> <ol> <li>Click Identify Performers on any scene page in Stash</li> <li>The sidecar extracts frames from the sprite sheet and detects faces using RetinaFace</li> <li>Each face is aligned and embedded using two models (FaceNet512 + ArcFace) with flip-averaging for stability</li> <li>Embeddings are searched against Voyager vector indices containing 366,000+ face references</li> <li>Results are clustered by person \u2014 the same performer appearing across multiple frames is grouped together</li> <li>Matched performers are shown with confidence scores and one-click tagging</li> </ol> <p>Matching modes:</p> <ul> <li>Clustered frequency matching (default) \u2014 Groups detected faces by person using cosine distance, then frequency-matches within each cluster. Multi-frame appearances boost confidence.</li> <li>Tagged-performer boost \u2014 Performers already tagged on the scene receive a small distance bonus (+0.03), reducing false negatives for known cast.</li> </ul> <p>Performance: On a GTX 1080 (8GB VRAM), a typical scene (60 frames) processes in ~5 seconds.</p>"},{"location":"features/#gallery-image-identification","title":"Gallery &amp; Image Identification","text":"<p>Extends face recognition to gallery images, which are typically higher quality and better-framed than video frames.</p> <ul> <li>Single image or full gallery identification</li> <li>Results grouped by performer across images with best/average distance</li> <li>Fingerprint caching avoids re-processing on subsequent requests</li> </ul>"},{"location":"features/#duplicate-scene-detection","title":"Duplicate Scene Detection","text":"<p>Finds duplicate scenes that Stash's built-in phash matching misses \u2014 different intros/outros, trimming, aspect ratio changes, or watermarks.</p> <p>Detection signals:</p> Signal Max Confidence How it works Stash-box ID match 100% Authoritative \u2014 same scene on stash-box Face fingerprint similarity 85% Same performers in same ratios (robust to trimming) Metadata overlap 60% Same studio + same performers <p>Signals combine with diminishing returns (<code>primary + secondary x 0.3</code>) to prevent false confidence inflation.</p> <p>Scalability: Uses candidate generation (SQL joins + inverted indices) to produce O(n) candidate pairs instead of O(n^2) brute-force comparisons. Handles 15,000+ scene libraries.</p>"},{"location":"features/#upstream-performer-sync","title":"Upstream Performer Sync","text":"<p>Detects metadata changes on stash-box endpoints (StashDB, FansDB, etc.) and presents per-field merge controls to keep your local Stash library current.</p> <p>3-way diff engine: Compares three states for each field:</p> <ul> <li>Upstream \u2014 current stash-box value</li> <li>Local \u2014 current value in your Stash</li> <li>Snapshot \u2014 last-seen upstream value (stored locally)</li> </ul> <p>This distinguishes intentional local differences from actual upstream changes. If you've deliberately set a different name locally, it won't keep suggesting the upstream name on every sync.</p> <p>Per-field merge controls:</p> Field type Options Name fields Keep local / Accept upstream / Demote to alias / Add as alias / Custom Aliases Union checkboxes (pick which aliases to keep) Simple fields Keep local / Accept upstream / Custom <p>Dismissal: Soft dismiss resurfaces if upstream changes again. Permanent dismiss skips entirely.</p>"},{"location":"features/#upstream-studio-sync","title":"Upstream Studio Sync","text":"<p>Same pattern as performer sync, applied to studios. Detects changes on stash-box endpoints and presents merge controls to keep your local studio metadata current.</p> <p>Tracked fields:</p> <ul> <li>Name \u2014 studio display name</li> <li>URL \u2014 studio website</li> <li>Parent studio \u2014 hierarchical studio relationships</li> </ul> <p>Parent studio comparison uses stash-box IDs rather than name matching, so renamed parent studios are still correctly identified. The 3-way diff engine (upstream / local / snapshot) works identically to performer sync \u2014 intentional local differences are preserved across syncs.</p> <p>Merge controls mirror the performer sync UI: keep local, accept upstream, or dismiss (soft or permanent).</p>"},{"location":"features/#upstream-scene-sync","title":"Upstream Scene Sync","text":"<p>Detects metadata changes for scenes linked to stash-box endpoints and provides per-field merge controls for updating your local library.</p> <p>Tracked field categories:</p> Category Fields Core metadata Title, details, date, director, code, URLs Studio assignment Studio linked to the scene Performer lineup Added/removed performers, alias changes Tags Tag set additions and removals <p>3-way diff with snapshots: Like performer and studio sync, scene sync stores a snapshot of the last-seen upstream state. This distinguishes genuine upstream edits from intentional local differences \u2014 if you have customized a scene title locally, it will not be flagged on every sync cycle.</p> <p>Per-field merge controls allow you to accept, reject, or customize each change independently. Performer lineup changes show which performers were added or removed upstream, and alias changes are surfaced when an upstream performer's credited name differs from the local alias.</p>"},{"location":"features/#recommendations-dashboard","title":"Recommendations Dashboard","text":"<p>A unified plugin page showing all actionable suggestions:</p> <ul> <li>Duplicate scenes \u2014 with confidence scores and signal breakdown</li> <li>Unidentified scenes \u2014 scenes with sprite sheets but no face recognition results</li> <li>Missing stash-box links \u2014 performers in your library without stash-box IDs</li> <li>Upstream updates \u2014 per-field changes from stash-box endpoints</li> </ul> <p>Each recommendation type runs as a background analyzer with incremental watermarking \u2014 only items modified since the last run are re-processed, keeping subsequent runs fast.</p>"},{"location":"features/#operation-queue","title":"Operation Queue","text":"<p>An async job queue with resource-aware scheduling that coordinates all background work \u2014 analysis runs, sync operations, and database updates.</p> <p>Priority levels:</p> Priority Use case Critical Database updates, rollback operations High User-initiated analysis (e.g., \"Identify Performers\" click) Normal Scheduled analysis runs Low Background housekeeping <p>Resource slots prevent contention by ensuring only compatible jobs run concurrently:</p> Slot What uses it Why limited <code>GPU</code> Face detection, embedding inference VRAM is finite <code>CPU_HEAVY</code> Clustering, fingerprint computation Avoid saturating cores <code>NETWORK</code> Stash-box API calls, database downloads Rate limits, bandwidth <code>LIGHT</code> Metadata reads, cache checks Runs alongside anything <p>Jobs declare which resource slot they need. The scheduler runs all compatible jobs in parallel but serializes jobs that compete for the same resource.</p> <p>Persistence: The queue survives sidecar restarts. In-progress jobs are re-queued on startup.</p> <p>Configurable schedules: Recurring analysis jobs (upstream sync, duplicate detection) can be scheduled via the plugin UI with cron-style intervals. Manual triggers are always available.</p> <p>Monitoring: The plugin UI shows active, queued, and completed jobs with status, duration, and error details.</p>"},{"location":"features/#model-management","title":"Model Management","text":"<p>On-demand download and lifecycle management for optional ONNX models that are not bundled in the Docker image.</p> <p>Supported optional models:</p> Model Purpose Size Tattoo detection Identifies tattoos for performer matching ~50 MB <p>Validation: Each model download is verified against a SHA256 checksum before activation. If a model file becomes corrupted (disk error, partial write), the sidecar detects the mismatch and marks the model as <code>corrupted</code>.</p> <p>Status tracking:</p> Status Meaning <code>not_installed</code> Model available for download but not present locally <code>installed</code> Model downloaded, verified, and ready for use <code>corrupted</code> Checksum mismatch \u2014 re-download required <p>Download progress UI: The plugin Settings tab shows real-time download progress for each model with a progress bar. Models can be installed or removed at any time without restarting the sidecar.</p>"},{"location":"features/#settings-system","title":"Settings System","text":"<p>The sidecar auto-detects your hardware at startup (GPU model, VRAM, CPU cores, RAM) and classifies it into a performance tier:</p> Tier Criteria Effect <code>gpu-high</code> CUDA + 4GB+ VRAM Full batch sizes, 8 concurrent workers <code>gpu-low</code> CUDA + &lt;4GB VRAM Reduced batch sizes, 6 concurrent workers <code>cpu</code> No CUDA Minimal batching, 2 concurrent workers <p>All settings can be adjusted in the plugin's Settings tab. Changes are stored in the local database and override tier defaults. Reset any setting to revert to the auto-detected default.</p> <p>See Settings Reference for the full list of configurable options.</p>"},{"location":"features/#database-self-update","title":"Database Self-Update","text":"<p>The face recognition database is distributed separately from the Docker image, allowing updates without rebuilding or restarting the container.</p> <p>Update process:</p> <ol> <li>Check for updates via the Settings UI or the <code>/database/check-update</code> API</li> <li>The sidecar compares your local <code>manifest.json</code> version against the latest stash-sense-data release</li> <li>Click Update \u2014 the sidecar downloads, verifies checksums, swaps files, and reloads</li> <li>A backup of the previous database is created automatically</li> <li>On failure, the sidecar rolls back to the backup</li> </ol> <p>During an update, face identification endpoints return 503 for a brief period (5-10 seconds). All other endpoints remain available. Your recommendation history and settings are never affected by database updates.</p> <p>See Database &amp; Updates for details on the data source and update procedures.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing Stash Sense:</p> <ol> <li>Stash running with GraphQL API enabled</li> <li>Stash API Key generated in Stash Settings &gt; Security</li> <li>Docker installed on your system</li> <li>NVIDIA GPU with 4GB+ VRAM recommended (CPU fallback available)</li> <li>Scene sprite sheets generated in Stash (Settings &gt; Tasks &gt; Generate &gt; Sprites)</li> </ol>"},{"location":"installation/#step-1-download-the-database","title":"Step 1: Download the Database","text":"<p>Stash Sense requires a pre-built face recognition database. Download the latest release from the stash-sense-data repository.</p> <pre><code>mkdir -p stash-sense-data\ncd stash-sense-data\n# Download the .zip file from the latest release (~1.5 GB)\nunzip stash-sense-data-*.zip\ncd ..\n</code></pre> <p>After extraction, you should have:</p> <pre><code>stash-sense-data/\n\u251c\u2500\u2500 performers.db            # Performer metadata (required)\n\u251c\u2500\u2500 face_facenet.voy         # FaceNet512 embedding index (required)\n\u251c\u2500\u2500 face_arcface.voy         # ArcFace embedding index (required)\n\u251c\u2500\u2500 faces.json               # Face reference data (required)\n\u251c\u2500\u2500 performers.json          # Performer lookup data (required)\n\u251c\u2500\u2500 manifest.json            # Database version and checksums (required)\n\u251c\u2500\u2500 face_adaface.voy         # AdaFace IR-101 embedding index (optional)\n\u251c\u2500\u2500 tattoo_embeddings.voy    # Tattoo embedding index (optional)\n\u2514\u2500\u2500 tattoo_embeddings.json   # Tattoo reference data (optional)\n</code></pre> <p>The six required files are needed for core face recognition. The optional files (<code>face_adaface.voy</code>, <code>tattoo_embeddings.voy</code>, <code>tattoo_embeddings.json</code>) are only needed if you enable the AdaFace model or tattoo detection features in settings.</p>"},{"location":"installation/#step-2-start-the-container","title":"Step 2: Start the Container","text":""},{"location":"installation/#docker-run","title":"Docker Run","text":"<pre><code>docker run -d \\\n  --name stash-sense \\\n  --gpus all \\\n  -p 6960:5000 \\\n  -e STASH_URL=http://your-stash-host:9999 \\\n  -e STASH_API_KEY=your-api-key \\\n  -v /path/to/stash-sense-data:/data:ro \\\n  -v stash-sense-insightface:/root/.insightface \\\n  carrotwaxr/stash-sense:latest\n</code></pre>"},{"location":"installation/#docker-compose","title":"Docker Compose","text":"<p>Create a <code>docker-compose.yml</code>:</p> <pre><code>services:\n  stash-sense:\n    image: carrotwaxr/stash-sense:latest\n    container_name: stash-sense\n    restart: unless-stopped\n    ports:\n      - \"6960:5000\"\n    volumes:\n      - /path/to/stash-sense-data:/data:ro\n      - stash-sense-insightface:/root/.insightface\n    environment:\n      - STASH_URL=http://your-stash-host:9999\n      - STASH_API_KEY=your-api-key\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n\nvolumes:\n  stash-sense-insightface:\n</code></pre> <p>Then start it:</p> <pre><code>docker compose up -d\n</code></pre>"},{"location":"installation/#cpu-only-mode","title":"CPU-Only Mode","text":"<p>If you don't have an NVIDIA GPU, remove the GPU configuration:</p> <p>Docker Run \u2014 remove <code>--gpus all</code>:</p> <pre><code>docker run -d \\\n  --name stash-sense \\\n  -p 6960:5000 \\\n  -e STASH_URL=http://your-stash-host:9999 \\\n  -e STASH_API_KEY=your-api-key \\\n  -v /path/to/stash-sense-data:/data:ro \\\n  -v stash-sense-insightface:/root/.insightface \\\n  carrotwaxr/stash-sense:latest\n</code></pre> <p>Docker Compose \u2014 remove the <code>deploy.resources</code> block.</p> <p>CPU mode works but is significantly slower for face detection (~2-3 seconds per frame vs ~200ms with GPU).</p>"},{"location":"installation/#step-3-verify","title":"Step 3: Verify","text":"<pre><code>curl http://localhost:6960/health\n</code></pre> <p>Should return:</p> <pre><code>{\n  \"status\": \"healthy\",\n  \"database_loaded\": true,\n  \"performer_count\": 108001,\n  \"face_count\": 366794\n}\n</code></pre>"},{"location":"installation/#step-4-install-the-stash-plugin","title":"Step 4: Install the Stash Plugin","text":"<p>Add the Stash Sense plugin source in Stash:</p> <ol> <li>Go to Settings &gt; Plugins &gt; Available Plugins</li> <li>Click Add Source and enter:</li> <li>Name: <code>Stash Sense</code></li> <li>URL: <code>https://carrotwaxr.github.io/stash-sense/plugin/index.yml</code></li> <li>Click Save</li> <li>Find Stash Sense in the available plugins list and click Install</li> <li>Go to Settings &gt; Plugins &gt; Stash Sense and set the Sidecar URL to <code>http://your-stash-sense-host:6960</code></li> </ol> <p>Tip: If Stash and Stash Sense run on the same machine, use <code>http://stash-sense:5000</code> (Docker networking) or <code>http://localhost:6960</code> (host networking).</p>"},{"location":"installation/#unraid-installation","title":"unRAID Installation","text":"<p>See unRAID Setup for template-based installation and GPU passthrough configuration.</p>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Configure settings \u2014 environment variables and volume mounts</li> <li>Plugin usage \u2014 how to identify performers and use the dashboard</li> <li>Features \u2014 full feature overview</li> <li>Database &amp; Updates \u2014 where the data comes from and how to update</li> </ul>"},{"location":"plugin/","title":"Stash Plugin Setup","text":""},{"location":"plugin/#installation","title":"Installation","text":""},{"location":"plugin/#1-copy-plugin-files-to-stash","title":"1. Copy plugin files to Stash","text":"<p>Copy the <code>plugin/</code> directory contents to your Stash plugins folder:</p> <pre><code>~/.stash/plugins/stash-sense/\n\u251c\u2500\u2500 stash-sense.yml\n\u251c\u2500\u2500 stash-sense.js\n\u251c\u2500\u2500 stash-sense.css\n\u2514\u2500\u2500 stash_sense_backend.py\n</code></pre> <p>Or if using Docker:</p> <pre><code>/root/.stash/plugins/stash-sense/\n</code></pre>"},{"location":"plugin/#2-reload-plugins","title":"2. Reload plugins","text":"<p>In Stash: Settings \u2192 Plugins \u2192 Reload Plugins</p>"},{"location":"plugin/#3-configure-the-plugin","title":"3. Configure the plugin","text":"<p>In Stash: Settings \u2192 Plugins \u2192 Stash Sense</p> Setting Value Sidecar URL <code>http://localhost:5000</code> or your Stash Sense host"},{"location":"plugin/#usage","title":"Usage","text":""},{"location":"plugin/#identifying-performers-in-a-scene","title":"Identifying Performers in a Scene","text":"<ol> <li>Navigate to a scene page in Stash</li> <li>Click the \"Identify Performers\" button</li> <li>Wait for analysis (uses scene's sprite sheet)</li> <li>Review results in the modal:</li> <li>Detected face thumbnail</li> <li>Best match from StashDB</li> <li>Confidence score</li> <li>Click \"Add to Scene\" to link the performer</li> </ol>"},{"location":"plugin/#understanding-results","title":"Understanding Results","text":"<p>Each detected face shows:</p> <ul> <li>Thumbnail: Cropped face from the scene</li> <li>Match: Best matching performer from StashDB</li> <li>Confidence: Lower is better (cosine distance)</li> <li>Appearances: How many frames this face appeared in</li> </ul> <p>Results are grouped by person - the same performer appearing in multiple frames is clustered together.</p>"},{"location":"plugin/#result-actions","title":"Result Actions","text":"Button Action Add to Scene Links performer to scene (if in your library) View on StashDB Opens performer page on stashdb.org Not in Library Performer matched but not in your Stash"},{"location":"plugin/#requirements","title":"Requirements","text":"<ul> <li>Scene must have a sprite sheet generated</li> <li>Stash Sense sidecar must be running and accessible</li> </ul>"},{"location":"plugin/#generating-sprite-sheets","title":"Generating Sprite Sheets","text":"<p>If a scene doesn't have sprites:</p> <ol> <li>Go to scene page</li> <li>Click Generate \u2192 Enable Sprite</li> <li>Or bulk generate: Settings \u2192 Tasks \u2192 Generate \u2192 Sprites</li> </ol>"},{"location":"plugin/#troubleshooting","title":"Troubleshooting","text":""},{"location":"plugin/#failed-to-connect-to-stash-sense","title":"\"Failed to connect to Stash Sense\"","text":"<ul> <li>Verify sidecar is running: <code>curl http://localhost:5000/health</code></li> <li>Check sidecar URL in plugin settings</li> <li>If using Docker, ensure network connectivity</li> </ul>"},{"location":"plugin/#no-faces-detected","title":"\"No faces detected\"","text":"<ul> <li>Scene may not have clear face shots</li> <li>Sprite sheet may be low quality</li> <li>Try a different scene to verify setup</li> </ul>"},{"location":"plugin/#performer-not-in-library","title":"\"Performer not in library\"","text":"<p>The face matched a StashDB performer who isn't in your Stash library. Options:</p> <ol> <li>Click \"View on StashDB\" to see the performer</li> <li>Manually add the performer to your Stash</li> <li>The plugin shows StashDB ID for easy lookup</li> </ol>"},{"location":"plugin/#plugin-not-appearing","title":"Plugin not appearing","text":"<ol> <li>Check plugin files are in correct location</li> <li>Reload plugins in Stash settings</li> <li>Check Stash logs for errors</li> </ol>"},{"location":"plugin/#ui-components","title":"UI Components","text":""},{"location":"plugin/#recommendations-dashboard","title":"Recommendations Dashboard","text":"<p>The main dashboard (accessible from the Stash navigation) shows all recommendation types:</p> <ul> <li>Upstream Changes \u2014 Performer field updates detected from stash-box endpoints, with a 3-way diff view showing local, upstream, and original values</li> <li>Duplicates \u2014 Candidate duplicate scenes identified by face fingerprint matching</li> <li>All Recommendations \u2014 Combined view of face match suggestions, upstream sync proposals, and duplicate candidates with filtering and bulk actions</li> </ul>"},{"location":"plugin/#settings-tab","title":"Settings Tab","text":"<p>Found under Settings &gt; Plugins &gt; Stash Sense, this tab provides:</p> <ul> <li>Sidecar URL configuration</li> <li>Model Management \u2014 Download or remove optional ONNX models (e.g., AdaFace) directly from the UI</li> <li>Hardware tier and current setting overrides display</li> </ul>"},{"location":"plugin/#operation-queue","title":"Operation Queue","text":"<p>The operations panel shows:</p> <ul> <li>Job Status \u2014 Real-time status of running, queued, completed, and failed jobs with progress indicators</li> <li>Scheduling \u2014 Configure recurring analysis jobs (interval, enable/disable) that automatically re-queue on a timer</li> </ul>"},{"location":"plugin/#scene-upstream-sync-detail","title":"Scene Upstream Sync Detail","text":"<p>When viewing a scene, the upstream sync detail view shows:</p> <ul> <li>Side-by-side comparison of local performer data vs. upstream stash-box data</li> <li>Field-level diffs with accept/reject controls for individual changes</li> <li>Links to the performer's stash-box page for manual review</li> </ul>"},{"location":"settings-system/","title":"Settings System","text":"<p>The sidecar includes a hardware-adaptive settings system that auto-detects your hardware at startup and picks optimal defaults. All runtime settings can be viewed and adjusted via the Settings API or the Plugin Settings UI.</p>"},{"location":"settings-system/#how-it-works","title":"How It Works","text":""},{"location":"settings-system/#hardware-detection","title":"Hardware Detection","text":"<p>On startup, the sidecar probes: - GPU: ONNX Runtime CUDA provider + pynvml for VRAM/model name - CPU: Core count (respects Docker cgroup limits) - Memory: Total and available RAM (respects Docker cgroup limits) - Storage: Free disk space at the data directory</p>"},{"location":"settings-system/#hardware-tiers","title":"Hardware Tiers","text":"<p>Based on detection results, the sidecar classifies your hardware into a tier:</p> Tier Criteria Batch Size Concurrency Detection Res Frames/Scene <code>gpu-high</code> CUDA + VRAM &gt;= 4GB 32 8 640px 60 <code>gpu-low</code> CUDA + VRAM &lt; 4GB 16 6 640px 60 <code>cpu</code> No CUDA 4 2 320px 30"},{"location":"settings-system/#setting-resolution","title":"Setting Resolution","text":"<p>Each setting is resolved in priority order:</p> <ol> <li>User override (stored in the database) \u2014 highest priority</li> <li>Tier default \u2014 based on your detected hardware tier</li> <li>Hardcoded fallback \u2014 always present as a baseline</li> </ol> <p>Only user overrides are stored. Absence means \"use the tier default.\"</p>"},{"location":"settings-system/#settings-reference","title":"Settings Reference","text":""},{"location":"settings-system/#performance","title":"Performance","text":"Setting Type Range Description <code>embedding_batch_size</code> int 1-128 Faces processed per GPU inference call <code>frame_extraction_concurrency</code> int 1-16 Parallel ffmpeg processes for frame extraction <code>detection_size</code> int 160-1280 Face detection input resolution (pixels)"},{"location":"settings-system/#rate-limits","title":"Rate Limits","text":"Setting Type Range Description <code>stash_api_rate</code> float 0.5-50 Max requests/second to local Stash instance"},{"location":"settings-system/#recognition","title":"Recognition","text":"Setting Type Range Description <code>gpu_enabled</code> bool - Use GPU for inference (disable to force CPU) <code>num_frames</code> int 10-200 Frames to sample per scene <code>face_candidates</code> int 5-100 Candidate matches retrieved from vector index per face"},{"location":"settings-system/#signals","title":"Signals","text":"Setting Type Description <code>body_signal_enabled</code> bool Use body proportion analysis <code>tattoo_signal_enabled</code> bool Use tattoo detection (requires model)"},{"location":"settings-system/#settings-api","title":"Settings API","text":""},{"location":"settings-system/#get-all-settings","title":"Get All Settings","text":"<pre><code>GET /settings\n</code></pre> <p>Returns all settings grouped by category with metadata for UI rendering:</p> <pre><code>{\n  \"hardware_tier\": \"gpu-high\",\n  \"categories\": {\n    \"performance\": {\n      \"label\": \"Performance\",\n      \"settings\": {\n        \"embedding_batch_size\": {\n          \"value\": 32,\n          \"default\": 32,\n          \"is_override\": false,\n          \"type\": \"int\",\n          \"min\": 1,\n          \"max\": 128,\n          \"label\": \"Embedding Batch Size\",\n          \"description\": \"Faces processed per GPU inference call\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"settings-system/#get-single-setting","title":"Get Single Setting","text":"<pre><code>GET /settings/{key}\n</code></pre>"},{"location":"settings-system/#update-setting","title":"Update Setting","text":"<pre><code>PUT /settings/{key}\nContent-Type: application/json\n\n{\"value\": 64}\n</code></pre>"},{"location":"settings-system/#bulk-update","title":"Bulk Update","text":"<pre><code>PUT /settings\nContent-Type: application/json\n\n{\"settings\": {\"stash_api_rate\": 10.0, \"num_frames\": 45}}\n</code></pre>"},{"location":"settings-system/#reset-setting-to-default","title":"Reset Setting to Default","text":"<pre><code>DELETE /settings/{key}\n</code></pre> <p>Removes the user override, reverting to the tier default.</p>"},{"location":"settings-system/#system-info","title":"System Info","text":"<pre><code>GET /system/info\n</code></pre> <p>Returns hardware profile, version, and uptime:</p> <pre><code>{\n  \"version\": \"0.1.0-beta.4\",\n  \"uptime_seconds\": 3600,\n  \"hardware\": {\n    \"gpu_available\": true,\n    \"gpu_name\": \"NVIDIA GeForce GTX 1080\",\n    \"gpu_vram_mb\": 8192,\n    \"cpu_cores\": 8,\n    \"memory_total_mb\": 32768,\n    \"tier\": \"gpu-high\",\n    \"summary\": \"NVIDIA GeForce GTX 1080 (8192MB VRAM), 32768MB RAM, 8 cores, 500000MB free disk\"\n  }\n}\n</code></pre>"},{"location":"settings-system/#environment-variable-migration","title":"Environment Variable Migration","text":"<p>The following env vars have been replaced by settings. If detected at startup, they are automatically migrated to the settings database and a deprecation warning is logged.</p> Old Env Var New Setting Notes <code>STASH_RATE_LIMIT</code> <code>stash_api_rate</code> <code>ENABLE_BODY_SIGNAL</code> <code>body_signal_enabled</code> <code>ENABLE_TATTOO_SIGNAL</code> <code>tattoo_signal_enabled</code> <code>FACE_CANDIDATES</code> <code>face_candidates</code> <p>Env vars that remain (connection strings and secrets): - <code>STASH_URL</code>, <code>STASH_API_KEY</code>, <code>DATA_DIR</code>, <code>LOG_LEVEL</code> - Per-endpoint <code>*_URL</code> and <code>*_API_KEY</code> vars</p>"},{"location":"settings-system/#architecture","title":"Architecture","text":""},{"location":"settings-system/#key-files","title":"Key Files","text":"File Purpose <code>api/hardware.py</code> Hardware detection, tier classification <code>api/settings.py</code> Setting definitions, tier defaults, resolution logic, env var migration <code>api/settings_router.py</code> FastAPI endpoints for settings and system info"},{"location":"settings-system/#startup-sequence","title":"Startup Sequence","text":"<ol> <li><code>init_hardware(data_dir)</code> \u2014 probes hardware, classifies tier</li> <li><code>init_settings(db, tier)</code> \u2014 creates settings manager with tier defaults</li> <li><code>migrate_env_vars(mgr)</code> \u2014 one-time migration of deprecated env vars</li> <li>Sidecar logs hardware summary and active tier</li> </ol>"},{"location":"settings-system/#storage","title":"Storage","text":"<p>Settings overrides are stored in the <code>user_settings</code> table of <code>stash_sense.db</code> with a <code>settings.</code> key prefix. The table uses a simple key-value structure with JSON-encoded values.</p>"},{"location":"stash-box-endpoints/","title":"Stash-Box Endpoints","text":"<p>Note: Database building has moved to the stash-sense-trainer repository. This document is preserved for reference but the build commands below apply to the trainer repo.</p> <p>This document describes the different stash-box endpoints supported for building face recognition databases, their characteristics, and considerations for each.</p>"},{"location":"stash-box-endpoints/#supported-endpoints","title":"Supported Endpoints","text":"Endpoint API Type Performers Images/Performer Rate Limit StashDB GraphQL ~100,000+ Multiple 240/min safe ThePornDB REST ~10,000 Multiple 240/min safe PMVStash GraphQL ~6,500 3.7 avg 300/min+ JAVStash GraphQL ~21,700 1.0 avg 300/min+ FansDB GraphQL Unknown Unknown Untested"},{"location":"stash-box-endpoints/#endpoint-details","title":"Endpoint Details","text":""},{"location":"stash-box-endpoints/#stashdb-primary","title":"StashDB (Primary)","text":"<ul> <li>URL: <code>https://stashdb.org/graphql</code></li> <li>API: Standard stash-box GraphQL</li> <li>Use: <code>database_builder.py</code> directly</li> <li>Notes: Largest database, highest quality metadata</li> </ul>"},{"location":"stash-box-endpoints/#theporndb","title":"ThePornDB","text":"<ul> <li>URL: <code>https://api.theporndb.net</code> (REST API)</li> <li>API: REST, NOT GraphQL (despite Stash config showing <code>/graphql</code>)</li> <li>Use: <code>build_theporndb.py</code> (custom script)</li> <li>Notes:</li> <li>Has pre-cropped face thumbnails (<code>face_url</code> field)</li> <li>Many performers cross-reference StashDB IDs</li> <li>Rate limit ~275/min, safe at 240/min</li> </ul>"},{"location":"stash-box-endpoints/#pmvstash","title":"PMVStash","text":"<ul> <li>URL: <code>https://pmvstash.org/graphql</code></li> <li>API: Standard stash-box GraphQL</li> <li>Use: <code>database_builder.py</code> with env override</li> <li>Notes:</li> <li>PMV-focused content</li> <li>Good image coverage (100% have images)</li> <li>34% have multiple images (avg 3.7)</li> <li>Portrait images (1280x1920)</li> </ul>"},{"location":"stash-box-endpoints/#javstash","title":"JAVStash","text":"<ul> <li>URL: <code>https://javstash.org/graphql</code></li> <li>API: Standard stash-box GraphQL</li> <li>Use: <code>database_builder.py</code> with env override</li> <li>Notes:</li> <li>JAV-focused content with Japanese names</li> <li>Limited to ~1 image per performer (see below)</li> <li>Square images (705x705)</li> </ul>"},{"location":"stash-box-endpoints/#database-building","title":"Database Building","text":"<p>Database building is handled by a separate private training pipeline. Pre-built databases are published as releases on the stash-sense-data repository.</p>"},{"location":"stash-box-endpoints/#javstash-single-image-limitation","title":"JAVStash: Single Image Limitation","text":"<p>Problem: JAVStash performers typically have only 1 image, which limits face recognition accuracy. Our standard approach of building multiple embeddings per performer (target: 5) doesn't work here.</p>"},{"location":"stash-box-endpoints/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Single embedding = higher false positive/negative rates</li> <li>No redundancy for poor-quality source images</li> <li>Can't leverage voting across multiple embeddings</li> </ul>"},{"location":"stash-box-endpoints/#potential-solutions-todo-discuss","title":"Potential Solutions (TODO: Discuss)","text":"<ol> <li>Scene Frame Extraction</li> <li>JAVStash has scene data with fingerprints</li> <li>Could extract frames from matched scenes in user's library</li> <li>Build embeddings from actual video content</li> <li> <p>Requires: scene matching pipeline, user's local content</p> </li> <li> <p>External Image Sources</p> </li> <li>Cross-reference with other databases (e.g., JAV actress databases)</li> <li>Scrape additional images from linked URLs in performer metadata</li> <li> <p>Ethical/legal considerations for scraping</p> </li> <li> <p>Data Augmentation</p> </li> <li>Generate synthetic variations of single image</li> <li>Horizontal flip, slight rotations, color adjustments</li> <li> <p>Risk: may not improve real-world accuracy</p> </li> <li> <p>Lower Confidence Threshold</p> </li> <li>Accept that JAV matching will be lower confidence</li> <li>Use separate threshold for JAV vs other sources</li> <li> <p>Let users decide on match acceptance</p> </li> <li> <p>Ensemble with ThePornDB</p> </li> <li>Some JAV content exists on ThePornDB</li> <li>Cross-reference and merge embeddings where possible</li> <li> <p>ThePornDB has StashDB links that might map to JAVStash</p> </li> <li> <p>User-Contributed Images</p> </li> <li>Allow users to contribute additional performer images</li> <li>Build community-sourced image database</li> <li>Privacy/consent considerations</li> </ol>"},{"location":"stash-box-endpoints/#recommended-approach","title":"Recommended Approach","text":"<p>Short-term: Options 1 (scene frames) and 4 (adjusted thresholds): - Don't require external data sources - Leverage data users already have - Can be implemented incrementally</p> <p>Long-term: Option 7 - Crowd-Sourced Cloud Service</p> <p>The ultimate solution is a public cloud service (similar to stash-box) where users can submit face embeddings extracted from their identified scenes. This would: - Bypass the single-image limitation entirely - Build models from real scene content, not just promo images - Scale with the community (more users = better models) - Preserve privacy (only embeddings, never raw images)</p>"},{"location":"stash-box-endpoints/#rate-limit-reference","title":"Rate Limit Reference","text":"<p>Tested January 2026:</p> Endpoint Tested Rate Result StashDB 240/min \u2705 No issues (running 24h+) ThePornDB 240/min \u2705 Safe (limit ~275/min) ThePornDB 360/min \u274c 429s at ~275 requests PMVStash 300/min \u2705 No issues JAVStash 300/min \u2705 No issues (slower response) <p>Recommendation: Use 240/min (0.25s delay) for all endpoints as a safe default.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#container-issues","title":"Container Issues","text":""},{"location":"troubleshooting/#container-wont-start","title":"Container won't start","text":"<p>Check logs: <pre><code>docker logs stash-sense\n</code></pre></p> <p>Common causes:</p> <ul> <li>Missing database files - ensure <code>/data</code> contains the <code>.voy</code> files</li> <li>GPU driver issues - try without GPU first to isolate</li> <li>Port conflict - change from 6960 to another port</li> </ul>"},{"location":"troubleshooting/#database-not-loaded","title":"\"Database not loaded\"","text":"<p>The container started but can't find database files.</p> <pre><code># Check what's in the data volume\ndocker exec stash-sense ls -la /data\n</code></pre> <p>Should show: <pre><code>face_facenet.voy\nface_arcface.voy\nperformers.json\nmanifest.json\n</code></pre></p> <p>If empty, the volume mount is misconfigured or database wasn't extracted.</p>"},{"location":"troubleshooting/#health-check-failing","title":"Health check failing","text":"<pre><code># Test manually\ncurl http://localhost:6960/health\n\n# Check container status\ndocker ps -a | grep stash-sense\n</code></pre> <p>If container is restarting, check logs for the error.</p>"},{"location":"troubleshooting/#gpu-issues","title":"GPU Issues","text":""},{"location":"troubleshooting/#cuda-not-available","title":"\"CUDA not available\"","text":"<p>The container fell back to CPU mode.</p> <p>Verify GPU is accessible: <pre><code>docker exec stash-sense nvidia-smi\n</code></pre></p> <p>If this fails:</p> <ol> <li>Ensure nvidia-container-toolkit is installed</li> <li>Container needs <code>--runtime=nvidia --gpus all</code></li> <li>Restart Docker daemon after toolkit install</li> </ol>"},{"location":"troubleshooting/#cuda-out-of-memory","title":"\"CUDA out of memory\"","text":"<p>GPU memory exhausted. Check what's using it:</p> <pre><code>nvidia-smi\n</code></pre> <p>Solutions: - Stop other GPU-using containers temporarily - Use CPU mode (slower but works)</p>"},{"location":"troubleshooting/#slow-performance-on-gpu","title":"Slow performance on GPU","text":"<p>If GPU is detected but still slow:</p> <ol> <li>Check GPU utilization during requests: <code>watch nvidia-smi</code></li> <li>Ensure models aren't being re-downloaded (mount model cache volume)</li> <li>First request is slow (model loading) - subsequent requests should be faster</li> </ol>"},{"location":"troubleshooting/#connection-issues","title":"Connection Issues","text":""},{"location":"troubleshooting/#plugin-cant-reach-sidecar","title":"Plugin can't reach sidecar","text":"<p>From Stash container, test connectivity: <pre><code>docker exec stash curl http://stash-sense:5000/health\n</code></pre></p> <p>Common fixes:</p> <ul> <li>Use container name if on same Docker network</li> <li>Use host IP if on different networks</li> <li>Check firewall isn't blocking port 6960</li> </ul>"},{"location":"troubleshooting/#sidecar-cant-reach-stash","title":"Sidecar can't reach Stash","text":"<p>Test from sidecar: <pre><code>docker exec stash-sense curl http://stash:9999/graphql\n</code></pre></p> <p>Common fixes:</p> <ul> <li>Verify STASH_URL is correct</li> <li>Verify STASH_API_KEY is valid</li> <li>Check Stash is running and accessible</li> </ul>"},{"location":"troubleshooting/#recognition-issues","title":"Recognition Issues","text":""},{"location":"troubleshooting/#no-faces-detected","title":"No faces detected","text":"<ul> <li>Low quality sprites: Regenerate with higher quality settings</li> <li>No clear face shots: Some scenes don't have good face visibility</li> <li>Very small faces: Detection has minimum size threshold</li> </ul>"},{"location":"troubleshooting/#wrong-matches","title":"Wrong matches","text":"<ul> <li>Low confidence score: Scores &gt; 0.6 are less reliable</li> <li>Similar looking performers: Some faces are genuinely similar</li> <li>Database coverage: Performer may not be in StashDB or have poor reference images</li> </ul>"},{"location":"troubleshooting/#performer-matched-but-not-in-library","title":"Performer matched but not in library","text":"<p>This is expected behavior - the face matched someone in StashDB who you haven't added to your Stash yet. Options:</p> <ol> <li>Click \"View on StashDB\" to verify the match</li> <li>Add the performer to your Stash manually</li> <li>Use Stash's \"Identify\" feature to import from StashDB</li> </ol>"},{"location":"troubleshooting/#database-issues","title":"Database Issues","text":""},{"location":"troubleshooting/#corrupt-database-files","title":"Corrupt database files","text":"<p>If you see index errors or crashes on load:</p> <pre><code># Remove and re-download\nrm -rf /path/to/data/*\n# Re-extract from release\ntar -xzf stash-sense-db.tar.gz -C /path/to/data/\n</code></pre>"},{"location":"troubleshooting/#outdated-database","title":"Outdated database","text":"<p>Check current version: <pre><code>curl http://localhost:6960/database/info\n</code></pre></p> <p>Compare with latest release on GitHub. New databases include more performers and improved embeddings.</p>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":"<p>If you're stuck:</p> <ol> <li>Check container logs: <code>docker logs stash-sense</code></li> <li>Test health endpoint: <code>curl http://localhost:6960/health</code></li> <li>Verify database files exist and have correct permissions</li> <li>Open an issue on GitHub with:</li> <li>Container logs</li> <li>Health endpoint response</li> <li>Docker/Unraid version</li> <li>GPU model (if applicable)</li> </ol>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/","title":"Settings Page UI/UX Pass","text":"<p>Date: 2026-02-19 Ticket: refactor: settings page UI/UX pass - consistent components, human-friendly intervals</p>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#problem","title":"Problem","text":"<p>The plugin settings page has accumulated visual inconsistencies: - Job Schedules uses raw <code>&lt;input type=\"number\"&gt;</code> with browser-native spinners and inline styles - Interval values displayed as raw hours (168 = 1 week) \u2014 meaningless at a glance - Three different coding patterns across Sidecar Settings, Job Schedules, and Upstream Sync sections - Inline styles scattered throughout <code>stash-sense-settings.js</code> instead of CSS classes - Inconsistent save patterns: auto-save vs explicit Save button</p>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#design-decisions","title":"Design Decisions","text":"<ul> <li>Server-side interval definitions: <code>JobDefinition</code> gets <code>allowed_intervals</code> and <code>description</code> fields. The <code>queue_types</code> API serializes them to the frontend. Single source of truth.</li> <li>Tiered interval presets: Light/Network jobs get frequent options (6h\u20132w), heavy/slow-changing jobs get infrequent options (1d\u20131mo).</li> <li>Native <code>&lt;select&gt;</code> for interval picker: Styled to match dark theme. Accessible, zero JS overhead, options come from server.</li> <li>Auto-save everywhere: All settings auto-save with 500ms debounce, except upstream field config (batch checkbox operation keeps explicit Save).</li> <li>No new component abstractions: Use existing CSS classes consistently + add a few new ones. No UI library needed.</li> <li>Defaults unchanged: 24h for DB update + upstream, 168h for duplicates + fingerprints.</li> </ul>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#server-changes","title":"Server Changes","text":""},{"location":"plans/2026-02-19-settings-ui-ux-pass/#job_modelspy","title":"job_models.py","text":"<p>Add predefined interval tiers:</p> <pre><code>INTERVALS_FREQUENT = [\n    (6, \"Every 6 hours\"), (12, \"Every 12 hours\"),\n    (24, \"Every day\"), (48, \"Every 2 days\"), (72, \"Every 3 days\"),\n    (168, \"Every week\"), (336, \"Every 2 weeks\"),\n]\n\nINTERVALS_INFREQUENT = [\n    (24, \"Every day\"), (48, \"Every 2 days\"), (72, \"Every 3 days\"),\n    (168, \"Every week\"), (336, \"Every 2 weeks\"), (720, \"Every month\"),\n]\n</code></pre> <p>Extend <code>JobDefinition</code>: - <code>description: str</code> \u2014 human-readable description of what the job does - <code>allowed_intervals: list[tuple[int, str]]</code> \u2014 valid interval options for this job type</p> <p>Job descriptions: - Database Update: \"Checks for updated face recognition data\" - Upstream Performer Changes: \"Detects field changes from stash-box sources\" - Duplicate Performer Detection: \"Finds performers that may be duplicates\" - Duplicate Scene File Detection: \"Finds scenes with identical file fingerprints\" - Duplicate Scene Detection: \"Finds scenes that may be the same content\" - Fingerprint Generation: \"Generates face recognition fingerprints for scenes\"</p> <p>Interval assignments: - <code>INTERVALS_FREQUENT</code>: Database Update, Upstream Performer Changes - <code>INTERVALS_INFREQUENT</code>: All three duplicate detectors, Fingerprint Generation</p>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#queue_routerpy","title":"queue_router.py","text":"<p>Serialize new fields in <code>queue_types</code> response: <pre><code>{\n  \"type_id\": \"database_update\",\n  \"display_name\": \"Database Update\",\n  \"description\": \"Checks for updated face recognition data\",\n  \"allowed_intervals\": [\n    {\"hours\": 6, \"label\": \"Every 6 hours\"},\n    {\"hours\": 12, \"label\": \"Every 12 hours\"},\n    ...\n  ]\n}\n</code></pre></p>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#frontend-changes","title":"Frontend Changes","text":""},{"location":"plans/2026-02-19-settings-ui-ux-pass/#new-css-classes-stash-sensecss","title":"New CSS classes (stash-sense.css)","text":"<ul> <li><code>.ss-select</code> \u2014 styled native <code>&lt;select&gt;</code> matching dark theme (same border/bg/font as <code>.ss-number-input input</code>)</li> <li><code>.ss-setting-hint</code> \u2014 13px secondary-color text (replaces 4+ inline style occurrences)</li> <li><code>.ss-setting-row-vertical</code> \u2014 variant of <code>.ss-setting-row</code> with <code>flex-direction: column</code></li> <li><code>.ss-setting-row-header</code> \u2014 flex row with space-between for compound row headers</li> <li><code>.ss-upstream-fields-wrapper</code> \u2014 wrapper for expandable field config area</li> <li>Fix <code>.ss-number-input</code> spinner rules: change from <code>opacity: 1</code> to <code>display: none</code> / <code>-webkit-appearance: none</code></li> </ul>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#schedule-row-layout","title":"Schedule row layout","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Database Update                    [toggle]  [Every day \u25be] \u2502\n\u2502  Checks for updated face recognition data                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>Description is static (what the job does), not echoing the interval</li> <li><code>&lt;select&gt;</code> disabled when toggle is off</li> <li>Auto-saves on toggle or dropdown change via <code>debouncedSave</code> pattern</li> </ul>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#renderschedulescategory-refactor","title":"renderSchedulesCategory() refactor","text":"<ul> <li>Replace all inline <code>styles: {}</code> with CSS classes</li> <li>Replace <code>&lt;input type=\"number\"&gt;</code> with <code>&lt;select&gt;</code> populated from <code>allowed_intervals</code></li> <li>Remove per-row Save button, use debounced auto-save</li> <li>Use <code>.ss-setting-row</code>, <code>.ss-setting-info</code>, <code>.ss-setting-control</code> consistently</li> </ul>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#renderendpointfieldconfig-cleanup","title":"renderEndpointFieldConfig() cleanup","text":"<ul> <li>Inline style on row wrapper \u2192 <code>.ss-setting-row .ss-setting-row-vertical</code></li> <li>Inline flex header \u2192 <code>.ss-setting-row-header</code></li> <li>\"Show Fields\" button \u2192 <code>.ss-btn .ss-btn-sm</code></li> <li>Fields wrapper \u2192 <code>.ss-upstream-fields-wrapper</code></li> <li>Loading/help text \u2192 <code>.ss-setting-hint</code></li> <li>Keep explicit Save for batch field config (not auto-save)</li> <li>No behavioral changes</li> </ul>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#other-inline-style-cleanup","title":"Other inline style cleanup","text":"<ul> <li>Description/help text throughout \u2192 <code>.ss-setting-hint</code></li> <li>Remove all remaining <code>style:</code> attributes in <code>stash-sense-settings.js</code></li> </ul>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#not-in-scope","title":"Not in Scope","text":"<ul> <li>StashBox provider cards at top of settings page</li> <li>Operations tab styling</li> <li>API behavior changes beyond serializing new metadata fields</li> </ul>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#testing","title":"Testing","text":"<ul> <li>Unit: <code>JobDefinition</code> serialization includes <code>description</code> and <code>allowed_intervals</code> in <code>queue_types</code> response</li> <li>Unit: Schedule update API accepts hour values from predefined lists</li> <li>Manual: Visual verification \u2014 no inline styles, consistent alignment, dropdown works, auto-save works</li> <li>Manual: Responsive layout at 768px breakpoint</li> <li>Manual: Toggle disables/enables dropdown</li> </ul>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#key-files","title":"Key Files","text":"<ul> <li><code>api/job_models.py</code> \u2014 <code>JobDefinition</code> extensions, interval tier constants</li> <li><code>api/queue_router.py</code> \u2014 serialize new fields</li> <li><code>plugin/stash-sense-settings.js</code> \u2014 full refactor of schedule + upstream sync rendering</li> <li><code>plugin/stash-sense.css</code> \u2014 new classes, spinner fix</li> </ul>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/","title":"Automated Scene Identification via Stash-Box Fingerprints","text":"<p>Date: 2026-02-20 Status: Design Ticket: feat: automated scene identification via stash-box fingerprints</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#summary","title":"Summary","text":"<p>Automated background job that matches local Stash scenes to stash-box entries using fingerprint lookups (MD5, OSHASH, PHASH). Results surface as recommendations with quality scoring, dismiss/accept actions, and \"Accept All\" for high-confidence matches. Mirrors Stash's Tagger UI functionality but adds scheduled background scraping, persistent dismissal tracking, and quality thresholds.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#background","title":"Background","text":"<p>Stash's built-in Tagger UI is functionally perfect for manual scene identification but requires the user to sit through each match interactively. Stash's <code>metadataIdentify</code> batch mode is too coarse \u2014 it either skips multi-match scenes entirely or blindly takes the first result, with no quality scoring or dismissal memory.</p> <p>This feature fills the gap: automated, scheduled, quality-aware scene identification that accumulates results over time and lets users batch-accept high-confidence matches while reviewing ambiguous ones.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#architecture","title":"Architecture","text":""},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#data-flow","title":"Data Flow","text":"<ol> <li> <p>Scan phase (NETWORK resource job): Query Stash for all local scenes with their file fingerprints. For each scene, determine which configured stash-box endpoints it's missing a <code>stash_id</code> for.</p> </li> <li> <p>Match phase: For scenes missing a stash_id, batch-submit their fingerprints to that stash-box's <code>findScenesBySceneFingerprints</code> GraphQL API (up to 40 scenes per batch, matching Stash's own batching).</p> </li> <li> <p>Score &amp; filter phase: For each candidate match, compute a quality score based on fingerprint match count/percentage, fingerprint types, duration agreement, and community vote count. Store qualifying matches as recommendations.</p> </li> <li> <p>Dismiss-aware deduplication: Before creating a recommendation, check if this specific <code>(local_scene_id, stashbox_scene_id)</code> pair has been dismissed. If so, skip it. A different stashbox scene matching the same local scene is a new recommendation \u2014 this handles the scenario where a wrong match is dismissed but a correct one appears later.</p> </li> </ol>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#why-not-baseupstreamanalyzer","title":"Why Not BaseUpstreamAnalyzer","text":"<p><code>BaseUpstreamAnalyzer</code> is designed for 3-way diffs on entities that already have a stash_id link. This analyzer finds scenes that don't have that link yet. It extends <code>BaseAnalyzer</code> directly, similar to the duplicate detection analyzers.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#quality-scoring","title":"Quality Scoring","text":"<p>Each candidate match is scored on multiple signals:</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#fingerprint-match-signals","title":"Fingerprint Match Signals","text":"<ul> <li>Exact hash match (MD5 or OSHASH): Highest confidence \u2014 same file content. If either matches, almost certainly correct.</li> <li>PHASH match: Lower confidence, quality depends on Hamming distance. Distance 0 is strong, 1-8 progressively weaker. We don't control the stash-box server's distance threshold, but we can apply our own stricter filtering on results.</li> <li>Match count: How many of the local scene's fingerprints appear in the stash-box result (e.g., 2/3 or 3/3).</li> </ul>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#duration-agreement","title":"Duration Agreement","text":"<p>Compare local file duration against the stash-box scene's fingerprint durations (each fingerprint submission includes a duration value). Close agreement (+/- a few seconds) reinforces the match. Large discrepancy is a warning signal.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#community-confidence","title":"Community Confidence","text":"<p>Net vote count from stash-box's fingerprint voting system (submissions minus reports). Higher net votes = more community validation of this fingerprint association.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#high-confidence-classification","title":"High-Confidence Classification","text":"<p>A match is flagged <code>high_confidence</code> (eligible for \"Accept All\") when it meets both user-configurable thresholds:</p> <ul> <li>Minimum matching fingerprints (default: 2): At least N fingerprints in agreement</li> <li>Minimum match percentage (default: 66%): At least this percentage of the local scene's fingerprints match</li> </ul> <p>Additional rule: If a local scene has multiple candidate matches from the same endpoint, none qualify for Accept All \u2014 ambiguity requires human judgment.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#user-configurable-settings","title":"User-Configurable Settings","text":"Setting Default Description <code>min_matching_fingerprints</code> 2 Minimum fingerprint matches for high-confidence <code>min_match_percentage</code> 66 Minimum % of local fingerprints that must match <p>These are stored via the existing user settings system (<code>getUserSetting</code>/<code>setUserSetting</code>).</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#dismissal-mechanics","title":"Dismissal Mechanics","text":""},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#pair-based-dismissal","title":"Pair-Based Dismissal","text":"<p>Dismissals are keyed on the <code>(local_scene_id, stashbox_scene_id)</code> pair, not just the local scene. This enables the critical scenario:</p> <ol> <li>Scan finds scene A matches stashbox scene X (wrong match)</li> <li>User dismisses it</li> <li>Next scan: stashbox scene X is still returned for scene A \u2192 pair is dismissed, skip</li> <li>Community fixes the fingerprint data; now scene A matches stashbox scene Y instead</li> <li>Scene Y is a different stashbox_scene_id \u2192 new recommendation surfaces</li> </ol>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#implementation","title":"Implementation","text":"<p>Uses the existing <code>dismissed_targets</code> table with <code>type=\"scene_fingerprint_match\"</code>. The <code>target_id</code> is the local scene ID. The paired stashbox scene ID is stored in the recommendation's <code>details</code> JSON and checked during the dismiss comparison. The <code>is_dismissed()</code> check is extended to compare against the details field for this recommendation type.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#recommendation-schema","title":"Recommendation Schema","text":""},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#type","title":"Type","text":"<p><code>scene_fingerprint_match</code> \u2014 added to the recommendation type registry alongside existing types.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#details-json-structure","title":"Details JSON Structure","text":"<pre><code>{\n  \"local_scene_id\": \"123\",\n  \"endpoint\": \"https://stashdb.org/graphql\",\n  \"stashbox_scene_id\": \"abc-uuid\",\n  \"stashbox_scene_title\": \"Scene Title\",\n  \"stashbox_studio\": \"Studio Name\",\n  \"stashbox_performers\": [\"Performer A\", \"Performer B\"],\n  \"stashbox_date\": \"2024-01-15\",\n  \"stashbox_cover_url\": \"https://...\",\n  \"matching_fingerprints\": [\n    {\"algorithm\": \"MD5\", \"hash\": \"abc123\", \"duration\": 1834, \"submissions\": 5},\n    {\"algorithm\": \"OSHASH\", \"hash\": \"def456\", \"duration\": 1834, \"submissions\": 3}\n  ],\n  \"total_local_fingerprints\": 3,\n  \"match_count\": 2,\n  \"match_percentage\": 66.7,\n  \"duration_local\": 1835,\n  \"duration_remote\": 1834,\n  \"high_confidence\": true,\n  \"phash_distance\": null\n}\n</code></pre> <p>Enough metadata is stored to render the UI without round-trips back to the stash-box API. Title, studio, performers, date, and cover URL are all captured at scan time.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#job-queue-integration","title":"Job Queue Integration","text":""},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#job-definition","title":"Job Definition","text":"<pre><code>\"scene_fingerprint_match\": JobDefinition(\n    type_id=\"scene_fingerprint_match\",\n    display_name=\"Scene Fingerprint Matching\",\n    description=\"Match local scenes to stash-box entries via fingerprints\",\n    resource=ResourceType.NETWORK,\n    default_priority=JobPriority.NORMAL,\n    supports_incremental=True,\n    schedulable=True,\n    default_interval_hours=168,  # weekly\n    allowed_intervals=((24, \"Daily\"), (72, \"Every 3 days\"), (168, \"Weekly\"), (336, \"Every 2 weeks\")),\n)\n</code></pre> <p>NETWORK resource because the bottleneck is stash-box API calls, not CPU or GPU.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#incremental-strategy","title":"Incremental Strategy","text":"<p>Full scan queries every local scene for fingerprints and stash_ids \u2014 expensive with large libraries. Incremental runs use a watermark based on <code>scene.updated_at</code> from Stash, so only scenes added or modified since the last run are re-checked. A scene getting a new file (re-encode, remux) updates its timestamp and triggers re-evaluation.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#batching","title":"Batching","text":"<p>The <code>findScenesBySceneFingerprints</code> API accepts up to 40 scenes per batch. For a 10K scene library where ~3K scenes lack a stash_id for an endpoint, that's ~75 API calls per endpoint \u2014 manageable with rate limiting.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#per-endpoint-iteration","title":"Per-Endpoint Iteration","text":"<p>The analyzer iterates each configured stash-box endpoint independently. A scene might be matched on StashDB but not yet on ThePornDB \u2014 each endpoint gets its own pass.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#action-flow","title":"Action Flow","text":""},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#accept-single-match","title":"Accept (Single Match)","text":"<ol> <li>Update the local scene in Stash: add <code>stash_id</code> entry <code>{endpoint, stash_id}</code> via <code>update_scene</code></li> <li>Mark the recommendation as resolved with <code>action: \"accepted\"</code></li> <li>(Future) Optionally submit the local scene's fingerprints back to stash-box as a community contribution</li> </ol>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#accept-all","title":"Accept All","text":"<p>Operates on all <code>high_confidence</code> recommendations, filterable by endpoint. Same action as single accept, repeated in batch. API accepts a filter: <code>{\"high_confidence\": true, \"endpoint\": \"...\"}</code>.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#dismiss","title":"Dismiss","text":"<p>Stores the <code>(local_scene_id, stashbox_scene_id)</code> pair in dismissed_targets. On re-scan, the analyzer skips this exact pair but allows new matches for the same local scene.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#fingerprint-submission-deferred","title":"Fingerprint Submission (Deferred)","text":"<p>Submitting local fingerprints back to stash-box after accepting a match mirrors Stash's Tagger behavior and contributes to community data quality. Deferred to a follow-up \u2014 requires stash-box API key permissions and adds complexity.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#new-code-required","title":"New Code Required","text":""},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#stash-client","title":"Stash Client","text":"<p><code>get_scenes_with_fingerprints(updated_after: str = None) -&gt; list[dict]</code> - Returns scenes with: id, title, stash_ids, files[].fingerprints (MD5, OSHASH, PHASH), files[].duration - Supports filtering by updated_at for incremental runs - May need a new GraphQL query since existing scene queries don't include file fingerprints</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#stashbox-client","title":"StashBox Client","text":"<p><code>find_scenes_by_fingerprints(fingerprint_sets: list[list[dict]]) -&gt; list[list[dict]]</code> - Wrapper around <code>findScenesBySceneFingerprints</code> batch GraphQL query - Input: list of fingerprint-sets (one set per local scene) - Output: list of match-lists (one list per local scene, each containing matched stashbox scenes with their fingerprint/metadata)</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#analyzer","title":"Analyzer","text":"<p><code>api/analyzers/scene_fingerprint_match.py</code> - Extends <code>BaseAnalyzer</code> - Implements the scan \u2192 match \u2192 score \u2192 recommend pipeline - Per-endpoint iteration with incremental watermark support</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#router-endpoints","title":"Router Endpoints","text":"<p><code>POST /recommendations/actions/accept-fingerprint-match</code> - Input: <code>{recommendation_id, scene_id, stash_id, endpoint}</code> - Adds stash_id to local scene, resolves recommendation</p> <p><code>POST /recommendations/actions/accept-all-fingerprint-matches</code> - Input: <code>{endpoint?: string}</code> (optional filter) - Batch accepts all high-confidence matches</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#plugin-backend-proxy","title":"Plugin Backend Proxy","text":"<p>New modes: <code>rec_accept_fingerprint_match</code>, <code>rec_accept_all_fingerprint_matches</code></p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#plugin-ui","title":"Plugin UI","text":"<ul> <li>New dashboard card: <code>scene_fingerprint_match</code> with count + Accept All button</li> <li>List view: local scene info + stashbox match info + fingerprint summary + actions</li> <li>Detail view: full fingerprint breakdown, duration comparison, per-fingerprint vote counts</li> <li>Settings: min_matching_fingerprints and min_match_percentage controls</li> </ul>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#plugin-ui-design","title":"Plugin UI Design","text":""},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#dashboard-card","title":"Dashboard Card","text":"<ul> <li>Icon: fingerprint or link icon</li> <li>Title: \"Scene Fingerprint Matches\"</li> <li>Subtitle: \"N pending (M high-confidence)\"</li> <li>Quick action: \"Accept All High-Confidence (M)\" button</li> <li>Click: navigate to list view</li> </ul>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#list-view","title":"List View","text":"<p>Each row shows: - Local scene: title (or filename), duration - Arrow / link indicator - Stashbox match: title, studio, performers (truncated), date - Fingerprint badge: \"2/3 match\" with color (green = all match, yellow = partial, red = single) - High-confidence indicator (checkmark badge) - Accept / Dismiss buttons</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#detail-view","title":"Detail View","text":"<ul> <li>Side-by-side: local scene info (left) vs stashbox scene info (right)</li> <li>Fingerprint table: algorithm, hash, match status, duration, community votes</li> <li>Duration comparison bar</li> <li>Accept / Dismiss with optional dismiss reason</li> </ul>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#testing","title":"Testing","text":""},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#unit-tests","title":"Unit Tests","text":"<ul> <li>Quality scoring logic: various fingerprint combinations \u2192 correct scores and high_confidence flags</li> <li>Dismiss pair matching: same pair dismissed \u2192 skip, different stashbox scene \u2192 allow</li> <li>Incremental watermark: only scenes after watermark are processed</li> </ul>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#integration-tests","title":"Integration Tests","text":"<ul> <li>Analyzer end-to-end with mocked Stash and StashBox clients</li> <li>Accept action: verify stash_id added to scene and recommendation resolved</li> <li>Accept All: verify only high_confidence matches are accepted</li> <li>Re-scan after dismiss: dismissed pair skipped, new match surfaces</li> </ul>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#documentation","title":"Documentation","text":"<p>None needed beyond this design document. The feature is user-facing through the existing recommendations UI patterns.</p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#key-files","title":"Key Files","text":"File Change <code>api/analyzers/scene_fingerprint_match.py</code> New analyzer <code>api/stash_client_unified.py</code> New fingerprint query method <code>api/stashbox_client.py</code> New batch fingerprint lookup method <code>api/recommendations_router.py</code> Accept/Accept All endpoints <code>api/recommendations_db.py</code> Dismiss pair comparison for this type <code>api/job_models.py</code> Job registration <code>plugin/stash-sense-recommendations.js</code> Dashboard card, list/detail views <code>plugin/stash_sense_backend.py</code> New proxy modes <code>plugin/stash-sense.css</code> Fingerprint match styling"},{"location":"plans/2026-02-20-scene-fingerprint-matching-design/#open-questions","title":"Open Questions","text":"<ul> <li>Fingerprint submission back to stash-box: Include in v1 or defer? Adds community value but requires API key permissions and error handling for permission failures.</li> <li>Cover image display: Should we show the stashbox scene's cover image in the match UI? Helpful for visual confirmation but requires proxying images through the sidecar (CSP restrictions from Stash UI).</li> <li>Scene metadata preview: When accepting a match, should we also offer to pull in the stashbox scene's metadata (title, date, performers, tags) like the Tagger does, or just link the stash_id and let the upstream scene change detector handle field sync on the next run?</li> </ul>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-implementation/","title":"Scene Fingerprint Matching - Implementation Plan","text":"<p>For Claude: REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.</p> <p>Goal: Automated background job that matches local Stash scenes to stash-box entries via fingerprint (MD5/OSHASH/PHASH) lookups, with quality scoring, pair-based dismissals, and batch accept.</p> <p>Architecture: New <code>SceneFingerprintMatchAnalyzer</code> extends <code>BaseAnalyzer</code> (not <code>BaseUpstreamAnalyzer</code>, since we're finding unlinked scenes rather than diffing linked ones). Quality scoring is a standalone module. Composite <code>target_id</code> encodes <code>{scene_id}|{endpoint}|{stashbox_scene_id}</code> so the existing dismissal system handles pair-based logic without DB changes.</p> <p>Tech Stack: Python/FastAPI (sidecar), vanilla JS (plugin), SQLite (recommendations DB), GraphQL (Stash + StashBox APIs)</p> <p>Design doc: <code>docs/plans/2026-02-20-scene-fingerprint-matching-design.md</code></p>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-implementation/#task-1-quality-scoring-module","title":"Task 1: Quality Scoring Module","text":"<p>Pure functions with no external dependencies. TDD-first.</p> <p>Files: - Create: <code>api/scene_fingerprint_scoring.py</code> - Create: <code>api/tests/test_scene_fingerprint_scoring.py</code></p> <p>Step 1: Write the failing tests</p> <pre><code># api/tests/test_scene_fingerprint_scoring.py\n\"\"\"Tests for scene fingerprint match quality scoring.\"\"\"\n\nimport pytest\nfrom scene_fingerprint_scoring import score_match, is_high_confidence\n\n\nclass TestScoreMatch:\n    \"\"\"Test quality score computation for a candidate match.\"\"\"\n\n    def test_single_md5_match(self):\n        \"\"\"MD5 exact match should score high.\"\"\"\n        result = score_match(\n            matching_fingerprints=[\n                {\"algorithm\": \"MD5\", \"hash\": \"abc123\", \"duration\": 1834.5, \"submissions\": 3}\n            ],\n            total_local_fingerprints=3,\n            local_duration=1835.0,\n        )\n        assert result[\"match_count\"] == 1\n        assert result[\"match_percentage\"] == pytest.approx(33.3, abs=0.1)\n        assert result[\"has_exact_hash\"] is True\n\n    def test_multiple_fingerprints_all_match(self):\n        \"\"\"All fingerprints matching = 100% match percentage.\"\"\"\n        result = score_match(\n            matching_fingerprints=[\n                {\"algorithm\": \"MD5\", \"hash\": \"abc\", \"duration\": 1800.0, \"submissions\": 5},\n                {\"algorithm\": \"OSHASH\", \"hash\": \"def\", \"duration\": 1800.0, \"submissions\": 3},\n                {\"algorithm\": \"PHASH\", \"hash\": \"ghi\", \"duration\": 1800.0, \"submissions\": 1},\n            ],\n            total_local_fingerprints=3,\n            local_duration=1800.0,\n        )\n        assert result[\"match_count\"] == 3\n        assert result[\"match_percentage\"] == pytest.approx(100.0)\n        assert result[\"has_exact_hash\"] is True\n\n    def test_phash_only_match(self):\n        \"\"\"PHASH-only match should flag has_exact_hash=False.\"\"\"\n        result = score_match(\n            matching_fingerprints=[\n                {\"algorithm\": \"PHASH\", \"hash\": \"abc\", \"duration\": 1800.0, \"submissions\": 1}\n            ],\n            total_local_fingerprints=2,\n            local_duration=1800.0,\n        )\n        assert result[\"has_exact_hash\"] is False\n\n    def test_duration_agreement(self):\n        \"\"\"Close duration = good agreement, large gap = bad.\"\"\"\n        close = score_match(\n            matching_fingerprints=[\n                {\"algorithm\": \"MD5\", \"hash\": \"a\", \"duration\": 1834.0, \"submissions\": 1}\n            ],\n            total_local_fingerprints=1,\n            local_duration=1835.0,\n        )\n        assert close[\"duration_agreement\"] is True\n\n        far = score_match(\n            matching_fingerprints=[\n                {\"algorithm\": \"MD5\", \"hash\": \"a\", \"duration\": 1700.0, \"submissions\": 1}\n            ],\n            total_local_fingerprints=1,\n            local_duration=1835.0,\n        )\n        assert far[\"duration_agreement\"] is False\n\n    def test_empty_fingerprints(self):\n        \"\"\"No matching fingerprints should return zero counts.\"\"\"\n        result = score_match(\n            matching_fingerprints=[],\n            total_local_fingerprints=3,\n            local_duration=1800.0,\n        )\n        assert result[\"match_count\"] == 0\n        assert result[\"match_percentage\"] == 0.0\n\n\nclass TestIsHighConfidence:\n    \"\"\"Test high-confidence classification.\"\"\"\n\n    def test_meets_both_thresholds(self):\n        assert is_high_confidence(match_count=2, match_percentage=66.7) is True\n\n    def test_below_count_threshold(self):\n        assert is_high_confidence(match_count=1, match_percentage=100.0) is False\n\n    def test_below_percentage_threshold(self):\n        assert is_high_confidence(match_count=2, match_percentage=50.0) is False\n\n    def test_custom_thresholds(self):\n        assert is_high_confidence(\n            match_count=3, match_percentage=80.0,\n            min_count=3, min_percentage=80\n        ) is True\n\n    def test_custom_thresholds_fail(self):\n        assert is_high_confidence(\n            match_count=2, match_percentage=80.0,\n            min_count=3, min_percentage=80\n        ) is False\n</code></pre> <p>Step 2: Run tests to verify they fail</p> <p>Run: <code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -m pytest tests/test_scene_fingerprint_scoring.py -v</code> Expected: FAIL with <code>ModuleNotFoundError: No module named 'scene_fingerprint_scoring'</code></p> <p>Step 3: Write minimal implementation</p> <pre><code># api/scene_fingerprint_scoring.py\n\"\"\"Quality scoring for scene fingerprint matches.\n\nScores a candidate match between a local scene and a stash-box scene\nbased on fingerprint overlap, type, duration agreement, and community votes.\n\"\"\"\n\n# Duration tolerance in seconds for \"agreement\"\nDURATION_TOLERANCE_SECONDS = 5.0\n\n# Algorithms considered \"exact hash\" (not perceptual)\nEXACT_HASH_ALGORITHMS = {\"MD5\", \"OSHASH\"}\n\n# Default high-confidence thresholds\nDEFAULT_MIN_COUNT = 2\nDEFAULT_MIN_PERCENTAGE = 66\n\n\ndef score_match(\n    matching_fingerprints: list[dict],\n    total_local_fingerprints: int,\n    local_duration: float,\n) -&gt; dict:\n    \"\"\"Score a candidate fingerprint match.\n\n    Args:\n        matching_fingerprints: List of dicts with keys:\n            algorithm (str), hash (str), duration (float), submissions (int)\n        total_local_fingerprints: How many fingerprints the local scene has\n        local_duration: Local scene file duration in seconds\n\n    Returns:\n        Dict with: match_count, match_percentage, has_exact_hash,\n        duration_agreement, duration_diff, total_submissions\n    \"\"\"\n    match_count = len(matching_fingerprints)\n\n    if total_local_fingerprints &gt; 0:\n        match_percentage = (match_count / total_local_fingerprints) * 100\n    else:\n        match_percentage = 0.0\n\n    has_exact_hash = any(\n        fp[\"algorithm\"] in EXACT_HASH_ALGORITHMS\n        for fp in matching_fingerprints\n    )\n\n    # Duration agreement: compare local vs remote fingerprint durations\n    duration_diffs = []\n    for fp in matching_fingerprints:\n        remote_dur = fp.get(\"duration\")\n        if remote_dur is not None and local_duration is not None:\n            duration_diffs.append(abs(local_duration - remote_dur))\n\n    if duration_diffs:\n        avg_diff = sum(duration_diffs) / len(duration_diffs)\n        duration_agreement = avg_diff &lt;= DURATION_TOLERANCE_SECONDS\n        duration_diff = round(avg_diff, 2)\n    else:\n        duration_agreement = True  # No data to disagree\n        duration_diff = None\n\n    total_submissions = sum(fp.get(\"submissions\", 0) for fp in matching_fingerprints)\n\n    return {\n        \"match_count\": match_count,\n        \"match_percentage\": round(match_percentage, 1),\n        \"has_exact_hash\": has_exact_hash,\n        \"duration_agreement\": duration_agreement,\n        \"duration_diff\": duration_diff,\n        \"total_submissions\": total_submissions,\n    }\n\n\ndef is_high_confidence(\n    match_count: int,\n    match_percentage: float,\n    min_count: int = DEFAULT_MIN_COUNT,\n    min_percentage: int = DEFAULT_MIN_PERCENTAGE,\n) -&gt; bool:\n    \"\"\"Check if a match qualifies as high-confidence for batch accept.\n\n    Both thresholds must be met.\n    \"\"\"\n    return match_count &gt;= min_count and match_percentage &gt;= min_percentage\n</code></pre> <p>Step 4: Run tests to verify they pass</p> <p>Run: <code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -m pytest tests/test_scene_fingerprint_scoring.py -v</code> Expected: All PASS</p> <p>Step 5: Commit</p> <pre><code>git add api/scene_fingerprint_scoring.py api/tests/test_scene_fingerprint_scoring.py\ngit commit -m \"feat: add scene fingerprint match quality scoring module (#48)\"\n</code></pre>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-implementation/#task-2-stash-client-scene-fingerprint-query","title":"Task 2: Stash Client - Scene Fingerprint Query","text":"<p>Add a method to query Stash for scenes with their file fingerprint hashes.</p> <p>Files: - Modify: <code>api/stash_client_unified.py</code> (add method after <code>get_scenes_for_fingerprinting</code> ~line 815) - Create: <code>api/tests/test_scene_fingerprint_query.py</code></p> <p>Step 1: Write the failing test</p> <pre><code># api/tests/test_scene_fingerprint_query.py\n\"\"\"Tests for scene fingerprint query in Stash client.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch\n\n\n@pytest.mark.asyncio\nasync def test_get_scenes_with_fingerprints_returns_expected_shape():\n    \"\"\"Verify the query method returns scenes with fingerprint data.\"\"\"\n    from stash_client_unified import StashClientUnified\n\n    mock_response = {\n        \"findScenes\": {\n            \"count\": 1,\n            \"scenes\": [\n                {\n                    \"id\": \"42\",\n                    \"title\": \"Test Scene\",\n                    \"updated_at\": \"2026-01-15T00:00:00Z\",\n                    \"files\": [\n                        {\n                            \"id\": \"f1\",\n                            \"duration\": 1835.5,\n                            \"fingerprints\": [\n                                {\"type\": \"md5\", \"value\": \"abc123\"},\n                                {\"type\": \"oshash\", \"value\": \"def456\"},\n                                {\"type\": \"phash\", \"value\": \"0011223344556677\"},\n                            ],\n                        }\n                    ],\n                    \"stash_ids\": [\n                        {\"endpoint\": \"https://stashdb.org/graphql\", \"stash_id\": \"uuid-1\"}\n                    ],\n                }\n            ],\n        }\n    }\n\n    client = StashClientUnified.__new__(StashClientUnified)\n    client._execute = AsyncMock(return_value=mock_response)\n\n    scenes, total = await client.get_scenes_with_fingerprints()\n\n    assert total == 1\n    assert len(scenes) == 1\n    scene = scenes[0]\n    assert scene[\"id\"] == \"42\"\n    assert len(scene[\"files\"]) == 1\n    assert len(scene[\"files\"][0][\"fingerprints\"]) == 3\n    assert scene[\"files\"][0][\"fingerprints\"][0][\"type\"] == \"md5\"\n\n\n@pytest.mark.asyncio\nasync def test_get_scenes_with_fingerprints_incremental():\n    \"\"\"Verify updated_after filter is passed to query.\"\"\"\n    from stash_client_unified import StashClientUnified\n\n    mock_response = {\"findScenes\": {\"count\": 0, \"scenes\": []}}\n\n    client = StashClientUnified.__new__(StashClientUnified)\n    client._execute = AsyncMock(return_value=mock_response)\n\n    await client.get_scenes_with_fingerprints(updated_after=\"2026-01-01T00:00:00Z\")\n\n    call_args = client._execute.call_args\n    variables = call_args[0][1]\n    assert \"scene_filter\" in variables\n    assert \"updated_at\" in variables[\"scene_filter\"]\n    assert variables[\"scene_filter\"][\"updated_at\"][\"value\"] == \"2026-01-01T00:00:00Z\"\n</code></pre> <p>Step 2: Run tests to verify they fail</p> <p>Run: <code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -m pytest tests/test_scene_fingerprint_query.py -v</code> Expected: FAIL with <code>AttributeError: ... has no attribute 'get_scenes_with_fingerprints'</code></p> <p>Step 3: Write minimal implementation</p> <p>Add this method to <code>StashClientUnified</code> in <code>api/stash_client_unified.py</code>, after <code>get_scenes_for_fingerprinting</code> (~line 815):</p> <pre><code>    async def get_scenes_with_fingerprints(\n        self,\n        updated_after: Optional[str] = None,\n        limit: int = 100,\n        offset: int = 0,\n    ) -&gt; tuple[list[dict], int]:\n        \"\"\"\n        Get scenes with file fingerprint hashes for stash-box matching.\n        Returns (scenes, total_count).\n\n        Each scene includes files[].fingerprints with type (md5/oshash/phash)\n        and value, plus stash_ids showing which endpoints are already linked.\n        \"\"\"\n        query = \"\"\"\n        query ScenesWithFingerprints($filter: FindFilterType, $scene_filter: SceneFilterType) {\n          findScenes(filter: $filter, scene_filter: $scene_filter) {\n            count\n            scenes {\n              id\n              title\n              updated_at\n              files {\n                id\n                duration\n                fingerprints {\n                  type\n                  value\n                }\n              }\n              stash_ids {\n                endpoint\n                stash_id\n              }\n            }\n          }\n        }\n        \"\"\"\n        filter_input = {\"per_page\": limit, \"page\": (offset // limit) + 1}\n        scene_filter = {}\n\n        if updated_after:\n            scene_filter[\"updated_at\"] = {\"value\": updated_after, \"modifier\": \"GREATER_THAN\"}\n\n        data = await self._execute(query, {\"filter\": filter_input, \"scene_filter\": scene_filter})\n        return data[\"findScenes\"][\"scenes\"], data[\"findScenes\"][\"count\"]\n</code></pre> <p>Step 4: Run tests to verify they pass</p> <p>Run: <code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -m pytest tests/test_scene_fingerprint_query.py -v</code> Expected: All PASS</p> <p>Step 5: Commit</p> <pre><code>git add api/stash_client_unified.py api/tests/test_scene_fingerprint_query.py\ngit commit -m \"feat: add get_scenes_with_fingerprints query to Stash client (#48)\"\n</code></pre>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-implementation/#task-3-stashbox-client-batch-fingerprint-lookup","title":"Task 3: StashBox Client - Batch Fingerprint Lookup","text":"<p>Add a method to query StashBox's <code>findScenesBySceneFingerprints</code> batch API.</p> <p>Files: - Modify: <code>api/stashbox_client.py</code> (add method) - Create: <code>api/tests/test_stashbox_fingerprint_query.py</code></p> <p>Step 1: Write the failing test</p> <pre><code># api/tests/test_stashbox_fingerprint_query.py\n\"\"\"Tests for stash-box batch fingerprint lookup.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock\n\n\n@pytest.mark.asyncio\nasync def test_find_scenes_by_fingerprints_returns_matched_scenes():\n    \"\"\"Verify batch lookup returns per-scene match lists.\"\"\"\n    from stashbox_client import StashBoxClient\n\n    # StashBox returns one list per input fingerprint set\n    mock_response = {\n        \"findScenesBySceneFingerprints\": [\n            # Matches for scene 1's fingerprints\n            [\n                {\n                    \"id\": \"sb-uuid-1\",\n                    \"title\": \"Matched Scene\",\n                    \"date\": \"2024-01-15\",\n                    \"studio\": {\"id\": \"st-1\", \"name\": \"Studio A\"},\n                    \"performers\": [{\"performer\": {\"id\": \"p-1\", \"name\": \"Actor A\"}, \"as\": None}],\n                    \"urls\": [{\"url\": \"https://example.com\", \"site\": {\"name\": \"Example\"}}],\n                    \"fingerprints\": [\n                        {\"hash\": \"abc123\", \"algorithm\": \"MD5\", \"duration\": 1834, \"submissions\": 5, \"created\": \"2024-01-01\", \"updated\": \"2024-06-01\"},\n                        {\"hash\": \"def456\", \"algorithm\": \"OSHASH\", \"duration\": 1834, \"submissions\": 3, \"created\": \"2024-01-01\", \"updated\": \"2024-06-01\"},\n                    ],\n                    \"duration\": 1834,\n                }\n            ],\n            # No matches for scene 2's fingerprints\n            [],\n        ]\n    }\n\n    client = StashBoxClient.__new__(StashBoxClient)\n    client._execute = AsyncMock(return_value=mock_response)\n\n    fingerprint_sets = [\n        [{\"hash\": \"abc123\", \"algorithm\": \"MD5\"}, {\"hash\": \"def456\", \"algorithm\": \"OSHASH\"}],\n        [{\"hash\": \"xyz789\", \"algorithm\": \"MD5\"}],\n    ]\n\n    results = await client.find_scenes_by_fingerprints(fingerprint_sets)\n\n    assert len(results) == 2\n    assert len(results[0]) == 1  # One match for scene 1\n    assert results[0][0][\"id\"] == \"sb-uuid-1\"\n    assert len(results[0][0][\"fingerprints\"]) == 2\n    assert len(results[1]) == 0  # No matches for scene 2\n\n\n@pytest.mark.asyncio\nasync def test_find_scenes_by_fingerprints_empty_input():\n    \"\"\"Empty input should return empty output without API call.\"\"\"\n    from stashbox_client import StashBoxClient\n\n    client = StashBoxClient.__new__(StashBoxClient)\n    client._execute = AsyncMock()\n\n    results = await client.find_scenes_by_fingerprints([])\n\n    assert results == []\n    client._execute.assert_not_called()\n</code></pre> <p>Step 2: Run tests to verify they fail</p> <p>Run: <code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -m pytest tests/test_stashbox_fingerprint_query.py -v</code> Expected: FAIL with <code>AttributeError: ... has no attribute 'find_scenes_by_fingerprints'</code></p> <p>Step 3: Write minimal implementation</p> <p>Add to <code>StashBoxClient</code> in <code>api/stashbox_client.py</code>:</p> <pre><code>    async def find_scenes_by_fingerprints(\n        self, fingerprint_sets: list[list[dict]]\n    ) -&gt; list[list[dict]]:\n        \"\"\"Batch lookup scenes by fingerprint sets.\n\n        Uses stash-box's findScenesBySceneFingerprints query which accepts\n        multiple fingerprint sets (one per local scene) and returns matched\n        stash-box scenes for each.\n\n        Args:\n            fingerprint_sets: List of fingerprint lists. Each inner list has\n                dicts with keys: hash (str), algorithm (str: MD5/OSHASH/PHASH)\n\n        Returns:\n            List of match-lists, one per input fingerprint set. Each match\n            includes scene metadata and matched fingerprints.\n        \"\"\"\n        if not fingerprint_sets:\n            return []\n\n        query = \"\"\"\n        query FindScenesByFingerprints($fingerprints: [[FingerprintQueryInput!]!]!) {\n          findScenesBySceneFingerprints(fingerprints: $fingerprints) {\n            id\n            title\n            date\n            duration\n            studio { id name }\n            performers { performer { id name } as }\n            urls { url site { name } }\n            images { id url }\n            fingerprints {\n              hash\n              algorithm\n              duration\n              submissions\n              created\n              updated\n            }\n          }\n        }\n        \"\"\"\n\n        data = await self._execute(\n            query, variables={\"fingerprints\": fingerprint_sets}\n        )\n        return data.get(\"findScenesBySceneFingerprints\", [])\n</code></pre> <p>Step 4: Run tests to verify they pass</p> <p>Run: <code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -m pytest tests/test_stashbox_fingerprint_query.py -v</code> Expected: All PASS</p> <p>Step 5: Commit</p> <pre><code>git add api/stashbox_client.py api/tests/test_stashbox_fingerprint_query.py\ngit commit -m \"feat: add batch fingerprint scene lookup to StashBox client (#48)\"\n</code></pre>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-implementation/#task-4-scenefingerprintmatchanalyzer","title":"Task 4: SceneFingerprintMatchAnalyzer","text":"<p>The core analyzer that orchestrates: query scenes \u2192 batch lookup \u2192 score \u2192 recommend.</p> <p>Files: - Create: <code>api/analyzers/scene_fingerprint_match.py</code> - Create: <code>api/tests/test_scene_fingerprint_match_analyzer.py</code></p> <p>Step 1: Write the failing tests</p> <pre><code># api/tests/test_scene_fingerprint_match_analyzer.py\n\"\"\"Tests for SceneFingerprintMatchAnalyzer.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, MagicMock, patch\n\nfrom analyzers.scene_fingerprint_match import SceneFingerprintMatchAnalyzer\n\n\ndef make_mock_db():\n    \"\"\"Create a mock RecommendationsDB.\"\"\"\n    db = MagicMock()\n    db.is_dismissed.return_value = False\n    db.create_recommendation.return_value = 1\n    db.get_user_setting.return_value = None\n    db.get_watermark.return_value = None\n    db.set_watermark.return_value = None\n    return db\n\n\ndef make_mock_stash():\n    \"\"\"Create a mock StashClientUnified.\"\"\"\n    stash = AsyncMock()\n    stash.get_stashbox_connections.return_value = [\n        {\"endpoint\": \"https://stashdb.org/graphql\", \"api_key\": \"key1\", \"name\": \"StashDB\"}\n    ]\n    return stash\n\n\ndef make_scene(scene_id, title, fingerprints, stash_ids=None, duration=1800.0, updated_at=\"2026-01-15T00:00:00Z\"):\n    \"\"\"Helper to build a scene dict.\"\"\"\n    return {\n        \"id\": scene_id,\n        \"title\": title,\n        \"updated_at\": updated_at,\n        \"files\": [\n            {\n                \"id\": f\"file-{scene_id}\",\n                \"duration\": duration,\n                \"fingerprints\": fingerprints,\n            }\n        ],\n        \"stash_ids\": stash_ids or [],\n    }\n\n\ndef make_stashbox_match(scene_id, title, fingerprints, studio=None, performers=None, date=None, duration=None):\n    \"\"\"Helper to build a stash-box match result.\"\"\"\n    return {\n        \"id\": scene_id,\n        \"title\": title,\n        \"date\": date,\n        \"duration\": duration,\n        \"studio\": studio,\n        \"performers\": performers or [],\n        \"urls\": [],\n        \"images\": [],\n        \"fingerprints\": fingerprints,\n    }\n\n\nclass TestAnalyzerRun:\n    \"\"\"Test the full analyzer run pipeline.\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_creates_recommendation_for_match(self):\n        \"\"\"A scene with matching fingerprints should produce a recommendation.\"\"\"\n        db = make_mock_db()\n        stash = make_mock_stash()\n\n        # Local scene with 2 fingerprints, no stash_ids for this endpoint\n        stash.get_scenes_with_fingerprints.return_value = (\n            [make_scene(\"42\", \"My Scene\", [\n                {\"type\": \"md5\", \"value\": \"abc123\"},\n                {\"type\": \"oshash\", \"value\": \"def456\"},\n            ])],\n            1,\n        )\n\n        stashbox_match = make_stashbox_match(\n            \"sb-uuid-1\", \"Matched Scene\",\n            [\n                {\"hash\": \"abc123\", \"algorithm\": \"MD5\", \"duration\": 1800, \"submissions\": 5, \"created\": \"2024-01-01\", \"updated\": \"2024-06-01\"},\n                {\"hash\": \"def456\", \"algorithm\": \"OSHASH\", \"duration\": 1800, \"submissions\": 3, \"created\": \"2024-01-01\", \"updated\": \"2024-06-01\"},\n            ],\n            studio={\"id\": \"s1\", \"name\": \"Studio A\"},\n            performers=[{\"performer\": {\"id\": \"p1\", \"name\": \"Actor A\"}, \"as\": None}],\n            date=\"2024-01-15\",\n        )\n\n        with patch(\"analyzers.scene_fingerprint_match.StashBoxClient\") as MockSBC:\n            mock_sbc = AsyncMock()\n            mock_sbc.find_scenes_by_fingerprints.return_value = [[stashbox_match]]\n            MockSBC.return_value = mock_sbc\n\n            analyzer = SceneFingerprintMatchAnalyzer(stash, db)\n            result = await analyzer.run(incremental=False)\n\n        assert result.recommendations_created == 1\n        db.create_recommendation.assert_called_once()\n        call_kwargs = db.create_recommendation.call_args[1]\n        assert call_kwargs[\"type\"] == \"scene_fingerprint_match\"\n        assert call_kwargs[\"target_type\"] == \"scene\"\n        # Composite target_id: scene_id|endpoint|stashbox_scene_id\n        assert \"42|\" in call_kwargs[\"target_id\"]\n        assert \"sb-uuid-1\" in call_kwargs[\"target_id\"]\n\n    @pytest.mark.asyncio\n    async def test_skips_scene_already_linked_to_endpoint(self):\n        \"\"\"Scene with existing stash_id for this endpoint should be skipped.\"\"\"\n        db = make_mock_db()\n        stash = make_mock_stash()\n\n        stash.get_scenes_with_fingerprints.return_value = (\n            [make_scene(\"42\", \"My Scene\",\n                [{\"type\": \"md5\", \"value\": \"abc123\"}],\n                stash_ids=[{\"endpoint\": \"https://stashdb.org/graphql\", \"stash_id\": \"existing-uuid\"}],\n            )],\n            1,\n        )\n\n        with patch(\"analyzers.scene_fingerprint_match.StashBoxClient\") as MockSBC:\n            mock_sbc = AsyncMock()\n            mock_sbc.find_scenes_by_fingerprints.return_value = [[]]\n            MockSBC.return_value = mock_sbc\n\n            analyzer = SceneFingerprintMatchAnalyzer(stash, db)\n            result = await analyzer.run(incremental=False)\n\n        assert result.recommendations_created == 0\n        # Should not even call fingerprint lookup for already-linked scenes\n        mock_sbc.find_scenes_by_fingerprints.assert_not_called()\n\n    @pytest.mark.asyncio\n    async def test_skips_dismissed_pair(self):\n        \"\"\"Dismissed (scene, stashbox_scene) pair should be skipped.\"\"\"\n        db = make_mock_db()\n        # This specific pair is dismissed\n        db.is_dismissed.return_value = True\n        stash = make_mock_stash()\n\n        stash.get_scenes_with_fingerprints.return_value = (\n            [make_scene(\"42\", \"My Scene\", [{\"type\": \"md5\", \"value\": \"abc123\"}])],\n            1,\n        )\n\n        stashbox_match = make_stashbox_match(\n            \"sb-uuid-1\", \"Matched Scene\",\n            [{\"hash\": \"abc123\", \"algorithm\": \"MD5\", \"duration\": 1800, \"submissions\": 5, \"created\": \"2024-01-01\", \"updated\": \"2024-06-01\"}],\n        )\n\n        with patch(\"analyzers.scene_fingerprint_match.StashBoxClient\") as MockSBC:\n            mock_sbc = AsyncMock()\n            mock_sbc.find_scenes_by_fingerprints.return_value = [[stashbox_match]]\n            MockSBC.return_value = mock_sbc\n\n            analyzer = SceneFingerprintMatchAnalyzer(stash, db)\n            result = await analyzer.run(incremental=False)\n\n        assert result.recommendations_created == 0\n        db.create_recommendation.assert_not_called()\n\n    @pytest.mark.asyncio\n    async def test_no_stashbox_connections_returns_zero(self):\n        \"\"\"If no stash-box endpoints configured, analyzer exits cleanly.\"\"\"\n        db = make_mock_db()\n        stash = make_mock_stash()\n        stash.get_stashbox_connections.return_value = []\n\n        analyzer = SceneFingerprintMatchAnalyzer(stash, db)\n        result = await analyzer.run()\n\n        assert result.items_processed == 0\n        assert result.recommendations_created == 0\n\n    @pytest.mark.asyncio\n    async def test_ambiguous_matches_not_high_confidence(self):\n        \"\"\"Multiple matches for same scene from same endpoint = not high confidence.\"\"\"\n        db = make_mock_db()\n        stash = make_mock_stash()\n\n        stash.get_scenes_with_fingerprints.return_value = (\n            [make_scene(\"42\", \"My Scene\", [\n                {\"type\": \"md5\", \"value\": \"abc123\"},\n                {\"type\": \"oshash\", \"value\": \"def456\"},\n            ])],\n            1,\n        )\n\n        match_a = make_stashbox_match(\"sb-1\", \"Match A\", [\n            {\"hash\": \"abc123\", \"algorithm\": \"MD5\", \"duration\": 1800, \"submissions\": 3, \"created\": \"2024-01-01\", \"updated\": \"2024-06-01\"},\n            {\"hash\": \"def456\", \"algorithm\": \"OSHASH\", \"duration\": 1800, \"submissions\": 2, \"created\": \"2024-01-01\", \"updated\": \"2024-06-01\"},\n        ])\n        match_b = make_stashbox_match(\"sb-2\", \"Match B\", [\n            {\"hash\": \"abc123\", \"algorithm\": \"MD5\", \"duration\": 1800, \"submissions\": 1, \"created\": \"2024-01-01\", \"updated\": \"2024-06-01\"},\n        ])\n\n        with patch(\"analyzers.scene_fingerprint_match.StashBoxClient\") as MockSBC:\n            mock_sbc = AsyncMock()\n            mock_sbc.find_scenes_by_fingerprints.return_value = [[match_a, match_b]]\n            MockSBC.return_value = mock_sbc\n\n            analyzer = SceneFingerprintMatchAnalyzer(stash, db)\n            result = await analyzer.run(incremental=False)\n\n        assert result.recommendations_created == 2\n        # Both recs created, but check high_confidence is False for all\n        for call in db.create_recommendation.call_args_list:\n            details = call[1][\"details\"]\n            assert details[\"high_confidence\"] is False\n</code></pre> <p>Step 2: Run tests to verify they fail</p> <p>Run: <code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -m pytest tests/test_scene_fingerprint_match_analyzer.py -v</code> Expected: FAIL with <code>ModuleNotFoundError: No module named 'analyzers.scene_fingerprint_match'</code></p> <p>Step 3: Write the implementation</p> <pre><code># api/analyzers/scene_fingerprint_match.py\n\"\"\"Analyzer: match local scenes to stash-box entries via fingerprints.\n\nExtends BaseAnalyzer (not BaseUpstreamAnalyzer) because this finds\nunlinked scenes rather than diffing already-linked entities.\n\"\"\"\n\nimport logging\nfrom typing import Optional\n\nfrom .base import BaseAnalyzer, AnalysisResult\nfrom scene_fingerprint_scoring import score_match, is_high_confidence\nfrom stashbox_client import StashBoxClient\n\nlogger = logging.getLogger(__name__)\n\n# Max scenes per stash-box batch query (matches Stash tagger batching)\nBATCH_SIZE = 40\n\n\nclass SceneFingerprintMatchAnalyzer(BaseAnalyzer):\n    type = \"scene_fingerprint_match\"\n\n    async def run(self, incremental: bool = True) -&gt; AnalysisResult:\n        connections = await self.stash.get_stashbox_connections()\n        if not connections:\n            return AnalysisResult(items_processed=0, recommendations_created=0)\n\n        # Load user-configurable thresholds\n        min_count = self._get_setting(\"scene_fp_min_count\", 2)\n        min_percentage = self._get_setting(\"scene_fp_min_percentage\", 66)\n\n        total_processed = 0\n        total_created = 0\n\n        for conn in connections:\n            endpoint = conn[\"endpoint\"]\n            api_key = conn.get(\"api_key\", \"\")\n            endpoint_name = conn.get(\"name\", endpoint)\n\n            processed, created = await self._process_endpoint(\n                endpoint, api_key, endpoint_name,\n                incremental, min_count, min_percentage,\n            )\n            total_processed += processed\n            total_created += created\n\n        return AnalysisResult(\n            items_processed=total_processed,\n            recommendations_created=total_created,\n        )\n\n    async def _process_endpoint(\n        self,\n        endpoint: str,\n        api_key: str,\n        endpoint_name: str,\n        incremental: bool,\n        min_count: int,\n        min_percentage: int,\n    ) -&gt; tuple[int, int]:\n        \"\"\"Process one stash-box endpoint. Returns (processed, created).\"\"\"\n        watermark_key = f\"scene_fp_match_{endpoint}\"\n        watermark = self.rec_db.get_watermark(watermark_key) if incremental else None\n\n        # Fetch all local scenes with fingerprint data\n        scenes_needing_match = []\n        offset = 0\n        latest_updated = watermark\n\n        while True:\n            scenes, total = await self.stash.get_scenes_with_fingerprints(\n                updated_after=watermark, limit=100, offset=offset,\n            )\n            if not scenes:\n                break\n\n            for scene in scenes:\n                # Track latest updated_at for watermark\n                updated_at = scene.get(\"updated_at\")\n                if updated_at and (latest_updated is None or updated_at &gt; latest_updated):\n                    latest_updated = updated_at\n\n                # Skip scenes already linked to this endpoint\n                linked_endpoints = {\n                    sid[\"endpoint\"] for sid in (scene.get(\"stash_ids\") or [])\n                }\n                if endpoint in linked_endpoints:\n                    continue\n\n                # Collect fingerprints from all files\n                fingerprints = []\n                duration = None\n                for f in scene.get(\"files\") or []:\n                    if duration is None and f.get(\"duration\"):\n                        duration = f[\"duration\"]\n                    for fp in f.get(\"fingerprints\") or []:\n                        fingerprints.append({\n                            \"hash\": fp[\"value\"],\n                            \"algorithm\": fp[\"type\"].upper(),\n                        })\n\n                if fingerprints:\n                    scenes_needing_match.append({\n                        \"scene\": scene,\n                        \"fingerprints\": fingerprints,\n                        \"duration\": duration,\n                    })\n\n            offset += len(scenes)\n            if offset &gt;= total:\n                break\n\n        if not scenes_needing_match:\n            if latest_updated:\n                self.rec_db.set_watermark(watermark_key, latest_updated)\n            return 0, 0\n\n        # Batch query stash-box\n        stashbox = StashBoxClient(endpoint, api_key)\n        created = 0\n\n        for batch_start in range(0, len(scenes_needing_match), BATCH_SIZE):\n            batch = scenes_needing_match[batch_start:batch_start + BATCH_SIZE]\n            fp_sets = [item[\"fingerprints\"] for item in batch]\n\n            results = await stashbox.find_scenes_by_fingerprints(fp_sets)\n\n            for i, matches in enumerate(results):\n                item = batch[i]\n                scene = item[\"scene\"]\n                local_fps = item[\"fingerprints\"]\n                local_duration = item[\"duration\"]\n                is_ambiguous = len(matches) &gt; 1\n\n                for match in matches:\n                    # Build composite target_id for pair-based dismissal\n                    target_id = f\"{scene['id']}|{endpoint}|{match['id']}\"\n\n                    if self.is_dismissed(\"scene\", target_id):\n                        continue\n\n                    # Find which local fingerprints matched this stash-box scene\n                    local_hashes = {fp[\"hash\"] for fp in local_fps}\n                    matching_fps = [\n                        fp for fp in match.get(\"fingerprints\", [])\n                        if fp[\"hash\"] in local_hashes\n                    ]\n\n                    score_result = score_match(\n                        matching_fingerprints=matching_fps,\n                        total_local_fingerprints=len(local_fps),\n                        local_duration=local_duration or 0,\n                    )\n\n                    high_conf = (\n                        not is_ambiguous\n                        and is_high_confidence(\n                            score_result[\"match_count\"],\n                            score_result[\"match_percentage\"],\n                            min_count=min_count,\n                            min_percentage=min_percentage,\n                        )\n                    )\n\n                    performers = [\n                        p[\"performer\"][\"name\"]\n                        for p in (match.get(\"performers\") or [])\n                        if p.get(\"performer\")\n                    ]\n                    studio = match.get(\"studio\")\n                    images = match.get(\"images\") or []\n\n                    details = {\n                        \"local_scene_id\": scene[\"id\"],\n                        \"local_scene_title\": scene.get(\"title\") or f\"Scene {scene['id']}\",\n                        \"endpoint\": endpoint,\n                        \"endpoint_name\": endpoint_name,\n                        \"stashbox_scene_id\": match[\"id\"],\n                        \"stashbox_scene_title\": match.get(\"title\"),\n                        \"stashbox_studio\": studio.get(\"name\") if studio else None,\n                        \"stashbox_performers\": performers,\n                        \"stashbox_date\": match.get(\"date\"),\n                        \"stashbox_cover_url\": images[0][\"url\"] if images else None,\n                        \"matching_fingerprints\": matching_fps,\n                        \"total_local_fingerprints\": len(local_fps),\n                        \"match_count\": score_result[\"match_count\"],\n                        \"match_percentage\": score_result[\"match_percentage\"],\n                        \"has_exact_hash\": score_result[\"has_exact_hash\"],\n                        \"duration_local\": local_duration,\n                        \"duration_remote\": match.get(\"duration\"),\n                        \"duration_agreement\": score_result[\"duration_agreement\"],\n                        \"duration_diff\": score_result[\"duration_diff\"],\n                        \"total_submissions\": score_result[\"total_submissions\"],\n                        \"high_confidence\": high_conf,\n                    }\n\n                    confidence = score_result[\"match_percentage\"] / 100.0\n\n                    rec_id = self.create_recommendation(\n                        target_type=\"scene\",\n                        target_id=target_id,\n                        details=details,\n                        confidence=confidence,\n                    )\n                    if rec_id:\n                        created += 1\n\n        processed = len(scenes_needing_match)\n        self.update_progress(processed, created)\n\n        if latest_updated:\n            self.rec_db.set_watermark(watermark_key, latest_updated)\n\n        return processed, created\n\n    def _get_setting(self, key: str, default):\n        \"\"\"Read a user setting with fallback.\"\"\"\n        val = self.rec_db.get_user_setting(key)\n        if val is None:\n            return default\n        try:\n            return int(val)\n        except (ValueError, TypeError):\n            return default\n</code></pre> <p>Step 4: Run tests to verify they pass</p> <p>Run: <code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -m pytest tests/test_scene_fingerprint_match_analyzer.py -v</code> Expected: All PASS</p> <p>Step 5: Commit</p> <pre><code>git add api/analyzers/scene_fingerprint_match.py api/tests/test_scene_fingerprint_match_analyzer.py\ngit commit -m \"feat: implement SceneFingerprintMatchAnalyzer (#48)\"\n</code></pre>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-implementation/#task-5-job-registration-and-analyzer-registry","title":"Task 5: Job Registration and Analyzer Registry","text":"<p>Wire the analyzer into the job queue and analyzer registry.</p> <p>Files: - Modify: <code>api/job_models.py</code> (add registration) - Modify: <code>api/recommendations_router.py</code> (add to ANALYZERS dict)</p> <p>Step 1: Add job registration</p> <p>Add to <code>api/job_models.py</code> after the existing <code>_register</code> calls (after the <code>upstream_scene_changes</code> block):</p> <pre><code>_register(\n    \"scene_fingerprint_match\",\n    \"Scene Fingerprint Matching\",\n    \"Match local scenes to stash-box entries via fingerprints\",\n    ResourceType.NETWORK,\n    JobPriority.NORMAL,\n    supports_incremental=True,\n    schedulable=True,\n    default_interval_hours=168,\n    allowed_intervals=(\n        (24, \"Daily\"),\n        (72, \"Every 3 days\"),\n        (168, \"Weekly\"),\n        (336, \"Every 2 weeks\"),\n    ),\n)\n</code></pre> <p>Step 2: Add to analyzer registry</p> <p>In <code>api/recommendations_router.py</code>, add the import at the top with the other analyzer imports:</p> <pre><code>from analyzers.scene_fingerprint_match import SceneFingerprintMatchAnalyzer\n</code></pre> <p>Add to the <code>ANALYZERS</code> dict:</p> <pre><code>\"scene_fingerprint_match\": SceneFingerprintMatchAnalyzer,\n</code></pre> <p>Step 3: Verify sidecar starts</p> <p>Run: <code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -c \"from job_models import JOB_REGISTRY; assert 'scene_fingerprint_match' in JOB_REGISTRY; print('OK')\"</code> Expected: <code>OK</code></p> <p>Run: <code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -c \"from recommendations_router import ANALYZERS; assert 'scene_fingerprint_match' in ANALYZERS; print('OK')\"</code> Expected: <code>OK</code></p> <p>Step 4: Commit</p> <pre><code>git add api/job_models.py api/recommendations_router.py\ngit commit -m \"feat: register scene fingerprint match job and analyzer (#48)\"\n</code></pre>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-implementation/#task-6-router-accept-fingerprint-match-endpoint","title":"Task 6: Router - Accept Fingerprint Match Endpoint","text":"<p>Add API endpoint to accept a single fingerprint match (adds stash_id to local scene).</p> <p>Files: - Modify: <code>api/recommendations_router.py</code> (add endpoint + request model) - Add test to: <code>api/tests/test_scene_fingerprint_match_analyzer.py</code></p> <p>Step 1: Write the failing test</p> <p>Add to <code>api/tests/test_scene_fingerprint_match_analyzer.py</code>:</p> <pre><code>class TestAcceptAction:\n    \"\"\"Test the accept fingerprint match action logic.\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_accept_adds_stash_id_to_scene(self):\n        \"\"\"Accepting a match should add the stash_id to the local scene.\"\"\"\n        from recommendations_router import _accept_fingerprint_match\n\n        mock_stash = AsyncMock()\n        mock_stash.get_scene_by_id.return_value = {\n            \"id\": \"42\",\n            \"stash_ids\": [\n                {\"endpoint\": \"https://other.org/graphql\", \"stash_id\": \"other-uuid\"},\n            ],\n        }\n        mock_stash.update_scene.return_value = {\"id\": \"42\"}\n\n        mock_db = MagicMock()\n        mock_db.get_recommendation.return_value = MagicMock(\n            id=1, type=\"scene_fingerprint_match\", status=\"pending\"\n        )\n        mock_db.resolve_recommendation.return_value = True\n\n        await _accept_fingerprint_match(\n            stash=mock_stash,\n            db=mock_db,\n            rec_id=1,\n            scene_id=\"42\",\n            endpoint=\"https://stashdb.org/graphql\",\n            stash_id=\"sb-uuid-1\",\n        )\n\n        # Verify stash_ids includes both old and new\n        call_kwargs = mock_stash.update_scene.call_args[1]\n        stash_ids = call_kwargs[\"stash_ids\"]\n        assert len(stash_ids) == 2\n        assert {\"endpoint\": \"https://stashdb.org/graphql\", \"stash_id\": \"sb-uuid-1\"} in stash_ids\n\n        # Verify recommendation was resolved\n        mock_db.resolve_recommendation.assert_called_once_with(1, action=\"accepted\")\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -m pytest tests/test_scene_fingerprint_match_analyzer.py::TestAcceptAction -v</code> Expected: FAIL with <code>ImportError: cannot import name '_accept_fingerprint_match'</code></p> <p>Step 3: Write the implementation</p> <p>Add to <code>api/recommendations_router.py</code>:</p> <pre><code>class AcceptFingerprintMatchRequest(BaseModel):\n    recommendation_id: int\n    scene_id: str\n    endpoint: str\n    stash_id: str\n\n\nasync def _accept_fingerprint_match(\n    stash, db, rec_id: int, scene_id: str, endpoint: str, stash_id: str,\n):\n    \"\"\"Accept a fingerprint match: add stash_id to local scene, resolve rec.\"\"\"\n    # Get current scene stash_ids\n    scene = await stash.get_scene_by_id(scene_id)\n    existing_stash_ids = scene.get(\"stash_ids\") or []\n\n    # Append new stash_id (avoid duplicates)\n    new_entry = {\"endpoint\": endpoint, \"stash_id\": stash_id}\n    already_linked = any(\n        s[\"endpoint\"] == endpoint and s[\"stash_id\"] == stash_id\n        for s in existing_stash_ids\n    )\n    if not already_linked:\n        updated_stash_ids = existing_stash_ids + [new_entry]\n        await stash.update_scene(scene_id, stash_ids=updated_stash_ids)\n\n    # Resolve the recommendation\n    db.resolve_recommendation(rec_id, action=\"accepted\")\n\n\n@router.post(\"/actions/accept-fingerprint-match\")\nasync def accept_fingerprint_match(request: AcceptFingerprintMatchRequest):\n    stash = get_stash_client()\n    db = get_rec_db()\n    await _accept_fingerprint_match(\n        stash, db,\n        rec_id=request.recommendation_id,\n        scene_id=request.scene_id,\n        endpoint=request.endpoint,\n        stash_id=request.stash_id,\n    )\n    return {\"success\": True}\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -m pytest tests/test_scene_fingerprint_match_analyzer.py::TestAcceptAction -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add api/recommendations_router.py api/tests/test_scene_fingerprint_match_analyzer.py\ngit commit -m \"feat: add accept-fingerprint-match API endpoint (#48)\"\n</code></pre>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-implementation/#task-7-router-accept-all-fingerprint-matches-endpoint","title":"Task 7: Router - Accept All Fingerprint Matches Endpoint","text":"<p>Batch accept all high-confidence matches, optionally filtered by endpoint.</p> <p>Files: - Modify: <code>api/recommendations_router.py</code> (add endpoint) - Add test to: <code>api/tests/test_scene_fingerprint_match_analyzer.py</code></p> <p>Step 1: Write the failing test</p> <p>Add to <code>api/tests/test_scene_fingerprint_match_analyzer.py</code>:</p> <pre><code>class TestAcceptAllAction:\n    \"\"\"Test the accept-all fingerprint matches action.\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_accepts_only_high_confidence(self):\n        \"\"\"Only high_confidence recommendations should be accepted.\"\"\"\n        from recommendations_router import _accept_all_fingerprint_matches\n        from recommendations_db import Recommendation\n\n        high_conf_rec = Recommendation(\n            id=1, type=\"scene_fingerprint_match\", status=\"pending\",\n            target_type=\"scene\", target_id=\"42|https://stashdb.org/graphql|sb-1\",\n            details={\n                \"local_scene_id\": \"42\",\n                \"endpoint\": \"https://stashdb.org/graphql\",\n                \"stashbox_scene_id\": \"sb-1\",\n                \"high_confidence\": True,\n            },\n            resolution_action=None, resolution_details=None, resolved_at=None,\n            confidence=0.67, source_analysis_id=None,\n            created_at=\"2026-01-01\", updated_at=\"2026-01-01\",\n        )\n        low_conf_rec = Recommendation(\n            id=2, type=\"scene_fingerprint_match\", status=\"pending\",\n            target_type=\"scene\", target_id=\"43|https://stashdb.org/graphql|sb-2\",\n            details={\n                \"local_scene_id\": \"43\",\n                \"endpoint\": \"https://stashdb.org/graphql\",\n                \"stashbox_scene_id\": \"sb-2\",\n                \"high_confidence\": False,\n            },\n            resolution_action=None, resolution_details=None, resolved_at=None,\n            confidence=0.33, source_analysis_id=None,\n            created_at=\"2026-01-01\", updated_at=\"2026-01-01\",\n        )\n\n        mock_db = MagicMock()\n        mock_db.get_recommendations.return_value = [high_conf_rec, low_conf_rec]\n        mock_db.resolve_recommendation.return_value = True\n\n        mock_stash = AsyncMock()\n        mock_stash.get_scene_by_id.return_value = {\"id\": \"42\", \"stash_ids\": []}\n        mock_stash.update_scene.return_value = {\"id\": \"42\"}\n\n        accepted = await _accept_all_fingerprint_matches(mock_stash, mock_db)\n\n        assert accepted == 1\n        mock_db.resolve_recommendation.assert_called_once_with(1, action=\"accepted\")\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -m pytest tests/test_scene_fingerprint_match_analyzer.py::TestAcceptAllAction -v</code> Expected: FAIL with <code>ImportError: cannot import name '_accept_all_fingerprint_matches'</code></p> <p>Step 3: Write the implementation</p> <p>Add to <code>api/recommendations_router.py</code>:</p> <pre><code>class AcceptAllFingerprintMatchesRequest(BaseModel):\n    endpoint: Optional[str] = None\n\n\nasync def _accept_all_fingerprint_matches(\n    stash, db, endpoint: Optional[str] = None,\n) -&gt; int:\n    \"\"\"Accept all high-confidence fingerprint matches. Returns count accepted.\"\"\"\n    recs = db.get_recommendations(\n        status=\"pending\", type=\"scene_fingerprint_match\", limit=10000,\n    )\n\n    accepted = 0\n    for rec in recs:\n        details = rec.details or {}\n        if not details.get(\"high_confidence\"):\n            continue\n        if endpoint and details.get(\"endpoint\") != endpoint:\n            continue\n\n        try:\n            await _accept_fingerprint_match(\n                stash, db,\n                rec_id=rec.id,\n                scene_id=details[\"local_scene_id\"],\n                endpoint=details[\"endpoint\"],\n                stash_id=details[\"stashbox_scene_id\"],\n            )\n            accepted += 1\n        except Exception as e:\n            logger.warning(\"Failed to accept rec %s: %s\", rec.id, e)\n\n    return accepted\n\n\n@router.post(\"/actions/accept-all-fingerprint-matches\")\nasync def accept_all_fingerprint_matches(\n    request: AcceptAllFingerprintMatchesRequest = AcceptAllFingerprintMatchesRequest(),\n):\n    stash = get_stash_client()\n    db = get_rec_db()\n    accepted = await _accept_all_fingerprint_matches(stash, db, request.endpoint)\n    return {\"success\": True, \"accepted_count\": accepted}\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -m pytest tests/test_scene_fingerprint_match_analyzer.py::TestAcceptAllAction -v</code> Expected: PASS</p> <p>Step 5: Run all tests</p> <p>Run: <code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -m pytest tests/test_scene_fingerprint_match_analyzer.py tests/test_scene_fingerprint_scoring.py tests/test_scene_fingerprint_query.py tests/test_stashbox_fingerprint_query.py -v</code> Expected: All PASS</p> <p>Step 6: Commit</p> <pre><code>git add api/recommendations_router.py api/tests/test_scene_fingerprint_match_analyzer.py\ngit commit -m \"feat: add accept-all-fingerprint-matches API endpoint (#48)\"\n</code></pre>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-implementation/#task-8-plugin-backend-proxy-modes","title":"Task 8: Plugin Backend Proxy Modes","text":"<p>Add proxy modes so the plugin JS can call the new sidecar endpoints.</p> <p>Files: - Modify: <code>plugin/stash_sense_backend.py</code></p> <p>Step 1: Add proxy handlers</p> <p>In <code>plugin/stash_sense_backend.py</code>, add handlers within the recommendations section (where other <code>rec_*</code> modes are handled). Find the pattern for existing <code>rec_</code> modes and add:</p> <pre><code>    elif mode == \"rec_accept_fingerprint_match\":\n        scene_id = input.get(\"scene_id\")\n        recommendation_id = input.get(\"recommendation_id\")\n        endpoint = input.get(\"endpoint\")\n        stash_id = input.get(\"stash_id\")\n        return sidecar_post(\n            base_url,\n            \"/recommendations/actions/accept-fingerprint-match\",\n            json={\n                \"recommendation_id\": recommendation_id,\n                \"scene_id\": scene_id,\n                \"endpoint\": endpoint,\n                \"stash_id\": stash_id,\n            },\n        )\n\n    elif mode == \"rec_accept_all_fingerprint_matches\":\n        endpoint = input.get(\"endpoint\")\n        payload = {}\n        if endpoint:\n            payload[\"endpoint\"] = endpoint\n        return sidecar_post(\n            base_url,\n            \"/recommendations/actions/accept-all-fingerprint-matches\",\n            json=payload,\n        )\n</code></pre> <p>Step 2: Test manually</p> <p>Deploy plugin to Stash and verify modes are reachable (or defer to integration testing after UI is built).</p> <p>Step 3: Commit</p> <pre><code>git add plugin/stash_sense_backend.py\ngit commit -m \"feat: add plugin proxy modes for fingerprint match actions (#48)\"\n</code></pre>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-implementation/#task-9-plugin-ui-dashboard-card","title":"Task 9: Plugin UI - Dashboard Card","text":"<p>Add the fingerprint match card to the recommendations dashboard.</p> <p>Files: - Modify: <code>plugin/stash-sense-recommendations.js</code> (API methods + dashboard card) - Modify: <code>plugin/stash-sense.css</code> (fingerprint card styling)</p> <p>Step 1: Add API methods</p> <p>Add to the <code>RecommendationsAPI</code> object in <code>stash-sense-recommendations.js</code>:</p> <pre><code>acceptFingerprintMatch: async function(recommendationId, sceneId, endpoint, stashId) {\n    return apiCall('rec_accept_fingerprint_match', {\n        recommendation_id: recommendationId,\n        scene_id: sceneId,\n        endpoint: endpoint,\n        stash_id: stashId,\n    });\n},\n\nacceptAllFingerprintMatches: async function(endpoint) {\n    return apiCall('rec_accept_all_fingerprint_matches', {\n        endpoint: endpoint || null,\n    });\n},\n</code></pre> <p>Step 2: Add dashboard card rendering</p> <p>Find the dashboard card rendering section (where other recommendation types are rendered as cards). Add a case for <code>scene_fingerprint_match</code>:</p> <pre><code>// Inside the dashboard card rendering logic, add:\nif (counts.scene_fingerprint_match) {\n    const fpCount = counts.scene_fingerprint_match;\n    const pending = fpCount.pending || 0;\n\n    // Count high-confidence from the pending recommendations\n    const card = document.createElement('div');\n    card.className = 'ss-rec-card ss-rec-card-fingerprint';\n    card.innerHTML = `\n        &lt;div class=\"ss-rec-card-header\"&gt;\n            &lt;span class=\"ss-rec-card-icon\"&gt;&amp;#128279;&lt;/span&gt;\n            &lt;span class=\"ss-rec-card-title\"&gt;Scene Fingerprint Matches&lt;/span&gt;\n        &lt;/div&gt;\n        &lt;div class=\"ss-rec-card-body\"&gt;\n            &lt;span class=\"ss-rec-card-count\"&gt;${pending}&lt;/span&gt;\n            &lt;span class=\"ss-rec-card-label\"&gt;pending matches&lt;/span&gt;\n        &lt;/div&gt;\n    `;\n    card.addEventListener('click', () =&gt; {\n        currentState.view = 'list';\n        currentState.type = 'scene_fingerprint_match';\n        currentState.status = 'pending';\n        currentState.page = 1;\n        renderCurrentView(container);\n    });\n    cardsContainer.appendChild(card);\n}\n</code></pre> <p>Step 3: Commit</p> <pre><code>git add plugin/stash-sense-recommendations.js plugin/stash-sense.css\ngit commit -m \"feat: add fingerprint match dashboard card and API methods (#48)\"\n</code></pre>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-implementation/#task-10-plugin-ui-list-view","title":"Task 10: Plugin UI - List View","text":"<p>Add card rendering for fingerprint match recommendations in the list view.</p> <p>Files: - Modify: <code>plugin/stash-sense-recommendations.js</code></p> <p>Step 1: Add list card renderer</p> <p>Find <code>renderRecommendationCard</code> and add a case for <code>scene_fingerprint_match</code>:</p> <pre><code>// Inside renderRecommendationCard, add case:\nif (rec.type === 'scene_fingerprint_match') {\n    const d = rec.details;\n    const matchColor = d.match_percentage &gt;= 100 ? '#28a745' :\n                       d.match_percentage &gt;= 66 ? '#ffc107' : '#dc3545';\n\n    card.innerHTML = `\n        &lt;div class=\"ss-rec-card-header\"&gt;\n            &lt;div class=\"ss-rec-card-title-row\"&gt;\n                &lt;span class=\"ss-rec-card-title\"&gt;${escapeHtml(d.local_scene_title)}&lt;/span&gt;\n                ${d.high_confidence ? '&lt;span class=\"ss-badge ss-badge-success\"&gt;High Confidence&lt;/span&gt;' : ''}\n            &lt;/div&gt;\n            &lt;span class=\"ss-rec-card-subtitle\"&gt;${escapeHtml(d.endpoint_name || d.endpoint)}&lt;/span&gt;\n        &lt;/div&gt;\n        &lt;div class=\"ss-fp-match-summary\"&gt;\n            &lt;div class=\"ss-fp-match-arrow\"&gt;\n                &lt;span class=\"ss-fp-local\"&gt;${escapeHtml(d.local_scene_title)}&lt;/span&gt;\n                &lt;span class=\"ss-fp-arrow\"&gt;&amp;rarr;&lt;/span&gt;\n                &lt;span class=\"ss-fp-remote\"&gt;${escapeHtml(d.stashbox_scene_title || 'Unknown')}&lt;/span&gt;\n            &lt;/div&gt;\n            &lt;div class=\"ss-fp-match-meta\"&gt;\n                ${d.stashbox_studio ? `&lt;span class=\"ss-fp-studio\"&gt;${escapeHtml(d.stashbox_studio)}&lt;/span&gt;` : ''}\n                ${d.stashbox_performers?.length ? `&lt;span class=\"ss-fp-performers\"&gt;${d.stashbox_performers.map(escapeHtml).join(', ')}&lt;/span&gt;` : ''}\n                ${d.stashbox_date ? `&lt;span class=\"ss-fp-date\"&gt;${d.stashbox_date}&lt;/span&gt;` : ''}\n            &lt;/div&gt;\n        &lt;/div&gt;\n        &lt;div class=\"ss-fp-match-badge\" style=\"color: ${matchColor}\"&gt;\n            ${d.match_count}/${d.total_local_fingerprints} fingerprints\n            ${d.has_exact_hash ? '(exact)' : '(phash)'}\n        &lt;/div&gt;\n    `;\n}\n</code></pre> <p>Step 2: Add Accept All button for fingerprint matches</p> <p>In the list view header area (where \"Accept All\" exists for upstream types), add support for <code>scene_fingerprint_match</code>:</p> <pre><code>// In the list view header, if type is scene_fingerprint_match and status is pending:\nif (currentState.type === 'scene_fingerprint_match' &amp;&amp; currentState.status === 'pending') {\n    const acceptAllBtn = document.createElement('button');\n    acceptAllBtn.className = 'ss-btn ss-btn-primary ss-btn-sm';\n    acceptAllBtn.textContent = 'Accept All High-Confidence';\n    acceptAllBtn.addEventListener('click', async () =&gt; {\n        acceptAllBtn.disabled = true;\n        acceptAllBtn.textContent = 'Accepting...';\n        try {\n            const result = await RecommendationsAPI.acceptAllFingerprintMatches();\n            acceptAllBtn.textContent = `Accepted ${result.accepted_count}!`;\n            acceptAllBtn.classList.add('ss-btn-success');\n            setTimeout(() =&gt; renderCurrentView(container), 1500);\n        } catch (e) {\n            acceptAllBtn.textContent = `Failed: ${e.message}`;\n            acceptAllBtn.classList.add('ss-btn-error');\n            acceptAllBtn.disabled = false;\n        }\n    });\n    headerRight.appendChild(acceptAllBtn);\n}\n</code></pre> <p>Step 3: Commit</p> <pre><code>git add plugin/stash-sense-recommendations.js\ngit commit -m \"feat: add fingerprint match list view with Accept All (#48)\"\n</code></pre>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-implementation/#task-11-plugin-ui-detail-view","title":"Task 11: Plugin UI - Detail View","text":"<p>Add detailed view for a single fingerprint match recommendation.</p> <p>Files: - Modify: <code>plugin/stash-sense-recommendations.js</code> - Modify: <code>plugin/stash-sense.css</code></p> <p>Step 1: Add detail renderer</p> <p>Find the detail view type dispatch (where <code>renderDuplicatePerformerDetail</code>, etc. are called) and add:</p> <pre><code>} else if (rec.type === 'scene_fingerprint_match') {\n    renderFingerprintMatchDetail(content, rec);\n}\n</code></pre> <p>Then add the renderer function:</p> <pre><code>function renderFingerprintMatchDetail(content, rec) {\n    const d = rec.details;\n    const isPending = rec.status === 'pending';\n\n    content.innerHTML = `\n        &lt;div class=\"ss-fp-detail\"&gt;\n            &lt;div class=\"ss-fp-detail-header\"&gt;\n                &lt;h3&gt;Scene Fingerprint Match&lt;/h3&gt;\n                &lt;span class=\"ss-badge ${d.high_confidence ? 'ss-badge-success' : 'ss-badge-warning'}\"&gt;\n                    ${d.high_confidence ? 'High Confidence' : 'Review Recommended'}\n                &lt;/span&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"ss-fp-detail-comparison\"&gt;\n                &lt;div class=\"ss-fp-detail-side\"&gt;\n                    &lt;h4&gt;Local Scene&lt;/h4&gt;\n                    &lt;div class=\"ss-fp-field\"&gt;&lt;strong&gt;Title:&lt;/strong&gt; ${escapeHtml(d.local_scene_title)}&lt;/div&gt;\n                    &lt;div class=\"ss-fp-field\"&gt;&lt;strong&gt;Duration:&lt;/strong&gt; ${d.duration_local ? formatDuration(d.duration_local) : 'N/A'}&lt;/div&gt;\n                    &lt;div class=\"ss-fp-field\"&gt;&lt;strong&gt;Fingerprints:&lt;/strong&gt; ${d.total_local_fingerprints}&lt;/div&gt;\n                &lt;/div&gt;\n                &lt;div class=\"ss-fp-detail-divider\"&gt;&lt;/div&gt;\n                &lt;div class=\"ss-fp-detail-side\"&gt;\n                    &lt;h4&gt;Stash-Box Match&lt;/h4&gt;\n                    &lt;div class=\"ss-fp-field\"&gt;&lt;strong&gt;Title:&lt;/strong&gt; ${escapeHtml(d.stashbox_scene_title || 'Unknown')}&lt;/div&gt;\n                    ${d.stashbox_studio ? `&lt;div class=\"ss-fp-field\"&gt;&lt;strong&gt;Studio:&lt;/strong&gt; ${escapeHtml(d.stashbox_studio)}&lt;/div&gt;` : ''}\n                    ${d.stashbox_performers?.length ? `&lt;div class=\"ss-fp-field\"&gt;&lt;strong&gt;Performers:&lt;/strong&gt; ${d.stashbox_performers.map(escapeHtml).join(', ')}&lt;/div&gt;` : ''}\n                    ${d.stashbox_date ? `&lt;div class=\"ss-fp-field\"&gt;&lt;strong&gt;Date:&lt;/strong&gt; ${d.stashbox_date}&lt;/div&gt;` : ''}\n                    &lt;div class=\"ss-fp-field\"&gt;&lt;strong&gt;Duration:&lt;/strong&gt; ${d.duration_remote ? formatDuration(d.duration_remote) : 'N/A'}&lt;/div&gt;\n                    &lt;div class=\"ss-fp-field\"&gt;&lt;strong&gt;Endpoint:&lt;/strong&gt; ${escapeHtml(d.endpoint_name || d.endpoint)}&lt;/div&gt;\n                &lt;/div&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"ss-fp-detail-fingerprints\"&gt;\n                &lt;h4&gt;Fingerprint Comparison&lt;/h4&gt;\n                &lt;div class=\"ss-fp-table-wrap\"&gt;\n                    &lt;table class=\"ss-fp-table\"&gt;\n                        &lt;thead&gt;\n                            &lt;tr&gt;\n                                &lt;th&gt;Algorithm&lt;/th&gt;\n                                &lt;th&gt;Hash&lt;/th&gt;\n                                &lt;th&gt;Duration&lt;/th&gt;\n                                &lt;th&gt;Submissions&lt;/th&gt;\n                            &lt;/tr&gt;\n                        &lt;/thead&gt;\n                        &lt;tbody&gt;\n                            ${(d.matching_fingerprints || []).map(fp =&gt; `\n                                &lt;tr&gt;\n                                    &lt;td&gt;&lt;span class=\"ss-badge ss-badge-${fp.algorithm === 'PHASH' ? 'warning' : 'success'}\"&gt;${fp.algorithm}&lt;/span&gt;&lt;/td&gt;\n                                    &lt;td class=\"ss-fp-hash\"&gt;${fp.hash}&lt;/td&gt;\n                                    &lt;td&gt;${fp.duration ? formatDuration(fp.duration) : 'N/A'}&lt;/td&gt;\n                                    &lt;td&gt;${fp.submissions || 0}&lt;/td&gt;\n                                &lt;/tr&gt;\n                            `).join('')}\n                        &lt;/tbody&gt;\n                    &lt;/table&gt;\n                &lt;/div&gt;\n                &lt;div class=\"ss-fp-summary-line\"&gt;\n                    &lt;strong&gt;${d.match_count}/${d.total_local_fingerprints}&lt;/strong&gt; fingerprints match\n                    (${d.match_percentage}%)\n                    ${d.duration_agreement ? '&amp;mdash; duration agrees' : '&amp;mdash; &lt;span class=\"ss-text-warning\"&gt;duration mismatch&lt;/span&gt;'}\n                &lt;/div&gt;\n            &lt;/div&gt;\n\n            ${isPending ? `\n                &lt;div class=\"ss-fp-detail-actions\"&gt;\n                    &lt;button id=\"ss-fp-accept-btn\" class=\"ss-btn ss-btn-primary\"&gt;Accept Match&lt;/button&gt;\n                    &lt;button id=\"ss-fp-dismiss-btn\" class=\"ss-btn ss-btn-secondary\"&gt;Dismiss&lt;/button&gt;\n                &lt;/div&gt;\n            ` : `\n                &lt;div class=\"ss-fp-detail-status\"&gt;\n                    Status: &lt;strong&gt;${rec.status}&lt;/strong&gt;\n                    ${rec.resolution_action ? ` (${rec.resolution_action})` : ''}\n                &lt;/div&gt;\n            `}\n        &lt;/div&gt;\n    `;\n\n    if (!isPending) return;\n\n    // Accept button\n    const acceptBtn = content.querySelector('#ss-fp-accept-btn');\n    acceptBtn.addEventListener('click', async () =&gt; {\n        acceptBtn.disabled = true;\n        acceptBtn.textContent = 'Accepting...';\n        try {\n            await RecommendationsAPI.acceptFingerprintMatch(\n                rec.id, d.local_scene_id, d.endpoint, d.stashbox_scene_id\n            );\n            acceptBtn.textContent = 'Accepted!';\n            acceptBtn.classList.add('ss-btn-success');\n            setTimeout(() =&gt; {\n                currentState.view = 'list';\n                renderCurrentView(document.getElementById('ss-recommendations'));\n            }, 1500);\n        } catch (e) {\n            acceptBtn.textContent = `Failed: ${e.message}`;\n            acceptBtn.classList.add('ss-btn-error');\n            acceptBtn.disabled = false;\n        }\n    });\n\n    // Dismiss button\n    const dismissBtn = content.querySelector('#ss-fp-dismiss-btn');\n    dismissBtn.addEventListener('click', async () =&gt; {\n        dismissBtn.disabled = true;\n        dismissBtn.textContent = 'Dismissing...';\n        try {\n            await RecommendationsAPI.dismiss(rec.id);\n            dismissBtn.textContent = 'Dismissed!';\n            dismissBtn.classList.add('ss-btn-success');\n            setTimeout(() =&gt; {\n                currentState.view = 'list';\n                renderCurrentView(document.getElementById('ss-recommendations'));\n            }, 1500);\n        } catch (e) {\n            dismissBtn.textContent = `Failed: ${e.message}`;\n            dismissBtn.classList.add('ss-btn-error');\n            dismissBtn.disabled = false;\n        }\n    });\n}\n\nfunction formatDuration(seconds) {\n    if (!seconds &amp;&amp; seconds !== 0) return 'N/A';\n    const m = Math.floor(seconds / 60);\n    const s = Math.floor(seconds % 60);\n    return `${m}:${s.toString().padStart(2, '0')}`;\n}\n</code></pre> <p>Step 2: Add CSS</p> <p>Add to <code>plugin/stash-sense.css</code>:</p> <pre><code>/* Scene Fingerprint Match Detail */\n.ss-fp-detail-comparison {\n    display: flex;\n    gap: 1rem;\n    margin: 1rem 0;\n}\n\n.ss-fp-detail-side {\n    flex: 1;\n    padding: 1rem;\n    background: var(--bs-body-bg);\n    border-radius: 0.5rem;\n}\n\n.ss-fp-detail-divider {\n    width: 2px;\n    background: var(--bs-border-color);\n}\n\n.ss-fp-field {\n    margin: 0.4rem 0;\n    font-size: 0.9rem;\n}\n\n.ss-fp-table {\n    width: 100%;\n    border-collapse: collapse;\n    margin: 0.5rem 0;\n}\n\n.ss-fp-table th,\n.ss-fp-table td {\n    padding: 0.5rem;\n    text-align: left;\n    border-bottom: 1px solid var(--bs-border-color);\n}\n\n.ss-fp-hash {\n    font-family: monospace;\n    font-size: 0.8rem;\n    max-width: 200px;\n    overflow: hidden;\n    text-overflow: ellipsis;\n}\n\n.ss-fp-summary-line {\n    margin-top: 0.5rem;\n    font-size: 0.9rem;\n}\n\n.ss-fp-detail-actions {\n    display: flex;\n    gap: 0.5rem;\n    margin-top: 1rem;\n    padding-top: 1rem;\n    border-top: 1px solid var(--bs-border-color);\n}\n\n.ss-fp-detail-status {\n    margin-top: 1rem;\n    padding: 0.5rem;\n    background: var(--bs-body-bg);\n    border-radius: 0.25rem;\n}\n\n/* List view */\n.ss-fp-match-summary {\n    padding: 0.5rem 0;\n}\n\n.ss-fp-match-arrow {\n    display: flex;\n    align-items: center;\n    gap: 0.5rem;\n    font-size: 0.9rem;\n}\n\n.ss-fp-arrow {\n    color: var(--bs-secondary-color);\n}\n\n.ss-fp-match-meta {\n    display: flex;\n    gap: 0.75rem;\n    font-size: 0.8rem;\n    color: var(--bs-secondary-color);\n    margin-top: 0.25rem;\n}\n\n.ss-fp-match-badge {\n    font-weight: 600;\n    font-size: 0.85rem;\n}\n\n.ss-text-warning {\n    color: #ffc107;\n}\n</code></pre> <p>Step 3: Deploy and verify visually</p> <pre><code>scp plugin/stash-sense-recommendations.js plugin/stash-sense.css root@10.0.0.4:/mnt/nvme_cache/appdata/stash/config/plugins/stash-sense/\n</code></pre> <p>Hard refresh Stash UI (Ctrl+Shift+R) and verify the card, list, and detail views render correctly.</p> <p>Step 4: Commit</p> <pre><code>git add plugin/stash-sense-recommendations.js plugin/stash-sense.css\ngit commit -m \"feat: add fingerprint match detail view with accept/dismiss (#48)\"\n</code></pre>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-implementation/#task-12-integration-test-and-final-verification","title":"Task 12: Integration Test and Final Verification","text":"<p>End-to-end verification of the full pipeline.</p> <p>Files: - All previously modified files</p> <p>Step 1: Run the full test suite</p> <p><pre><code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -m pytest tests/ -v\n</code></pre> Expected: All tests pass, no regressions</p> <p>Step 2: Start sidecar and verify import</p> <p><pre><code>cd /home/carrot/code/stash-sense/api &amp;&amp; ../.venv/bin/python -c \"\nfrom job_models import JOB_REGISTRY\nfrom recommendations_router import ANALYZERS\nfrom analyzers.scene_fingerprint_match import SceneFingerprintMatchAnalyzer\nfrom scene_fingerprint_scoring import score_match, is_high_confidence\n\nassert 'scene_fingerprint_match' in JOB_REGISTRY\nassert 'scene_fingerprint_match' in ANALYZERS\nprint('All imports OK')\nprint(f'Job: {JOB_REGISTRY[\\\"scene_fingerprint_match\\\"].display_name}')\nprint(f'Analyzer: {ANALYZERS[\\\"scene_fingerprint_match\\\"].__name__}')\n\"\n</code></pre> Expected: All imports OK with correct names</p> <p>Step 3: Start sidecar and test endpoints</p> <pre><code>cd /home/carrot/code/stash-sense/api\nsource ../.venv/bin/activate\nmake sidecar\n</code></pre> <p>In a separate terminal, verify the new endpoints exist:</p> <pre><code>curl -s http://localhost:5000/docs | grep -o 'fingerprint[^\"]*' | head -5\n</code></pre> <p>Step 4: Commit any fixes, then final commit</p> <pre><code>git add -A\ngit status  # Review what's staged\ngit commit -m \"feat: scene fingerprint matching - integration verification (#48)\"\n</code></pre>"},{"location":"plans/2026-02-20-scene-fingerprint-matching-implementation/#implementation-checklist","title":"Implementation Checklist","text":"# Component File(s) Tests 1 Quality scoring <code>api/scene_fingerprint_scoring.py</code> <code>test_scene_fingerprint_scoring.py</code> 2 Stash client query <code>api/stash_client_unified.py</code> <code>test_scene_fingerprint_query.py</code> 3 StashBox batch lookup <code>api/stashbox_client.py</code> <code>test_stashbox_fingerprint_query.py</code> 4 Analyzer <code>api/analyzers/scene_fingerprint_match.py</code> <code>test_scene_fingerprint_match_analyzer.py</code> 5 Job registration <code>api/job_models.py</code>, <code>api/recommendations_router.py</code> Import verification 6 Accept endpoint <code>api/recommendations_router.py</code> <code>test_scene_fingerprint_match_analyzer.py</code> 7 Accept All endpoint <code>api/recommendations_router.py</code> <code>test_scene_fingerprint_match_analyzer.py</code> 8 Plugin proxy <code>plugin/stash_sense_backend.py</code> Manual 9 Dashboard card <code>plugin/stash-sense-recommendations.js</code> Visual 10 List view <code>plugin/stash-sense-recommendations.js</code> Visual 11 Detail view <code>plugin/stash-sense-recommendations.js</code>, <code>plugin/stash-sense.css</code> Visual 12 Integration All Full suite"},{"location":"plans/2026-02-20-scene-fingerprint-matching-implementation/#key-design-decisions","title":"Key Design Decisions","text":"<ul> <li>Composite <code>target_id</code>: <code>{scene_id}|{endpoint}|{stashbox_scene_id}</code> enables pair-based dismissals using the existing DB schema with zero changes to <code>RecommendationsDB</code></li> <li>Not using <code>BaseUpstreamAnalyzer</code>: This analyzer finds unlinked scenes, not diffs on linked ones</li> <li>Ambiguity rule: Multiple matches for one scene from same endpoint \u2192 none are <code>high_confidence</code></li> <li>Incremental watermark: Keyed per-endpoint as <code>scene_fp_match_{endpoint}</code> in watermark table</li> <li>Batch size 40: Matches Stash tagger's own batching to stash-box API</li> </ul>"},{"location":"research/face-recognition-quality-investigation/","title":"Face Recognition Quality Investigation","text":""},{"location":"research/face-recognition-quality-investigation/#date-2026-02-09","title":"Date: 2026-02-09","text":""},{"location":"research/face-recognition-quality-investigation/#implementation-status-complete-pending-database-rebuild","title":"Implementation Status: COMPLETE - Pending Database Rebuild","text":"<p>All critical fixes have been implemented in both the trainer and sidecar:</p> <ol> <li>Face alignment - Added InsightFace <code>norm_crop</code> 5-point similarity transform (trainer: <code>575525e</code>, sidecar: <code>66d9ef8</code>)</li> <li>ArcFace normalization - Fixed from <code>x/255</code> to <code>(x-127.5)/128</code> (trainer: <code>ce06344</code>, sidecar: <code>66d9ef8</code>)</li> <li>Flip-averaging - Added horizontal flip embedding averaging (trainer: <code>ce06344</code>, sidecar: <code>66d9ef8</code>)</li> <li>Aspect-ratio resize - Replaced stretching with aspect-ratio-preserving resize + black padding (trainer: <code>ce06344</code>, sidecar: <code>66d9ef8</code>)</li> <li>Manifest updated - Reflects actual pipeline models (trainer: <code>2975e31</code>)</li> <li>Nuke script - <code>python -m api.nuke_enrichment_data --yes</code> to clear all enrichment data (trainer: <code>259ebd3</code>)</li> </ol> <p>Next step: Run nuke script, then re-scrape with high-trust sources first (stashdb, theporndb), followed by reference sites and medium-trust sources. See plan's \"Post-Implementation: Database Rebuild Instructions\" section.</p>"},{"location":"research/face-recognition-quality-investigation/#problem-statement","title":"Problem Statement","text":"<p>Face recognition results are extremely poor. The system fails to correctly identify performers even in controlled conditions (still photos from galleries) where identification should be trivial. Example: Gallery 2139 (Holly Randall shoot featuring Jayden Jaymes) - the system thinks there are multiple different people and doesn't rank Jayden Jaymes highly, despite her having multiple face embeddings in the database.</p>"},{"location":"research/face-recognition-quality-investigation/#investigation-summary","title":"Investigation Summary","text":"<p>Root cause analysis traced the problem to two critical bugs in <code>embeddings.py</code> (shared between trainer and sidecar), plus several missing best practices that competitive systems implement.</p>"},{"location":"research/face-recognition-quality-investigation/#bug-1-no-face-alignment-primary-70-of-quality-loss","title":"Bug 1: No Face Alignment (PRIMARY - ~70% of quality loss)","text":""},{"location":"research/face-recognition-quality-investigation/#whats-happening","title":"What's happening","text":"<p>In both <code>stash-sense-trainer/api/embeddings.py</code> and <code>stash-sense/api/embeddings.py</code>, the <code>detect_faces()</code> method crops faces using a raw bounding box with zero alignment:</p> <pre><code># embeddings.py line 150 (trainer), line 150 (sidecar)\nface_img = image[y1:y2, x1:x2]  # Raw bounding box crop\n</code></pre>"},{"location":"research/face-recognition-quality-investigation/#what-should-happen","title":"What should happen","text":"<p>FaceNet512 and ArcFace were trained on aligned faces. Face alignment is a standard preprocessing step in face recognition where detected faces are rotated so that the eyes are horizontally level before cropping. This normalizes head pose and is critical for embedding consistency.</p> <p>DeepFace's own <code>extract_faces()</code> function does this by default (<code>align=True</code>):</p> <ol> <li>Detect eye landmarks</li> <li>Calculate rotation angle: <code>angle = degrees(arctan2(left_eye_y - right_eye_y, left_eye_x - right_eye_x))</code></li> <li>Apply 2D affine rotation via <code>cv2.warpAffine()</code> to make eyes horizontal</li> <li>Crop the aligned face</li> </ol> <p>Our code already has the landmark data - InsightFace's RetinaFace provides 5-point facial landmarks (<code>face.kps</code> containing left_eye, right_eye, nose, mouth_left, mouth_right). We store them in <code>DetectedFace.landmarks</code> but never use them for alignment.</p>"},{"location":"research/face-recognition-quality-investigation/#why-this-destroys-quality","title":"Why this destroys quality","text":"<p>Without alignment, a 15-degree head tilt produces embeddings that are significantly different from the same person with a level head. The models never learned to be invariant to rotation because their training data was always pre-aligned. This means:</p> <ul> <li>Different photos of the same person at different head angles produce widely scattered embeddings</li> <li>The \"centroid\" of a performer's embedding cluster is noisy and unstable</li> <li>Query-time embeddings from natural photos rarely land near the correct cluster</li> </ul>"},{"location":"research/face-recognition-quality-investigation/#evidence","title":"Evidence","text":"<p>This is a well-established requirement in face recognition literature. Every competitive system does alignment: - StashFace (cc1234): Uses DeepFace's <code>extract_faces()</code> which has <code>align=True</code> by default - FaceStash (kozobot): Uses the <code>face_recognition</code> library which internally uses dlib's alignment - DeepFace itself: Alignment is ON by default in all pipelines - InsightFace's own recognition pipeline: Uses landmark-based alignment before embedding</p>"},{"location":"research/face-recognition-quality-investigation/#bug-2-wrong-arcface-normalization-secondary-30-of-quality-loss","title":"Bug 2: Wrong ArcFace Normalization (SECONDARY - ~30% of quality loss)","text":""},{"location":"research/face-recognition-quality-investigation/#whats-happening_1","title":"What's happening","text":"<pre><code># embeddings.py line 206 (trainer), line 206 (sidecar)\narcface_input = arcface_input / 255.0  # Produces [0, 1] range - WRONG\n</code></pre>"},{"location":"research/face-recognition-quality-investigation/#what-should-happen_1","title":"What should happen","text":"<p>Per the ArcFace paper, DeepFace's own preprocessing code (<code>preprocessing.py:66-71</code>), and StashFace's implementation:</p> <pre><code>arcface_input = (arcface_input - 127.5) / 128  # Produces ~[-1, 1] range - CORRECT\n</code></pre> <p>The ArcFace model expects input pixels normalized to approximately <code>[-0.996, 0.996]</code> via <code>(x - 127.5) / 128</code>. Feeding <code>[0, 1]</code> range input shifts the entire activation distribution. The model still produces embeddings, but they're degraded - lower discriminative power.</p>"},{"location":"research/face-recognition-quality-investigation/#evidence_1","title":"Evidence","text":"<ul> <li>DeepFace source <code>preprocessing.py</code> line 66-71: <code>img -= 127.5; img /= 128</code> for ArcFace normalization</li> <li>ArcFace paper: \"each pixel in RGB images is normalised by subtracting 127.5 then divided by 128\"</li> <li>StashFace uses <code>preprocessing.normalize_input(resized, \"ArcFace\")</code> which applies this correct normalization</li> </ul>"},{"location":"research/face-recognition-quality-investigation/#additional-facenet-normalization-concern","title":"Additional FaceNet normalization concern","text":"<p>Our FaceNet normalization: <pre><code>facenet_input = (facenet_input - 127.5) / 127.5  # Our code\n</code></pre></p> <p>StashFace uses <code>\"Facenet2018\"</code> normalization which is: <pre><code>img /= 127.5  # Then img -= 1\n# Equivalent to: (img - 127.5) / 127.5\n</code></pre></p> <p>These are mathematically equivalent, so our FaceNet normalization is correct.</p>"},{"location":"research/face-recognition-quality-investigation/#additional-issues-found","title":"Additional Issues Found","text":""},{"location":"research/face-recognition-quality-investigation/#issue-3-no-aspect-ratio-preserving-resize","title":"Issue 3: No Aspect-Ratio-Preserving Resize","text":"<p>Our code resizes faces by stretching to the target size:</p> <pre><code>pil_img = pil_img.resize(target_size, Image.Resampling.BILINEAR)  # Non-preserving stretch\n</code></pre> <p>DeepFace and StashFace use aspect-ratio-preserving resize with black padding:</p> <pre><code># DeepFace preprocessing.resize_image()\nfactor = min(target_h / img_h, target_w / img_w)\nresized = cv2.resize(img, (int(img_w * factor), int(img_h * factor)))\n# Then pad with black to reach target_size\n</code></pre> <p>Impact: Moderate. Stretching distorts facial proportions slightly, adding noise to embeddings.</p>"},{"location":"research/face-recognition-quality-investigation/#issue-4-no-horizontal-flip-augmentation-at-query-time","title":"Issue 4: No Horizontal Flip Augmentation at Query Time","text":"<p>StashFace's key technique: For every detected face, it generates embeddings for BOTH the original and a horizontally flipped version, then averages them:</p> <pre><code>face_batch = np.stack([face, face[:, ::-1, :]], axis=0)  # Original + flipped\nembeddings_batch = ensemble.get_face_embeddings_batch(face_batch)\nfacenet = np.mean(embeddings_batch['facenet'], axis=0)  # Average\narc = np.mean(embeddings_batch['arc'], axis=0)           # Average\n</code></pre> <p>Why this helps: Faces are roughly symmetric. Averaging original + mirror embeddings: - Cancels out left-right asymmetries from lighting, expression, and slight pose - Produces a more \"canonical\" embedding closer to the center of the person's embedding cluster - Reduces the impact of minor alignment imperfections - Is essentially a free accuracy boost at the cost of 2x inference time</p>"},{"location":"research/face-recognition-quality-investigation/#issue-5-e4m3-quantized-storage-stashface-optimization","title":"Issue 5: E4M3 Quantized Storage (StashFace optimization)","text":"<p>StashFace uses <code>StorageDataType.E4M3</code> (8-bit float) for Voyager indices, reducing memory by 4x with minimal accuracy loss for cosine similarity. We use full 32-bit float. This is an optimization, not a bug - but worth considering for a 553K+ face database.</p>"},{"location":"research/face-recognition-quality-investigation/#issue-6-manifest-inconsistency","title":"Issue 6: Manifest Inconsistency","text":"<p>The manifest says <code>\"detector\": \"yolov8\"</code> but the code uses <code>buffalo_sc</code> (InsightFace RetinaFace). Either the manifest is stale from a prior version, or a different detector was used for the current DB build. This should be clarified and fixed during the rebuild.</p>"},{"location":"research/face-recognition-quality-investigation/#competitive-analysis-stashface-cc1234","title":"Competitive Analysis: StashFace (cc1234)","text":"<p>StashFace is the primary competitive reference. Here's what it does right that we should match or exceed:</p> Feature StashFace Our System Gap Face alignment DeepFace <code>extract_faces(align=True)</code> None (raw crop) CRITICAL ArcFace normalization <code>(x-127.5)/128</code> via DeepFace <code>x/255</code> CRITICAL FaceNet normalization <code>\"Facenet2018\"</code> via DeepFace <code>(x-127.5)/127.5</code> OK (equivalent) Horizontal flip averaging Yes (original + flipped, averaged) No HIGH Aspect-ratio resize Yes (DeepFace <code>resize_image</code>) No (stretch) Medium Index space Cosine Cosine OK Index storage E4M3 (8-bit) Float32 Low (optimization) Embedding dimensions 512 (both models) 512 (both models) OK Model weights Equal (1.0 each) FaceNet 0.6, ArcFace 0.4 Tuning difference Face detection YOLOv8 + MediaPipe InsightFace RetinaFace Different but OK Database size ~130K performers ~70K performers Different scope Confidence scoring Softmax with temperature + boost Linear (1 - distance) Different approach"},{"location":"research/face-recognition-quality-investigation/#proposed-fix-plan","title":"Proposed Fix Plan","text":""},{"location":"research/face-recognition-quality-investigation/#phase-1-fix-embeddingspy-both-trainer-and-sidecar","title":"Phase 1: Fix embeddings.py (both trainer and sidecar)","text":""},{"location":"research/face-recognition-quality-investigation/#1a-add-face-alignment-using-insightface-landmarks","title":"1a. Add face alignment using InsightFace landmarks","text":"<p>Use the 5-point landmarks already available from RetinaFace to align faces before cropping. The alignment should:</p> <ol> <li>Get left_eye and right_eye coordinates from <code>face.kps[0]</code> and <code>face.kps[1]</code></li> <li>Calculate rotation angle to make eyes horizontal</li> <li>Apply affine rotation via <code>cv2.warpAffine()</code></li> <li>Crop the aligned face</li> </ol> <p>This approach is simpler and faster than DeepFace's alignment because we already have reliable landmarks from RetinaFace (no need for a separate eye detector).</p> <p>For even better alignment, consider using InsightFace's built-in <code>norm_crop()</code> which does a 5-point affine alignment (not just rotation but also scaling and translation to place landmarks at standard positions). This is what InsightFace's own ArcFace model was trained with and would produce the best-aligned faces.</p>"},{"location":"research/face-recognition-quality-investigation/#1b-fix-arcface-normalization","title":"1b. Fix ArcFace normalization","text":"<p>Change from <code>/ 255.0</code> to <code>(x - 127.5) / 128</code>.</p>"},{"location":"research/face-recognition-quality-investigation/#1c-add-aspect-ratio-preserving-resize","title":"1c. Add aspect-ratio-preserving resize","text":"<p>Use either DeepFace's <code>preprocessing.resize_image()</code> directly, or implement equivalent logic with OpenCV/PIL that resizes maintaining aspect ratio and pads with black.</p>"},{"location":"research/face-recognition-quality-investigation/#1d-add-horizontal-flip-averaging","title":"1d. Add horizontal flip averaging","text":"<p>Generate embeddings for both the original face and its horizontal mirror, then average both vectors. This provides a free accuracy boost.</p>"},{"location":"research/face-recognition-quality-investigation/#phase-2-rebuild-the-face-database","title":"Phase 2: Rebuild the face database","text":"<p>Since embeddings.py is shared between trainer and sidecar:</p> <ol> <li>Keep all performer metadata - names, aliases, stashbox IDs, countries, etc. are all correct</li> <li>Keep all face image URLs - we know which images to download for each performer</li> <li>Delete all face embeddings and Voyager index entries</li> <li>Run a new enrichment pass to re-detect faces, align them, and generate correct embeddings</li> <li>Export new faces.json, performers.json, and Voyager indices</li> </ol> <p>The trainer's face URLs are stored in the <code>faces</code> table's <code>image_url</code> column. We can reprocess these same URLs with the corrected pipeline rather than re-scraping from stashbox. This means: - No stashbox API rate limiting concerns - Much faster than original scraping (just downloading images and processing) - Same curated image set, just better embeddings</p>"},{"location":"research/face-recognition-quality-investigation/#phase-3-update-sidecar-embeddingspy","title":"Phase 3: Update sidecar embeddings.py","text":"<p>Copy the corrected <code>embeddings.py</code> from the trainer to the sidecar. Since they're supposed to be identical (the trainer file says \"this file is shared\"), this ensures query-time preprocessing matches DB build-time preprocessing exactly.</p>"},{"location":"research/face-recognition-quality-investigation/#phase-4-update-matchingscoring-optional-improvements","title":"Phase 4: Update matching/scoring (optional improvements)","text":"<p>After the critical fixes, consider: - Tuning fusion weights (StashFace uses equal 1.0/1.0 instead of our 0.6/0.4) - Implementing softmax-based confidence scoring instead of linear - Adding E4M3 quantized storage for memory efficiency - Re-evaluating health detection thresholds (may not trigger as often with correct embeddings)</p>"},{"location":"research/face-recognition-quality-investigation/#confidence-assessment","title":"Confidence Assessment","text":""},{"location":"research/face-recognition-quality-investigation/#will-this-fix-the-problem","title":"Will this fix the problem?","text":"<p>Very high confidence (95%+). The alignment issue alone accounts for most face recognition systems' accuracy. It's the single most important preprocessing step in face recognition, and we completely skip it. Every working system we examined does alignment.</p>"},{"location":"research/face-recognition-quality-investigation/#will-we-match-or-exceed-stashface","title":"Will we match or exceed StashFace?","text":"<p>Yes. After implementing all Phase 1 fixes, our system will use: - The same models (FaceNet512 + ArcFace) - The same distance metric (Cosine) - The same preprocessing (alignment + correct normalization + flip averaging) - A comparable database size (70K performers) - More sophisticated matching (dual-model health detection and adaptive fusion)</p> <p>Our health detection and adaptive fusion strategy is actually more sophisticated than StashFace's simple weighted voting, which should give us an edge in edge cases.</p>"},{"location":"research/face-recognition-quality-investigation/#what-about-insightfaces-built-in-arcface","title":"What about InsightFace's built-in ArcFace?","text":"<p>InsightFace ships with its own ArcFace recognition model (e.g., <code>w600k_r50</code> in the <code>buffalo_l</code> model pack). This is a different ArcFace implementation than DeepFace's, trained on a larger dataset (WebFace600K vs MS1M). We could potentially add it as a third signal or replace DeepFace's ArcFace entirely. However, this would be a larger change and the fixes above should already bring us to competitive quality.</p>"},{"location":"research/face-recognition-quality-investigation/#files-to-modify","title":"Files to Modify","text":""},{"location":"research/face-recognition-quality-investigation/#trainer-stash-sense-trainer","title":"Trainer (stash-sense-trainer)","text":"<ul> <li><code>api/embeddings.py</code> - Add alignment, fix normalization, add flip averaging</li> <li><code>api/face_processor.py</code> - May need updates for new preprocessing</li> <li><code>api/index_manager.py</code> - No changes needed (Cosine space is correct)</li> <li>Database: Delete face rows + re-populate via enrichment run</li> </ul>"},{"location":"research/face-recognition-quality-investigation/#sidecar-stash-sense","title":"Sidecar (stash-sense)","text":"<ul> <li><code>api/embeddings.py</code> - Mirror all changes from trainer</li> <li><code>api/matching.py</code> - Consider weight tuning after rebuild</li> <li><code>api/recognizer.py</code> - No structural changes needed</li> </ul>"},{"location":"research/face-recognition-quality-investigation/#appendix-how-stashfaces-pipeline-works-complete","title":"Appendix: How StashFace's Pipeline Works (Complete)","text":"<p>For reference, here's StashFace's complete pipeline as reverse-engineered from the source:</p> <pre><code>INPUT IMAGE\n    |\n[YOLOv8 Face Detection] or [MediaPipe Face Detection]\n    |\n[DeepFace extract_faces(align=True)]\n    |--- Eye landmark detection\n    |--- 2D affine rotation to level eyes\n    |--- Crop aligned face\n    |\n[For each detected face]\n    |\n    +-- [Create batch: original + horizontal flip]\n    |       face_batch = stack([face, face[:, ::-1, :]])\n    |\n    +-- [FaceNet512 Preprocessing]\n    |       resize_image(face, (160,160))  # aspect-ratio preserving + pad\n    |       normalize_input(img, \"Facenet2018\")  # (x/127.5) - 1\n    |       model(batch, training=False)\n    |\n    +-- [ArcFace Preprocessing]\n    |       resize_image(face, (112,112))  # aspect-ratio preserving + pad\n    |       normalize_input(img, \"ArcFace\")  # (x-127.5)/128\n    |       model(batch, training=False)\n    |\n    +-- [Average original + flipped embeddings per model]\n    |       facenet_emb = mean(facenet_batch, axis=0)\n    |       arcface_emb = mean(arcface_batch, axis=0)\n    |\n    +-- [Query Voyager Indices (Cosine, E4M3)]\n    |       facenet_results = index.query(facenet_emb, k=50)\n    |       arcface_results = index.query(arcface_emb, k=50)\n    |\n    +-- [Ensemble Fusion]\n    |       Weighted voting (1.0 each model)\n    |       Softmax confidence with temperature\n    |       Final score = normalized_votes * avg_confidence * boost\n    |\nOUTPUT: Ranked performer matches with confidence scores\n</code></pre>"},{"location":"research/reeval-2026-02-12/","title":"Face Recognition Re-evaluation Results (2026-02-12)","text":""},{"location":"research/reeval-2026-02-12/#context","title":"Context","text":"<p>New face recognition database imported from stash-sense-trainer with: - 107,759 performers, 277,097 faces from 5 sources (stashdb, fansdb, theporndb, pmvstash, javstash) - Proper face alignment (insightface_norm_crop_5point) - previously raw bbox crops - Correct ArcFace normalization ((x-127.5)/128) - previously /255 - Flip averaging for embeddings</p> <p>Previous testing (Feb 4-6) used garbage data and yielded ~44% accuracy, ~31% precision at best.</p>"},{"location":"research/reeval-2026-02-12/#test-setup","title":"Test Setup","text":"<ul> <li>20 stratified test scenes (15x 1080p, 5x 720p)</li> <li>48 expected performers across all scenes</li> <li>All scenes classified as \"sparse\" coverage tier</li> <li>Benchmark script: <code>api/benchmark/reeval.py</code></li> <li>Full JSON results: <code>api/benchmark_results/reeval_20260212_064023.json</code></li> <li>Runtime: 166 minutes</li> </ul>"},{"location":"research/reeval-2026-02-12/#phase-1-baseline-current-defaults","title":"Phase 1: Baseline (Current Defaults)","text":"Metric Value Accuracy 41.7% Precision 32.8% TP / Expected 20 / 48 False Positives 41 1080p accuracy 39.5% 720p accuracy 50.0% <p>Key finding: Accuracy is roughly the same as old DB (~44%). The alignment/normalization fixes didn't meaningfully improve scene identification. The bottleneck is elsewhere in the pipeline.</p>"},{"location":"research/reeval-2026-02-12/#phase-2-parameter-sweeps","title":"Phase 2: Parameter Sweeps","text":""},{"location":"research/reeval-2026-02-12/#max_distance-no-impact-above-05","title":"max_distance - No impact above 0.5","text":"max_distance Accuracy Precision TP FP FN 0.3 4.2% 100.0% 2 0 46 0.4 33.3% 43.2% 16 21 32 0.5 41.7% 32.8% 20 41 28 0.6 41.7% 32.8% 20 41 28 0.7 41.7% 32.8% 20 41 28 0.8 41.7% 32.8% 20 41 28 0.9 41.7% 32.8% 20 41 28 <p>Insight: Matches either land below 0.5 or don't land at all. Anything above 0.5 is noise. Can safely tighten to 0.5.</p>"},{"location":"research/reeval-2026-02-12/#min_face_size-no-impact-below-80","title":"min_face_size - No impact below 80","text":"min_face_size Accuracy Precision TP FP FN 30 41.7% 32.8% 20 41 28 40 41.7% 32.8% 20 41 28 60 41.7% 32.8% 20 41 28 80 37.5% 36.7% 18 31 30 100 29.2% 32.6% 14 29 34 <p>Insight: Below 80px there are no additional small faces being matched. Above 80 loses faces.</p>"},{"location":"research/reeval-2026-02-12/#fusion_weights-arcface-now-pulling-its-weight","title":"fusion_weights - ArcFace now pulling its weight","text":"Weights (fn/af) Accuracy Precision TP FP FN 0.7/0.3 39.6% 30.6% 19 43 29 0.6/0.4 (old default) 41.7% 32.8% 20 41 28 0.5/0.5 43.8% 33.9% 21 41 27 0.4/0.6 43.8% 32.8% 21 43 27 0.3/0.7 41.7% 31.2% 20 44 28 <p>Insight: Equal weighting is optimal. This confirms the ArcFace normalization fix is working - it's now a reliable signal. The old 0.6/0.4 bias toward FaceNet was compensating for broken ArcFace.</p>"},{"location":"research/reeval-2026-02-12/#matching_mode-frequency-wins","title":"matching_mode - Frequency wins","text":"Mode Accuracy Precision TP FP FN frequency 41.7% 32.8% 20 41 28 hybrid 39.6% 33.9% 19 37 29 <p>Insight: Hybrid has marginally better precision (fewer FPs) but loses 1 TP. Frequency is the better default.</p>"},{"location":"research/reeval-2026-02-12/#min_appearances-no-meaningful-impact","title":"min_appearances - No meaningful impact","text":"min_appearances Accuracy Precision TP FP FN 1 41.7% 32.8% 20 41 28 2 41.7% 32.8% 20 41 28 3 39.6% 47.5% 19 21 29 <p>Insight: 1\u21922 identical. 3 is a strong precision filter (47.5%) at slight accuracy cost. Could be useful in a \"high precision\" mode.</p>"},{"location":"research/reeval-2026-02-12/#min_unique_frames-the-biggest-lever-tied-with-num_frames","title":"min_unique_frames - THE biggest lever (tied with num_frames)","text":"min_unique_frames Accuracy Precision TP FP FN 1 60.4% 30.5% 29 66 19 2 (default) 41.7% 32.8% 20 41 28 3 31.2% 55.6% 15 12 33 <p>Insight: Dropping from 2\u21921 recovers 9 TPs (+45% relative) but adds 25 FPs. The unique frame requirement is the binding constraint for many performers who only appear clearly in one frame. Setting to 3 gives 55.6% precision but kills recall.</p>"},{"location":"research/reeval-2026-02-12/#min_confidence-zero-impact","title":"min_confidence - Zero impact","text":"min_confidence Accuracy Precision 0.2 41.7% 32.8% 0.3 41.7% 32.8% 0.35 41.7% 32.8% 0.4 41.7% 32.8% 0.5 41.7% 32.8% <p>Insight: The confidence threshold is never the binding constraint. All matches that pass the distance filter also pass any confidence threshold we tested.</p>"},{"location":"research/reeval-2026-02-12/#num_frames-strong-positive-correlation-with-accuracy","title":"num_frames - Strong positive correlation with accuracy","text":"num_frames Accuracy Precision TP FP FN 20 35.4% 44.7% 17 21 31 40 (default) 41.7% 32.8% 20 41 28 60 54.2% 36.1% 26 46 22 80 60.4% 30.5% 29 66 19 <p>Insight: Clear linear relationship. Each 20 extra frames adds ~6% accuracy. 60 is the best accuracy/precision/runtime balance: +12.5% accuracy over baseline, precision actually improves to 36.1%, and only 50% more runtime than 40 frames. 80 frames has diminishing returns and drops precision.</p>"},{"location":"research/reeval-2026-02-12/#phase-3-best-of-combination","title":"Phase 3: Best-of Combination","text":"<p>Combined best from each sweep: <pre><code>max_distance=0.5, min_face_size=30, fn/af=0.5/0.5,\nmatching_mode=frequency, min_appearances=1, min_unique_frames=1,\nmin_confidence=0.2, num_frames=80\n</code></pre></p> Config Accuracy Precision TP FP FN Baseline 41.7% 32.8% 20 41 28 Best combo 62.5% 30.0% 30 70 18 Delta +20.8% -2.8% +10 +29 -10 <p>The gains come primarily from more frames + dropping min_unique_frames to 1.</p>"},{"location":"research/reeval-2026-02-12/#phase-4-original-scene-re-validation-8-scenes","title":"Phase 4: Original Scene Re-validation (8 scenes)","text":"Config Accuracy Precision TP FP FN Default params 50.0% 26.7% 8 22 8 Best params 56.2% 22.5% 9 31 7 <p>Per-scene: - Scene 13938 (1080p): 2/2 both configs - Scene 16342 (1080p): 2/2 both configs - Scene 30835 (1080p): 0/2 both configs - stubborn failure - Scene 3283 (720p): 0/2 \u2192 1/2 (best params recovered one) - Others: 1/2 both configs</p>"},{"location":"research/reeval-2026-02-12/#phase-5-gallery-benchmark","title":"Phase 5: Gallery Benchmark","text":"<p>10 galleries tested, 5 distance thresholds. Results were nearly identical across all thresholds:</p> max_distance Accuracy Precision TP FP FN All (0.4-0.8) 78.3% 3.6% 18 483-488 5 <p>Major problem: Gallery 1062 (1,074 images, 2 expected performers) generated 279-283 FPs alone. Excluding it, the other 9 galleries had ~204 FPs for 16 TPs (~7.3% precision).</p> <p>Root cause: The \"2+ appearances OR single &lt; 0.4\" aggregation filter is far too permissive for galleries. Each face in each image produces a top-1 match, and with hundreds of images, random performers easily get 2+ appearances.</p> <p>Suggested fixes for gallery aggregation: 1. Require appearances in minimum % of images (e.g., 10%) not just absolute count 2. Scale minimum appearances with gallery size (e.g., max(2, image_count * 0.05)) 3. Use stricter distance thresholds for gallery mode 4. Require the performer to be the top-1 match (not just any match) in multiple images</p>"},{"location":"research/reeval-2026-02-12/#phase-6-image-benchmark","title":"Phase 6: Image Benchmark","text":"<p>Skipped - no images in the library had tagged performers with StashDB IDs.</p>"},{"location":"research/reeval-2026-02-12/#applied-parameter-changes","title":"Applied Parameter Changes","text":"<p>Based on these findings, the following \"safe\" changes were applied:</p> Parameter Old Default New Default Rationale facenet_weight 0.6 0.5 ArcFace now working correctly, equal weighting optimal arcface_weight 0.4 0.5 Same max_distance 0.7 0.5 Results plateau at 0.5, tighter = less noise num_frames 40 60 +12.5% accuracy, best precision of elevated frame counts <p>Not changed (needs further investigation): - min_unique_frames: kept at 2 (dropping to 1 has huge FP cost) - min_appearances: kept at 2 - min_confidence: kept at 0.35 (no impact either way)</p>"},{"location":"research/reeval-2026-02-12/#post-benchmark-clustering-ui-improvements-same-day","title":"Post-Benchmark: Clustering &amp; UI Improvements (same day)","text":""},{"location":"research/reeval-2026-02-12/#problem-flat-frequency-matching-produces-too-many-persons","title":"Problem: Flat Frequency Matching Produces Too Many \"Persons\"","text":"<p>The frequency matching mode had no concept of face clustering - it threw all face detections into a flat bag and counted performer appearances. A 2-person scene with 38 face detections would produce 34 \"persons\" in the UI, one per unique performer match.</p>"},{"location":"research/reeval-2026-02-12/#root-cause-l2-distance-on-concatenated-embeddings","title":"Root Cause: L2 Distance on Concatenated Embeddings","text":"<p>The existing <code>cluster_faces_by_person</code> used L2 distance on concatenated 1024-dim (FaceNet+ArcFace) embeddings with threshold 0.6. In that high-dimensional space, same-person faces are typically 1.0+ apart in L2 distance, so the 0.6 threshold was far too tight and almost no clustering happened.</p>"},{"location":"research/reeval-2026-02-12/#fix-clustered-frequency-matching","title":"Fix: Clustered Frequency Matching","text":"<p>Implemented a two-stage approach (<code>clustered_frequency_matching</code>):</p> <ol> <li>Stage 1 - Clustering: Group faces by cosine similarity (consistent with Voyager indices) using pre-computed embeddings (avoids recomputing). Threshold 0.6 cosine distance works well.</li> <li>Stage 2 - Identification: Within each cluster, run frequency-style matching to identify who each person is. Alternatives shown per cluster.</li> </ol> <p>Also merged clusters with the same best match (existing <code>merge_clusters_by_match</code>).</p>"},{"location":"research/reeval-2026-02-12/#results","title":"Results","text":"Scene Faces Old Persons New Persons Correct Top Matches 709 38 34 9 (2 dominant: 22 + 5 frames) Rilynn Rae, Keiran Lee 14650 34 ~30 2 Ella Reese, Tim Gottfrid 19759 94 ~80 6 (4 dominant: 27+24+20+17 frames) Karlie Montana, Madison Ivy, Keiran Lee, Voodoo"},{"location":"research/reeval-2026-02-12/#additional-improvements","title":"Additional Improvements","text":"<ul> <li>Tagged-performer awareness: Plugin sends existing scene performer StashDB IDs to the API. A small confidence boost (+0.03) is applied to already-tagged performers, and <code>already_tagged</code> flag is returned for UI display.</li> <li>\"Already tagged\" UI state: Shows \"Already tagged on scene\" with green badge instead of \"Add to Scene\" button for performers already on the scene.</li> <li>Grouped UI: Multi-frame clusters shown prominently; single-frame detections collapsed in a collapsible section.</li> <li>Embedding reuse: <code>RecognitionResult</code> now stores the pre-computed embedding, eliminating redundant embedding generation during clustering.</li> </ul>"},{"location":"research/reeval-2026-02-12/#future-investigation-areas","title":"Future Investigation Areas","text":"<ol> <li> <p>Why is accuracy still ~42% with good embeddings? The database quality improvement didn't help scenes. Investigate whether the problem is face detection (faces not being detected), frame selection (wrong moments), or matching (wrong performers winning).</p> </li> <li> <p>Failure analysis on specific performers: Which of the 28 FN performers are being missed and why? Are they in the database? Are their faces being detected? Are they being matched to wrong performers?</p> </li> <li> <p>Scene 30835: Consistently 0/2 across all parameter configs. What's special about this scene?</p> </li> <li> <p>Gallery aggregation redesign: Current \"2+ appearances\" filter is broken for galleries. Need percentage-based or statistical approach.</p> </li> <li> <p>min_unique_frames=1 with compensating FP filter: Could we drop to 1 frame but add a stricter distance threshold (e.g., require distance &lt; 0.4 for single-frame matches)?</p> </li> <li> <p>Image benchmark: Need to tag some images with performers to enable Phase 6.</p> </li> <li> <p>Larger scene sample: 20 scenes with 48 performers is small. Results can shift by 1-2 TPs. Consider running with 50+ scenes for more statistical power.</p> </li> </ol>"},{"location":"unraid/gpu-passthrough/","title":"GPU Passthrough for Unraid","text":""},{"location":"unraid/gpu-passthrough/#prerequisites","title":"Prerequisites","text":"<ul> <li>NVIDIA GPU (GTX 10-series or newer recommended)</li> <li>Unraid 6.12+</li> </ul>"},{"location":"unraid/gpu-passthrough/#step-1-install-nvidia-driver-plugin","title":"Step 1: Install Nvidia Driver Plugin","text":"<ol> <li>Go to Apps \u2192 Search \"Nvidia Driver\"</li> <li>Install Nvidia-Driver by ich777</li> <li>Reboot Unraid</li> </ol>"},{"location":"unraid/gpu-passthrough/#step-2-verify-gpu-detection","title":"Step 2: Verify GPU Detection","text":"<p>Open Unraid terminal and run:</p> <pre><code>nvidia-smi\n</code></pre> <p>You should see your GPU listed:</p> <pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 550.xx       Driver Version: 550.xx       CUDA Version: 12.x     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:XX:00.0 Off |                  N/A |\n+-------------------------------+----------------------+----------------------+\n</code></pre>"},{"location":"unraid/gpu-passthrough/#step-3-container-configuration","title":"Step 3: Container Configuration","text":"<p>The Stash Sense template automatically includes GPU settings.</p> <p>If configuring manually, ensure these are set:</p> Setting Value Extra Parameters <code>--runtime=nvidia --gpus all</code> NVIDIA_VISIBLE_DEVICES <code>all</code> (or specific GPU UUID)"},{"location":"unraid/gpu-passthrough/#troubleshooting","title":"Troubleshooting","text":""},{"location":"unraid/gpu-passthrough/#nvidia-smi-command-not-found","title":"\"nvidia-smi: command not found\"","text":"<p>Driver plugin not installed or Unraid needs reboot.</p> <pre><code># Check if plugin is installed\nls /boot/config/plugins/nvidia-driver.plg\n# If missing, reinstall from Apps\n</code></pre>"},{"location":"unraid/gpu-passthrough/#no-gpu-detected-in-container","title":"\"No GPU detected in container\"","text":"<p>Check container has GPU access:</p> <pre><code>docker exec stash-sense nvidia-smi\n</code></pre> <p>If this fails, verify:</p> <ol> <li>Extra Parameters includes <code>--runtime=nvidia</code></li> <li>Container was recreated after driver install (not just restarted)</li> </ol>"},{"location":"unraid/gpu-passthrough/#cuda-out-of-memory","title":"\"CUDA out of memory\"","text":"<p>Another process is using the GPU. Check what's using it:</p> <pre><code>nvidia-smi\n</code></pre> <p>Look at \"Processes\" section. Common culprits: - Plex hardware transcoding - Tdarr - Frigate - Other ML containers</p>"},{"location":"unraid/gpu-passthrough/#failed-to-initialize-nvml","title":"\"Failed to initialize NVML\"","text":"<p>The Nvidia container toolkit isn't configured. Re-install the driver plugin and reboot.</p>"},{"location":"unraid/gpu-passthrough/#selecting-a-specific-gpu-multi-gpu-systems","title":"Selecting a Specific GPU (Multi-GPU Systems)","text":"<p>If you have multiple GPUs, specify which one to use:</p> <pre><code># List GPU UUIDs\nnvidia-smi -L\n\n# Use specific GPU\nNVIDIA_VISIBLE_DEVICES=GPU-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n</code></pre> <p>Or by index: <pre><code>NVIDIA_VISIBLE_DEVICES=0  # First GPU\nNVIDIA_VISIBLE_DEVICES=1  # Second GPU\n</code></pre></p>"},{"location":"unraid/setup/","title":"Unraid Setup","text":""},{"location":"unraid/setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Unraid 6.12+</li> <li>Nvidia Driver plugin installed (for GPU acceleration)</li> <li>Stash running on Unraid or accessible from Unraid</li> </ul>"},{"location":"unraid/setup/#option-1-xml-template-recommended","title":"Option 1: XML Template (Recommended)","text":""},{"location":"unraid/setup/#1-download-the-template","title":"1. Download the template","text":"<p>Copy stash-sense.xml to your Unraid flash drive:</p> <pre><code>/boot/config/plugins/dockerMan/templates-user/stash-sense.xml\n</code></pre>"},{"location":"unraid/setup/#2-add-container","title":"2. Add container","text":"<p>Go to Docker \u2192 Add Container \u2192 Template \u2192 User Templates \u2192 stash-sense</p>"},{"location":"unraid/setup/#3-configure","title":"3. Configure","text":"Setting Value Stash URL <code>http://stash:9999</code> or your Stash IP/hostname Stash API Key From Stash: Settings \u2192 Security \u2192 API Key Data Directory Path to extracted database files"},{"location":"unraid/setup/#4-apply","title":"4. Apply","text":"<p>Click Apply. The container will download and start.</p>"},{"location":"unraid/setup/#option-2-docker-compose","title":"Option 2: Docker Compose","text":"<p>If you prefer docker-compose on Unraid:</p>"},{"location":"unraid/setup/#1-create-directory","title":"1. Create directory","text":"<pre><code>mkdir -p /mnt/user/appdata/stash-sense\n</code></pre>"},{"location":"unraid/setup/#2-create-docker-composeyml","title":"2. Create docker-compose.yml","text":"<pre><code>version: \"3.8\"\n\nservices:\n  stash-sense:\n    image: carrotwaxr/stash-sense:latest\n    container_name: stash-sense\n    runtime: nvidia\n    restart: unless-stopped\n    ports:\n      - \"6960:5000\"\n    environment:\n      - STASH_URL=http://stash:9999\n      - STASH_API_KEY=your-api-key\n      - NVIDIA_VISIBLE_DEVICES=all\n    volumes:\n      - /mnt/user/appdata/stash-sense/data:/data:ro\n      - /mnt/user/appdata/stash-sense/models:/root/.insightface\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n</code></pre>"},{"location":"unraid/setup/#3-start","title":"3. Start","text":"<pre><code>cd /mnt/user/appdata/stash-sense\ndocker compose up -d\n</code></pre>"},{"location":"unraid/setup/#database-setup","title":"Database Setup","text":"<p>Download the database release and extract:</p> <pre><code>cd /mnt/user/appdata/stash-sense/data\nwget https://github.com/carrotwaxr/stash-sense/releases/download/db-latest/stash-sense-db.tar.gz\ntar -xzf stash-sense-db.tar.gz\nrm stash-sense-db.tar.gz\n</code></pre> <p>You should have: <pre><code>data/\n\u251c\u2500\u2500 face_facenet.voy\n\u251c\u2500\u2500 face_arcface.voy\n\u251c\u2500\u2500 performers.json\n\u2514\u2500\u2500 manifest.json\n</code></pre></p>"},{"location":"unraid/setup/#networking","title":"Networking","text":"<p>If Stash runs on the same Unraid server, use Docker's internal networking:</p> <ul> <li>Stash URL: <code>http://stash:9999</code> (if Stash container is named \"stash\")</li> <li>Or use Unraid's IP: <code>http://192.168.1.x:9999</code></li> </ul>"},{"location":"unraid/setup/#next-steps","title":"Next Steps","text":"<ul> <li>GPU Passthrough - ensure GPU is working</li> <li>Plugin Setup - install the Stash plugin</li> <li>Troubleshooting - if something's not working</li> </ul>"}]}