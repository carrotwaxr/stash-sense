{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Stash Sense","text":"<p>AI-powered performer identification for Stash.</p>"},{"location":"#what-it-does","title":"What it does","text":"<ul> <li>Identifies performers in scenes using face recognition against StashDB</li> <li>Works with sprite sheets - no video processing needed, uses your existing Stash-generated sprites</li> <li>Runs locally on your GPU - no cloud dependencies, no data leaves your network</li> </ul>"},{"location":"#how-it-works","title":"How it works","text":"<ol> <li>You click \"Identify Performers\" on a scene in Stash</li> <li>Stash Sense fetches the scene's sprite sheet</li> <li>Faces are detected and matched against the StashDB performer database</li> <li>Results show matched performers with confidence scores</li> <li>One click to add identified performers to the scene</li> </ol>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>docker compose up -d\n</code></pre> <p>Then install the plugin in Stash and point it at <code>http://&lt;host&gt;:5000</code>.</p>"},{"location":"#requirements","title":"Requirements","text":"Component Requirement GPU NVIDIA with 4GB+ VRAM (CPU fallback available) Docker With nvidia-container-toolkit Stash Scene sprite sheets generated"},{"location":"#current-status","title":"Current Status","text":"<p>Database build in progress (~100k performers from StashDB). Pre-built database release coming soon.</p>"},{"location":"architecture/","title":"Stash Sense Architecture","text":"<p>Reference for how the system works and why key decisions were made.</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<p>Stash Sense is a two-component system: a sidecar API (Python/FastAPI) that runs face recognition and analysis, and a plugin (JS/CSS/Python) injected into the Stash web UI.</p> <p>Deployment model: The sidecar runs as a Docker container on unRAID (port 6960) alongside Stash. The plugin backend proxies all requests from Stash to the sidecar, bypassing browser CSP restrictions. The sidecar URL is configurable in Stash plugin settings.</p> <p>Two-database design: - <code>performers.db</code> \u2014 Read-only, distributable. Contains performer metadata, face references, and stash-box IDs sourced from the private trainer repo. Updated via GitHub Releases. - <code>stash_sense.db</code> \u2014 Read-write, user-local. Contains recommendations, dismissed items, analysis watermarks, upstream snapshots, and scene fingerprints. Persists across face DB updates.</p> <p>This separation means face DB updates never touch user state, and the distributable database contains no user-specific data.</p> <p>Docker image: Two-stage build on <code>nvidia/cuda:12.4.0-runtime-ubuntu22.04</code>. ONNX models (FaceNet512 + ArcFace, ~220MB) are baked into the image. Data files (Voyager indices ~1.1GB, performers.db ~210MB) are volume-mounted from NVMe for independent updates. PyTorch, TensorFlow, and DeepFace are stripped \u2014 only needed by the trainer's offline scripts.</p>"},{"location":"architecture/#face-recognition","title":"Face Recognition","text":"<p>Models: RetinaFace (buffalo_sc, ONNX GPU) for detection, FaceNet512 + ArcFace for embeddings. Both embedding models use flip-averaging (original + horizontally flipped face, averaged) for more stable representations.</p> <p>Pipeline (3-phase batch): 1. Extract \u2014 ffmpeg seeks N frames from scene, 8 concurrent workers 2. Detect \u2014 RetinaFace processes all frames, InsightFace <code>norm_crop</code> aligns faces via 5-point similarity transform 3. Embed + Match \u2014 All faces batched through both ONNX models in 2 calls (1 per model), then searched against Voyager indices (cosine space)</p> <p>This replaced a per-face sequential pipeline: 53.6s \u2192 4.8s on 1080p/60 frames/64 faces (11x speedup, embedding step 138x faster via GPU batching).</p> <p>Matching: <code>clustered_frequency_matching</code> is the default. Faces are clustered by person using cosine distance (threshold 0.6) on concatenated FaceNet+ArcFace embeddings, then frequency-matched within each cluster. Multi-frame appearances boost confidence. Tagged-performer boost (+0.03) applied when scene already has performers tagged.</p> <p>Tuned defaults (from 20-scene stratified benchmark): fusion weights 0.5/0.5 FaceNet/ArcFace, max_distance 0.5, num_frames 60, min_unique_frames 2. Baseline: ~42% scene accuracy, ~33% precision. Biggest improvement levers are num_frames and database coverage.</p>"},{"location":"architecture/#recommendations-engine","title":"Recommendations Engine","text":"<p>BaseAnalyzer pattern: Each recommendation type (duplicate scenes, missing stash-box links, upstream changes, etc.) is a pluggable analyzer with a consistent interface for running, progress tracking, and creating recommendations. The scheduler runs analyzers as background jobs.</p> <p>Incremental watermarking: Most analyzers track a <code>last_watermark</code> timestamp or cursor and only process items modified since the last run. This keeps incremental runs fast (~seconds) versus full scans (~73 min for 7K+ performers at 5 req/s rate limit).</p> <p>Polymorphic recommendations: A single <code>recommendations</code> table handles all entity types (duplicate_performer, duplicate_scene, unidentified_scene, missing_stashbox_link, stashbox_updates) with type-specific JSON payloads. Users review via a dashboard or contextual actions on affected pages.</p>"},{"location":"architecture/#duplicate-scene-detection","title":"Duplicate Scene Detection","text":"<p>Stash's built-in phash matching fails with different intros/outros, trimming, aspect ratio changes, or watermarks. Stash Sense uses multi-signal detection with a confidence hierarchy.</p> <p>Signal hierarchy with caps: - Stash-box ID match = 100% (authoritative) - Face fingerprint similarity = up to 85% - Metadata overlap = up to 60% - No single signal other than stash-box ID reaches 100%</p> <p>Face fingerprints: Per-scene records of which performers appeared, how many times detected, and proportion of total faces. Same performers appearing in the same ratios is robust to trimming and length differences.</p> <p>Diminishing returns scoring: <code>primary + secondary \u00d7 0.3</code> \u2014 the second signal adds less than the first, preventing false confidence inflation.</p> <p>Scalable candidate generation: Direct O(n\u00b2) comparison crashes on 15K+ scenes. Instead, a two-phase approach: 1. Candidate generation \u2014 SQL joins and inverted indices produce O(n) candidate pairs from 3 sources: stash-box ID grouping, face fingerprint self-join (shared performers), and metadata intersection (same studio AND performer). Yields 10K-50K candidates vs 112M brute-force comparisons. 2. Sequential scoring \u2014 Cursor-based pagination (<code>id &gt; last_id LIMIT 100</code>) iterates candidates, scores each, writes recommendations immediately. Constant memory throughout.</p>"},{"location":"architecture/#multi-signal-identification","title":"Multi-Signal Identification","text":"<p>Face recognition alone achieves ~50-60% accuracy. Additional biometric signals improve identification when faces are unclear or absent.</p> <p>Late fusion architecture: Each signal is searched independently; results are combined via multiplicative scoring. Missing signals are handled gracefully (no tattoo visible \u2192 neutral multiplier).</p> <p>Signals: - Face (primary) \u2014 Voyager index search, top-K candidates - Body proportions \u2014 MediaPipe pose estimation extracts shoulder-hip ratio, leg-torso ratio, arm-span-height ratio. Compared against database values with tolerance ~0.15. Returns penalty multiplier (1.0 compatible, 0.3 severe mismatch). - Tattoo presence \u2014 YOLO-based detection. Query shows tattoos but candidate has none \u2192 0.7x penalty. Matching tattoo locations \u2192 1.15x boost.</p> <p>Fusion: <code>final_score = face_score \u00d7 body_multiplier \u00d7 tattoo_multiplier</code></p> <p>Tattoo embedding matching (semantic similarity of tattoo designs via EfficientNet-B0 1280-dim vectors) exists in the trainer but is not yet deployed to the sidecar.</p>"},{"location":"architecture/#upstream-performer-sync","title":"Upstream Performer Sync","text":"<p>Detects stash-box field changes and presents per-field merge controls to sync local Stash against upstream updates.</p> <p>3-way diff engine: Compares upstream (current stash-box state) vs local (current Stash state) vs snapshot (last-seen upstream state stored in <code>upstream_snapshots</code> table). This distinguishes intentional local differences from actual upstream changes.</p> <p>Merge controls: Name fields get 5 options (keep/accept/demote-to-alias/add-as-alias/custom). Aliases use union checkboxes. Simple fields use 3-option radio (keep/accept/custom). Per-field monitoring can be disabled per endpoint via <code>upstream_field_config</code>.</p> <p>Field mapping complexity: Stash-box uses separate fields (cup_size, band_size, waist_size, hip_size, career_start_year, career_end_year) that Stash combines into compound strings (measurements \"38F-24-35\", career_length \"2015-2023\"). Translation handled in <code>recommendations_router.py:update_performer_fields()</code>.</p> <p>Dismissal model: Soft dismiss (permanent=0) resurfaces if upstream changes again. Permanent dismiss skips entirely.</p>"},{"location":"architecture/#gallery-image-identification","title":"Gallery &amp; Image Identification","text":"<p>Extends face recognition from scenes (frame extraction) to gallery images (typically higher quality, better-framed).</p> <p>Endpoints: <code>/identify/image</code> (single image) and <code>/identify/gallery</code> (all images in a gallery).</p> <p>Aggregation: Unlike scenes, gallery images are independent \u2014 no temporal clustering needed. Results are grouped by performer across images: best distance, average distance, image count. Filtering: 2+ appearances OR single match with distance &lt; 0.4.</p> <p>Fingerprint caching: <code>image_fingerprints</code> and <code>image_fingerprint_faces</code> tables store per-image results, avoiding re-processing on subsequent requests.</p>"},{"location":"architecture/#database-self-update","title":"Database Self-Update","text":"<p>The sidecar checks for new face recognition database releases on GitHub and performs hot-swap updates without container restart.</p> <p>Pipeline: Download \u2192 extract \u2192 verify checksums against <code>manifest.json</code> \u2192 swap \u2192 reload global state. Files are staged in <code>/data/staging/</code>, old files backed up to <code>/data/backup/</code>.</p> <p>Availability during update: Brief 503 responses (5-10s) on <code>/identify/*</code> endpoints only. An <code>update_in_progress</code> flag gates these endpoints. Non-identification endpoints remain available.</p> <p>Reload order: New DatabaseConfig \u2192 new FaceRecognizer (loads Voyager indices) \u2192 load manifest \u2192 rebuild MultiSignalMatcher \u2192 replace global references \u2192 clear flag.</p> <p>Safety: Disk space check (2.5x current DB size) before download. Rollback from backup if reload fails. <code>stash_sense.db</code> (recommendations) is never touched during updates.</p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"Variable Required Default Description <code>STASH_URL</code> Yes - URL to your Stash instance <code>STASH_API_KEY</code> Yes - Stash API key for fetching sprites <code>DATA_DIR</code> No <code>/data</code> Path to database files <code>LOG_LEVEL</code> No <code>info</code> Logging verbosity: debug, info, warning, error"},{"location":"configuration/#volume-mounts","title":"Volume Mounts","text":"Container Path Purpose Mode <code>/data</code> Database files Read-only <code>/root/.insightface</code> Model weights cache Read-write"},{"location":"configuration/#database-files","title":"Database Files","text":"<p>The <code>/data</code> volume should contain:</p> <pre><code>/data/\n\u251c\u2500\u2500 face_facenet.voy     # FaceNet embedding index\n\u251c\u2500\u2500 face_arcface.voy     # ArcFace embedding index\n\u251c\u2500\u2500 performers.json      # Performer metadata\n\u2514\u2500\u2500 manifest.json        # Database version info\n</code></pre>"},{"location":"configuration/#model-cache","title":"Model Cache","text":"<p>The <code>/root/.insightface</code> volume caches downloaded model weights. Without this mount, models are re-downloaded on every container start (~500MB).</p>"},{"location":"configuration/#api-endpoints","title":"API Endpoints","text":"Endpoint Method Description <code>/health</code> GET Health check, returns database status <code>/database/info</code> GET Database version and stats <code>/identify</code> POST Identify faces in a single image <code>/identify/scene</code> POST Identify performers in a scene (uses sprite sheet)"},{"location":"configuration/#example-health-check","title":"Example: Health Check","text":"<pre><code>curl http://localhost:5000/health\n</code></pre> <pre><code>{\n  \"status\": \"healthy\",\n  \"database_loaded\": true,\n  \"performer_count\": 50000,\n  \"face_count\": 150000\n}\n</code></pre>"},{"location":"configuration/#example-scene-identification","title":"Example: Scene Identification","text":"<pre><code>curl -X POST http://localhost:5000/identify/scene \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"scene_id\": \"123\"}'\n</code></pre>"},{"location":"configuration/#performance-tuning","title":"Performance Tuning","text":""},{"location":"configuration/#gpu-memory","title":"GPU Memory","text":"<p>The default configuration uses ~2-3GB VRAM. If you have memory constraints:</p> <ul> <li>Face detection (RetinaFace): ~1GB</li> <li>Embeddings run on CPU, don't use VRAM</li> </ul>"},{"location":"configuration/#concurrent-requests","title":"Concurrent Requests","text":"<p>The API handles one request at a time by default. For higher throughput, increase uvicorn workers (requires more VRAM):</p> <pre><code>CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"5000\", \"--workers\", \"2\"]\n</code></pre>"},{"location":"configuration/#confidence-thresholds","title":"Confidence Thresholds","text":"<p>Scores are cosine distances (lower = better match):</p> Score Interpretation &lt; 0.4 High confidence match 0.4 - 0.6 Likely match, verify visually 0.6 - 0.8 Possible match, needs review &gt; 0.8 Unlikely match"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#docker-compose-recommended","title":"Docker Compose (Recommended)","text":""},{"location":"installation/#1-clone-the-repository","title":"1. Clone the repository","text":"<pre><code>git clone https://github.com/carrotwaxr/stash-sense.git\ncd stash-sense\n</code></pre>"},{"location":"installation/#2-download-the-database","title":"2. Download the database","text":"<p>Download the latest database release and extract to <code>./data/</code>:</p> <pre><code>mkdir -p data\n# Download link will be provided when database is released\ntar -xzf stash-sense-db-latest.tar.gz -C data/\n</code></pre>"},{"location":"installation/#3-configure-environment","title":"3. Configure environment","text":"<p>Create a <code>.env</code> file or edit <code>docker-compose.yml</code>:</p> <pre><code>STASH_URL=http://your-stash-host:9999\nSTASH_API_KEY=your-api-key-here\n</code></pre> <p>Get your API key from Stash: Settings \u2192 Security \u2192 API Key</p>"},{"location":"installation/#4-start-the-container","title":"4. Start the container","text":"<pre><code>docker compose up -d\n</code></pre>"},{"location":"installation/#5-verify-its-running","title":"5. Verify it's running","text":"<pre><code>curl http://localhost:5000/health\n</code></pre> <p>Should return <code>{\"status\": \"healthy\", ...}</code></p>"},{"location":"installation/#6-install-the-stash-plugin","title":"6. Install the Stash plugin","text":"<p>See Plugin Setup.</p>"},{"location":"installation/#docker-run-alternative","title":"Docker Run (Alternative)","text":"<pre><code>docker run -d \\\n  --name stash-sense \\\n  --runtime=nvidia \\\n  --gpus all \\\n  -p 5000:5000 \\\n  -e STASH_URL=http://your-stash-host:9999 \\\n  -e STASH_API_KEY=your-api-key \\\n  -v /path/to/data:/data:ro \\\n  -v stash-sense-models:/root/.insightface \\\n  ghcr.io/carrotwaxr/stash-sense:latest\n</code></pre>"},{"location":"installation/#cpu-only-mode","title":"CPU-Only Mode","text":"<p>If you don't have an NVIDIA GPU, remove the GPU flags:</p> <pre><code>docker compose up -d  # It will auto-detect and fall back to CPU\n</code></pre> <p>CPU mode works but is significantly slower (~2-3 seconds per image vs ~200ms with GPU).</p>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Configure the service</li> <li>Install the Stash plugin</li> <li>Unraid users: see Unraid Setup</li> </ul>"},{"location":"plugin/","title":"Stash Plugin Setup","text":""},{"location":"plugin/#installation","title":"Installation","text":""},{"location":"plugin/#1-copy-plugin-files-to-stash","title":"1. Copy plugin files to Stash","text":"<p>Copy the <code>plugin/</code> directory contents to your Stash plugins folder:</p> <pre><code>~/.stash/plugins/stash-sense/\n\u251c\u2500\u2500 stash-sense.yml\n\u251c\u2500\u2500 stash-sense.js\n\u251c\u2500\u2500 stash-sense.css\n\u2514\u2500\u2500 stash_sense_backend.py\n</code></pre> <p>Or if using Docker:</p> <pre><code>/root/.stash/plugins/stash-sense/\n</code></pre>"},{"location":"plugin/#2-reload-plugins","title":"2. Reload plugins","text":"<p>In Stash: Settings \u2192 Plugins \u2192 Reload Plugins</p>"},{"location":"plugin/#3-configure-the-plugin","title":"3. Configure the plugin","text":"<p>In Stash: Settings \u2192 Plugins \u2192 Stash Sense</p> Setting Value Sidecar URL <code>http://localhost:5000</code> or your Stash Sense host"},{"location":"plugin/#usage","title":"Usage","text":""},{"location":"plugin/#identifying-performers-in-a-scene","title":"Identifying Performers in a Scene","text":"<ol> <li>Navigate to a scene page in Stash</li> <li>Click the \"Identify Performers\" button</li> <li>Wait for analysis (uses scene's sprite sheet)</li> <li>Review results in the modal:</li> <li>Detected face thumbnail</li> <li>Best match from StashDB</li> <li>Confidence score</li> <li>Click \"Add to Scene\" to link the performer</li> </ol>"},{"location":"plugin/#understanding-results","title":"Understanding Results","text":"<p>Each detected face shows:</p> <ul> <li>Thumbnail: Cropped face from the scene</li> <li>Match: Best matching performer from StashDB</li> <li>Confidence: Lower is better (cosine distance)</li> <li>Appearances: How many frames this face appeared in</li> </ul> <p>Results are grouped by person - the same performer appearing in multiple frames is clustered together.</p>"},{"location":"plugin/#result-actions","title":"Result Actions","text":"Button Action Add to Scene Links performer to scene (if in your library) View on StashDB Opens performer page on stashdb.org Not in Library Performer matched but not in your Stash"},{"location":"plugin/#requirements","title":"Requirements","text":"<ul> <li>Scene must have a sprite sheet generated</li> <li>Stash Sense sidecar must be running and accessible</li> </ul>"},{"location":"plugin/#generating-sprite-sheets","title":"Generating Sprite Sheets","text":"<p>If a scene doesn't have sprites:</p> <ol> <li>Go to scene page</li> <li>Click Generate \u2192 Enable Sprite</li> <li>Or bulk generate: Settings \u2192 Tasks \u2192 Generate \u2192 Sprites</li> </ol>"},{"location":"plugin/#troubleshooting","title":"Troubleshooting","text":""},{"location":"plugin/#failed-to-connect-to-stash-sense","title":"\"Failed to connect to Stash Sense\"","text":"<ul> <li>Verify sidecar is running: <code>curl http://localhost:5000/health</code></li> <li>Check sidecar URL in plugin settings</li> <li>If using Docker, ensure network connectivity</li> </ul>"},{"location":"plugin/#no-faces-detected","title":"\"No faces detected\"","text":"<ul> <li>Scene may not have clear face shots</li> <li>Sprite sheet may be low quality</li> <li>Try a different scene to verify setup</li> </ul>"},{"location":"plugin/#performer-not-in-library","title":"\"Performer not in library\"","text":"<p>The face matched a StashDB performer who isn't in your Stash library. Options:</p> <ol> <li>Click \"View on StashDB\" to see the performer</li> <li>Manually add the performer to your Stash</li> <li>The plugin shows StashDB ID for easy lookup</li> </ol>"},{"location":"plugin/#plugin-not-appearing","title":"Plugin not appearing","text":"<ol> <li>Check plugin files are in correct location</li> <li>Reload plugins in Stash settings</li> <li>Check Stash logs for errors</li> </ol>"},{"location":"settings-system/","title":"Settings System","text":"<p>The sidecar includes a hardware-adaptive settings system that auto-detects your hardware at startup and picks optimal defaults. All runtime settings can be viewed and adjusted via the Settings API or the Plugin Settings UI.</p>"},{"location":"settings-system/#how-it-works","title":"How It Works","text":""},{"location":"settings-system/#hardware-detection","title":"Hardware Detection","text":"<p>On startup, the sidecar probes: - GPU: ONNX Runtime CUDA provider + pynvml for VRAM/model name - CPU: Core count (respects Docker cgroup limits) - Memory: Total and available RAM (respects Docker cgroup limits) - Storage: Free disk space at the data directory</p>"},{"location":"settings-system/#hardware-tiers","title":"Hardware Tiers","text":"<p>Based on detection results, the sidecar classifies your hardware into a tier:</p> Tier Criteria Batch Size Concurrency Detection Res Frames/Scene <code>gpu-high</code> CUDA + VRAM &gt;= 4GB 32 8 640px 60 <code>gpu-low</code> CUDA + VRAM &lt; 4GB 16 6 640px 60 <code>cpu</code> No CUDA 4 2 320px 30"},{"location":"settings-system/#setting-resolution","title":"Setting Resolution","text":"<p>Each setting is resolved in priority order:</p> <ol> <li>User override (stored in the database) \u2014 highest priority</li> <li>Tier default \u2014 based on your detected hardware tier</li> <li>Hardcoded fallback \u2014 always present as a baseline</li> </ol> <p>Only user overrides are stored. Absence means \"use the tier default.\"</p>"},{"location":"settings-system/#settings-reference","title":"Settings Reference","text":""},{"location":"settings-system/#performance","title":"Performance","text":"Setting Type Range Description <code>embedding_batch_size</code> int 1-128 Faces processed per GPU inference call <code>frame_extraction_concurrency</code> int 1-16 Parallel ffmpeg processes for frame extraction <code>detection_size</code> int 160-1280 Face detection input resolution (pixels)"},{"location":"settings-system/#rate-limits","title":"Rate Limits","text":"Setting Type Range Description <code>stash_api_rate</code> float 0.5-50 Max requests/second to local Stash instance"},{"location":"settings-system/#recognition","title":"Recognition","text":"Setting Type Range Description <code>gpu_enabled</code> bool - Use GPU for inference (disable to force CPU) <code>num_frames</code> int 10-200 Frames to sample per scene <code>face_candidates</code> int 5-100 Candidate matches retrieved from vector index per face"},{"location":"settings-system/#signals","title":"Signals","text":"Setting Type Description <code>body_signal_enabled</code> bool Use body proportion analysis <code>tattoo_signal_enabled</code> bool Use tattoo detection (requires model)"},{"location":"settings-system/#settings-api","title":"Settings API","text":""},{"location":"settings-system/#get-all-settings","title":"Get All Settings","text":"<pre><code>GET /settings\n</code></pre> <p>Returns all settings grouped by category with metadata for UI rendering:</p> <pre><code>{\n  \"hardware_tier\": \"gpu-high\",\n  \"categories\": {\n    \"performance\": {\n      \"label\": \"Performance\",\n      \"settings\": {\n        \"embedding_batch_size\": {\n          \"value\": 32,\n          \"default\": 32,\n          \"is_override\": false,\n          \"type\": \"int\",\n          \"min\": 1,\n          \"max\": 128,\n          \"label\": \"Embedding Batch Size\",\n          \"description\": \"Faces processed per GPU inference call\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"settings-system/#get-single-setting","title":"Get Single Setting","text":"<pre><code>GET /settings/{key}\n</code></pre>"},{"location":"settings-system/#update-setting","title":"Update Setting","text":"<pre><code>PUT /settings/{key}\nContent-Type: application/json\n\n{\"value\": 64}\n</code></pre>"},{"location":"settings-system/#bulk-update","title":"Bulk Update","text":"<pre><code>PUT /settings\nContent-Type: application/json\n\n{\"settings\": {\"stash_api_rate\": 10.0, \"num_frames\": 45}}\n</code></pre>"},{"location":"settings-system/#reset-setting-to-default","title":"Reset Setting to Default","text":"<pre><code>DELETE /settings/{key}\n</code></pre> <p>Removes the user override, reverting to the tier default.</p>"},{"location":"settings-system/#system-info","title":"System Info","text":"<pre><code>GET /system/info\n</code></pre> <p>Returns hardware profile, version, and uptime:</p> <pre><code>{\n  \"version\": \"0.3.0\",\n  \"uptime_seconds\": 3600,\n  \"hardware\": {\n    \"gpu_available\": true,\n    \"gpu_name\": \"NVIDIA GeForce GTX 1080\",\n    \"gpu_vram_mb\": 8192,\n    \"cpu_cores\": 8,\n    \"memory_total_mb\": 32768,\n    \"tier\": \"gpu-high\",\n    \"summary\": \"NVIDIA GeForce GTX 1080 (8192MB VRAM), 32768MB RAM, 8 cores, 500000MB free disk\"\n  }\n}\n</code></pre>"},{"location":"settings-system/#environment-variable-migration","title":"Environment Variable Migration","text":"<p>The following env vars have been replaced by settings. If detected at startup, they are automatically migrated to the settings database and a deprecation warning is logged.</p> Old Env Var New Setting Notes <code>STASH_RATE_LIMIT</code> <code>stash_api_rate</code> <code>ENABLE_BODY_SIGNAL</code> <code>body_signal_enabled</code> <code>ENABLE_TATTOO_SIGNAL</code> <code>tattoo_signal_enabled</code> <code>FACE_CANDIDATES</code> <code>face_candidates</code> <p>Env vars that remain (connection strings and secrets): - <code>STASH_URL</code>, <code>STASH_API_KEY</code>, <code>DATA_DIR</code>, <code>LOG_LEVEL</code> - Per-endpoint <code>*_URL</code> and <code>*_API_KEY</code> vars</p>"},{"location":"settings-system/#architecture","title":"Architecture","text":""},{"location":"settings-system/#key-files","title":"Key Files","text":"File Purpose <code>api/hardware.py</code> Hardware detection, tier classification <code>api/settings.py</code> Setting definitions, tier defaults, resolution logic, env var migration <code>api/settings_router.py</code> FastAPI endpoints for settings and system info"},{"location":"settings-system/#startup-sequence","title":"Startup Sequence","text":"<ol> <li><code>init_hardware(data_dir)</code> \u2014 probes hardware, classifies tier</li> <li><code>init_settings(db, tier)</code> \u2014 creates settings manager with tier defaults</li> <li><code>migrate_env_vars(mgr)</code> \u2014 one-time migration of deprecated env vars</li> <li>Sidecar logs hardware summary and active tier</li> </ol>"},{"location":"settings-system/#storage","title":"Storage","text":"<p>Settings overrides are stored in the <code>user_settings</code> table of <code>stash_sense.db</code> with a <code>settings.</code> key prefix. The table uses a simple key-value structure with JSON-encoded values.</p>"},{"location":"stash-box-endpoints/","title":"Stash-Box Endpoints","text":"<p>Note: Database building has moved to the stash-sense-trainer repository. This document is preserved for reference but the build commands below apply to the trainer repo.</p> <p>This document describes the different stash-box endpoints supported for building face recognition databases, their characteristics, and considerations for each.</p>"},{"location":"stash-box-endpoints/#supported-endpoints","title":"Supported Endpoints","text":"Endpoint API Type Performers Images/Performer Rate Limit StashDB GraphQL ~100,000+ Multiple 240/min safe ThePornDB REST ~10,000 Multiple 240/min safe PMVStash GraphQL ~6,500 3.7 avg 300/min+ JAVStash GraphQL ~21,700 1.0 avg 300/min+ FansDB GraphQL Unknown Unknown Untested"},{"location":"stash-box-endpoints/#endpoint-details","title":"Endpoint Details","text":""},{"location":"stash-box-endpoints/#stashdb-primary","title":"StashDB (Primary)","text":"<ul> <li>URL: <code>https://stashdb.org/graphql</code></li> <li>API: Standard stash-box GraphQL</li> <li>Use: <code>database_builder.py</code> directly</li> <li>Notes: Largest database, highest quality metadata</li> </ul>"},{"location":"stash-box-endpoints/#theporndb","title":"ThePornDB","text":"<ul> <li>URL: <code>https://api.theporndb.net</code> (REST API)</li> <li>API: REST, NOT GraphQL (despite Stash config showing <code>/graphql</code>)</li> <li>Use: <code>build_theporndb.py</code> (custom script)</li> <li>Notes:</li> <li>Has pre-cropped face thumbnails (<code>face_url</code> field)</li> <li>Many performers cross-reference StashDB IDs</li> <li>Rate limit ~275/min, safe at 240/min</li> </ul>"},{"location":"stash-box-endpoints/#pmvstash","title":"PMVStash","text":"<ul> <li>URL: <code>https://pmvstash.org/graphql</code></li> <li>API: Standard stash-box GraphQL</li> <li>Use: <code>database_builder.py</code> with env override</li> <li>Notes:</li> <li>PMV-focused content</li> <li>Good image coverage (100% have images)</li> <li>34% have multiple images (avg 3.7)</li> <li>Portrait images (1280x1920)</li> </ul>"},{"location":"stash-box-endpoints/#javstash","title":"JAVStash","text":"<ul> <li>URL: <code>https://javstash.org/graphql</code></li> <li>API: Standard stash-box GraphQL</li> <li>Use: <code>database_builder.py</code> with env override</li> <li>Notes:</li> <li>JAV-focused content with Japanese names</li> <li>Limited to ~1 image per performer (see below)</li> <li>Square images (705x705)</li> </ul>"},{"location":"stash-box-endpoints/#running-builds","title":"Running Builds","text":"<pre><code># These commands apply to the stash-sense-trainer repo\ncd /home/carrot/code/stash-sense-trainer/api\nsource ../.venv/bin/activate\n\n# StashDB (default)\npython database_builder.py --output ./data --resume\n\n# PMVStash\nSTASHDB_URL=$PMVSTASH_URL STASHDB_API_KEY=$PMVSTASH_API_KEY \\\n  python database_builder.py --output ./data-pmvstash --resume\n\n# JAVStash\nSTASHDB_URL=$JAVSTASH_URL STASHDB_API_KEY=$JAVSTASH_API_KEY \\\n  python database_builder.py --output ./data-javstash --resume\n\n# ThePornDB (different script)\npython build_theporndb.py --output ./data-theporndb\n</code></pre>"},{"location":"stash-box-endpoints/#javstash-single-image-limitation","title":"JAVStash: Single Image Limitation","text":"<p>Problem: JAVStash performers typically have only 1 image, which limits face recognition accuracy. Our standard approach of building multiple embeddings per performer (target: 5) doesn't work here.</p>"},{"location":"stash-box-endpoints/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Single embedding = higher false positive/negative rates</li> <li>No redundancy for poor-quality source images</li> <li>Can't leverage voting across multiple embeddings</li> </ul>"},{"location":"stash-box-endpoints/#potential-solutions-todo-discuss","title":"Potential Solutions (TODO: Discuss)","text":"<ol> <li>Scene Frame Extraction</li> <li>JAVStash has scene data with fingerprints</li> <li>Could extract frames from matched scenes in user's library</li> <li>Build embeddings from actual video content</li> <li> <p>Requires: scene matching pipeline, user's local content</p> </li> <li> <p>External Image Sources</p> </li> <li>Cross-reference with other databases (e.g., JAV actress databases)</li> <li>Scrape additional images from linked URLs in performer metadata</li> <li> <p>Ethical/legal considerations for scraping</p> </li> <li> <p>Data Augmentation</p> </li> <li>Generate synthetic variations of single image</li> <li>Horizontal flip, slight rotations, color adjustments</li> <li> <p>Risk: may not improve real-world accuracy</p> </li> <li> <p>Lower Confidence Threshold</p> </li> <li>Accept that JAV matching will be lower confidence</li> <li>Use separate threshold for JAV vs other sources</li> <li> <p>Let users decide on match acceptance</p> </li> <li> <p>Ensemble with ThePornDB</p> </li> <li>Some JAV content exists on ThePornDB</li> <li>Cross-reference and merge embeddings where possible</li> <li> <p>ThePornDB has StashDB links that might map to JAVStash</p> </li> <li> <p>User-Contributed Images</p> </li> <li>Allow users to contribute additional performer images</li> <li>Build community-sourced image database</li> <li>Privacy/consent considerations</li> </ol>"},{"location":"stash-box-endpoints/#recommended-approach","title":"Recommended Approach","text":"<p>Short-term: Options 1 (scene frames) and 4 (adjusted thresholds): - Don't require external data sources - Leverage data users already have - Can be implemented incrementally</p> <p>Long-term: Option 7 - Crowd-Sourced Cloud Service</p> <p>The ultimate solution is a public cloud service (similar to stash-box) where users can submit face embeddings extracted from their identified scenes. This would: - Bypass the single-image limitation entirely - Build models from real scene content, not just promo images - Scale with the community (more users = better models) - Preserve privacy (only embeddings, never raw images)</p>"},{"location":"stash-box-endpoints/#rate-limit-reference","title":"Rate Limit Reference","text":"<p>Tested January 2026:</p> Endpoint Tested Rate Result StashDB 240/min \u2705 No issues (running 24h+) ThePornDB 240/min \u2705 Safe (limit ~275/min) ThePornDB 360/min \u274c 429s at ~275 requests PMVStash 300/min \u2705 No issues JAVStash 300/min \u2705 No issues (slower response) <p>Recommendation: Use 240/min (0.25s delay) for all endpoints as a safe default.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#container-issues","title":"Container Issues","text":""},{"location":"troubleshooting/#container-wont-start","title":"Container won't start","text":"<p>Check logs: <pre><code>docker logs stash-sense\n</code></pre></p> <p>Common causes:</p> <ul> <li>Missing database files - ensure <code>/data</code> contains the <code>.voy</code> files</li> <li>GPU driver issues - try without GPU first to isolate</li> <li>Port conflict - change from 5000 to another port</li> </ul>"},{"location":"troubleshooting/#database-not-loaded","title":"\"Database not loaded\"","text":"<p>The container started but can't find database files.</p> <pre><code># Check what's in the data volume\ndocker exec stash-sense ls -la /data\n</code></pre> <p>Should show: <pre><code>face_facenet.voy\nface_arcface.voy\nperformers.json\nmanifest.json\n</code></pre></p> <p>If empty, the volume mount is misconfigured or database wasn't extracted.</p>"},{"location":"troubleshooting/#health-check-failing","title":"Health check failing","text":"<pre><code># Test manually\ncurl http://localhost:5000/health\n\n# Check container status\ndocker ps -a | grep stash-sense\n</code></pre> <p>If container is restarting, check logs for the error.</p>"},{"location":"troubleshooting/#gpu-issues","title":"GPU Issues","text":""},{"location":"troubleshooting/#cuda-not-available","title":"\"CUDA not available\"","text":"<p>The container fell back to CPU mode.</p> <p>Verify GPU is accessible: <pre><code>docker exec stash-sense nvidia-smi\n</code></pre></p> <p>If this fails:</p> <ol> <li>Ensure nvidia-container-toolkit is installed</li> <li>Container needs <code>--runtime=nvidia --gpus all</code></li> <li>Restart Docker daemon after toolkit install</li> </ol>"},{"location":"troubleshooting/#cuda-out-of-memory","title":"\"CUDA out of memory\"","text":"<p>GPU memory exhausted. Check what's using it:</p> <pre><code>nvidia-smi\n</code></pre> <p>Solutions: - Stop other GPU-using containers temporarily - Use CPU mode (slower but works)</p>"},{"location":"troubleshooting/#slow-performance-on-gpu","title":"Slow performance on GPU","text":"<p>If GPU is detected but still slow:</p> <ol> <li>Check GPU utilization during requests: <code>watch nvidia-smi</code></li> <li>Ensure models aren't being re-downloaded (mount model cache volume)</li> <li>First request is slow (model loading) - subsequent requests should be faster</li> </ol>"},{"location":"troubleshooting/#connection-issues","title":"Connection Issues","text":""},{"location":"troubleshooting/#plugin-cant-reach-sidecar","title":"Plugin can't reach sidecar","text":"<p>From Stash container, test connectivity: <pre><code>docker exec stash curl http://stash-sense:5000/health\n</code></pre></p> <p>Common fixes:</p> <ul> <li>Use container name if on same Docker network</li> <li>Use host IP if on different networks</li> <li>Check firewall isn't blocking port 5000</li> </ul>"},{"location":"troubleshooting/#sidecar-cant-reach-stash","title":"Sidecar can't reach Stash","text":"<p>Test from sidecar: <pre><code>docker exec stash-sense curl http://stash:9999/graphql\n</code></pre></p> <p>Common fixes:</p> <ul> <li>Verify STASH_URL is correct</li> <li>Verify STASH_API_KEY is valid</li> <li>Check Stash is running and accessible</li> </ul>"},{"location":"troubleshooting/#recognition-issues","title":"Recognition Issues","text":""},{"location":"troubleshooting/#no-faces-detected","title":"No faces detected","text":"<ul> <li>Low quality sprites: Regenerate with higher quality settings</li> <li>No clear face shots: Some scenes don't have good face visibility</li> <li>Very small faces: Detection has minimum size threshold</li> </ul>"},{"location":"troubleshooting/#wrong-matches","title":"Wrong matches","text":"<ul> <li>Low confidence score: Scores &gt; 0.6 are less reliable</li> <li>Similar looking performers: Some faces are genuinely similar</li> <li>Database coverage: Performer may not be in StashDB or have poor reference images</li> </ul>"},{"location":"troubleshooting/#performer-matched-but-not-in-library","title":"Performer matched but not in library","text":"<p>This is expected behavior - the face matched someone in StashDB who you haven't added to your Stash yet. Options:</p> <ol> <li>Click \"View on StashDB\" to verify the match</li> <li>Add the performer to your Stash manually</li> <li>Use Stash's \"Identify\" feature to import from StashDB</li> </ol>"},{"location":"troubleshooting/#database-issues","title":"Database Issues","text":""},{"location":"troubleshooting/#corrupt-database-files","title":"Corrupt database files","text":"<p>If you see index errors or crashes on load:</p> <pre><code># Remove and re-download\nrm -rf /path/to/data/*\n# Re-extract from release\ntar -xzf stash-sense-db.tar.gz -C /path/to/data/\n</code></pre>"},{"location":"troubleshooting/#outdated-database","title":"Outdated database","text":"<p>Check current version: <pre><code>curl http://localhost:5000/database/info\n</code></pre></p> <p>Compare with latest release on GitHub. New databases include more performers and improved embeddings.</p>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":"<p>If you're stuck:</p> <ol> <li>Check container logs: <code>docker logs stash-sense</code></li> <li>Test health endpoint: <code>curl http://localhost:5000/health</code></li> <li>Verify database files exist and have correct permissions</li> <li>Open an issue on GitHub with:</li> <li>Container logs</li> <li>Health endpoint response</li> <li>Docker/Unraid version</li> <li>GPU model (if applicable)</li> </ol>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/","title":"Settings Page UI/UX Pass","text":"<p>Date: 2026-02-19 Ticket: refactor: settings page UI/UX pass - consistent components, human-friendly intervals</p>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#problem","title":"Problem","text":"<p>The plugin settings page has accumulated visual inconsistencies: - Job Schedules uses raw <code>&lt;input type=\"number\"&gt;</code> with browser-native spinners and inline styles - Interval values displayed as raw hours (168 = 1 week) \u2014 meaningless at a glance - Three different coding patterns across Sidecar Settings, Job Schedules, and Upstream Sync sections - Inline styles scattered throughout <code>stash-sense-settings.js</code> instead of CSS classes - Inconsistent save patterns: auto-save vs explicit Save button</p>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#design-decisions","title":"Design Decisions","text":"<ul> <li>Server-side interval definitions: <code>JobDefinition</code> gets <code>allowed_intervals</code> and <code>description</code> fields. The <code>queue_types</code> API serializes them to the frontend. Single source of truth.</li> <li>Tiered interval presets: Light/Network jobs get frequent options (6h\u20132w), heavy/slow-changing jobs get infrequent options (1d\u20131mo).</li> <li>Native <code>&lt;select&gt;</code> for interval picker: Styled to match dark theme. Accessible, zero JS overhead, options come from server.</li> <li>Auto-save everywhere: All settings auto-save with 500ms debounce, except upstream field config (batch checkbox operation keeps explicit Save).</li> <li>No new component abstractions: Use existing CSS classes consistently + add a few new ones. No UI library needed.</li> <li>Defaults unchanged: 24h for DB update + upstream, 168h for duplicates + fingerprints.</li> </ul>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#server-changes","title":"Server Changes","text":""},{"location":"plans/2026-02-19-settings-ui-ux-pass/#job_modelspy","title":"job_models.py","text":"<p>Add predefined interval tiers:</p> <pre><code>INTERVALS_FREQUENT = [\n    (6, \"Every 6 hours\"), (12, \"Every 12 hours\"),\n    (24, \"Every day\"), (48, \"Every 2 days\"), (72, \"Every 3 days\"),\n    (168, \"Every week\"), (336, \"Every 2 weeks\"),\n]\n\nINTERVALS_INFREQUENT = [\n    (24, \"Every day\"), (48, \"Every 2 days\"), (72, \"Every 3 days\"),\n    (168, \"Every week\"), (336, \"Every 2 weeks\"), (720, \"Every month\"),\n]\n</code></pre> <p>Extend <code>JobDefinition</code>: - <code>description: str</code> \u2014 human-readable description of what the job does - <code>allowed_intervals: list[tuple[int, str]]</code> \u2014 valid interval options for this job type</p> <p>Job descriptions: - Database Update: \"Checks for updated face recognition data\" - Upstream Performer Changes: \"Detects field changes from stash-box sources\" - Duplicate Performer Detection: \"Finds performers that may be duplicates\" - Duplicate Scene File Detection: \"Finds scenes with identical file fingerprints\" - Duplicate Scene Detection: \"Finds scenes that may be the same content\" - Fingerprint Generation: \"Generates face recognition fingerprints for scenes\"</p> <p>Interval assignments: - <code>INTERVALS_FREQUENT</code>: Database Update, Upstream Performer Changes - <code>INTERVALS_INFREQUENT</code>: All three duplicate detectors, Fingerprint Generation</p>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#queue_routerpy","title":"queue_router.py","text":"<p>Serialize new fields in <code>queue_types</code> response: <pre><code>{\n  \"type_id\": \"database_update\",\n  \"display_name\": \"Database Update\",\n  \"description\": \"Checks for updated face recognition data\",\n  \"allowed_intervals\": [\n    {\"hours\": 6, \"label\": \"Every 6 hours\"},\n    {\"hours\": 12, \"label\": \"Every 12 hours\"},\n    ...\n  ]\n}\n</code></pre></p>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#frontend-changes","title":"Frontend Changes","text":""},{"location":"plans/2026-02-19-settings-ui-ux-pass/#new-css-classes-stash-sensecss","title":"New CSS classes (stash-sense.css)","text":"<ul> <li><code>.ss-select</code> \u2014 styled native <code>&lt;select&gt;</code> matching dark theme (same border/bg/font as <code>.ss-number-input input</code>)</li> <li><code>.ss-setting-hint</code> \u2014 13px secondary-color text (replaces 4+ inline style occurrences)</li> <li><code>.ss-setting-row-vertical</code> \u2014 variant of <code>.ss-setting-row</code> with <code>flex-direction: column</code></li> <li><code>.ss-setting-row-header</code> \u2014 flex row with space-between for compound row headers</li> <li><code>.ss-upstream-fields-wrapper</code> \u2014 wrapper for expandable field config area</li> <li>Fix <code>.ss-number-input</code> spinner rules: change from <code>opacity: 1</code> to <code>display: none</code> / <code>-webkit-appearance: none</code></li> </ul>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#schedule-row-layout","title":"Schedule row layout","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Database Update                    [toggle]  [Every day \u25be] \u2502\n\u2502  Checks for updated face recognition data                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>Description is static (what the job does), not echoing the interval</li> <li><code>&lt;select&gt;</code> disabled when toggle is off</li> <li>Auto-saves on toggle or dropdown change via <code>debouncedSave</code> pattern</li> </ul>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#renderschedulescategory-refactor","title":"renderSchedulesCategory() refactor","text":"<ul> <li>Replace all inline <code>styles: {}</code> with CSS classes</li> <li>Replace <code>&lt;input type=\"number\"&gt;</code> with <code>&lt;select&gt;</code> populated from <code>allowed_intervals</code></li> <li>Remove per-row Save button, use debounced auto-save</li> <li>Use <code>.ss-setting-row</code>, <code>.ss-setting-info</code>, <code>.ss-setting-control</code> consistently</li> </ul>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#renderendpointfieldconfig-cleanup","title":"renderEndpointFieldConfig() cleanup","text":"<ul> <li>Inline style on row wrapper \u2192 <code>.ss-setting-row .ss-setting-row-vertical</code></li> <li>Inline flex header \u2192 <code>.ss-setting-row-header</code></li> <li>\"Show Fields\" button \u2192 <code>.ss-btn .ss-btn-sm</code></li> <li>Fields wrapper \u2192 <code>.ss-upstream-fields-wrapper</code></li> <li>Loading/help text \u2192 <code>.ss-setting-hint</code></li> <li>Keep explicit Save for batch field config (not auto-save)</li> <li>No behavioral changes</li> </ul>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#other-inline-style-cleanup","title":"Other inline style cleanup","text":"<ul> <li>Description/help text throughout \u2192 <code>.ss-setting-hint</code></li> <li>Remove all remaining <code>style:</code> attributes in <code>stash-sense-settings.js</code></li> </ul>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#not-in-scope","title":"Not in Scope","text":"<ul> <li>StashBox provider cards at top of settings page</li> <li>Operations tab styling</li> <li>API behavior changes beyond serializing new metadata fields</li> </ul>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#testing","title":"Testing","text":"<ul> <li>Unit: <code>JobDefinition</code> serialization includes <code>description</code> and <code>allowed_intervals</code> in <code>queue_types</code> response</li> <li>Unit: Schedule update API accepts hour values from predefined lists</li> <li>Manual: Visual verification \u2014 no inline styles, consistent alignment, dropdown works, auto-save works</li> <li>Manual: Responsive layout at 768px breakpoint</li> <li>Manual: Toggle disables/enables dropdown</li> </ul>"},{"location":"plans/2026-02-19-settings-ui-ux-pass/#key-files","title":"Key Files","text":"<ul> <li><code>api/job_models.py</code> \u2014 <code>JobDefinition</code> extensions, interval tier constants</li> <li><code>api/queue_router.py</code> \u2014 serialize new fields</li> <li><code>plugin/stash-sense-settings.js</code> \u2014 full refactor of schedule + upstream sync rendering</li> <li><code>plugin/stash-sense.css</code> \u2014 new classes, spinner fix</li> </ul>"},{"location":"research/face-recognition-quality-investigation/","title":"Face Recognition Quality Investigation","text":""},{"location":"research/face-recognition-quality-investigation/#date-2026-02-09","title":"Date: 2026-02-09","text":""},{"location":"research/face-recognition-quality-investigation/#implementation-status-complete-pending-database-rebuild","title":"Implementation Status: COMPLETE - Pending Database Rebuild","text":"<p>All critical fixes have been implemented in both the trainer and sidecar:</p> <ol> <li>Face alignment - Added InsightFace <code>norm_crop</code> 5-point similarity transform (trainer: <code>575525e</code>, sidecar: <code>66d9ef8</code>)</li> <li>ArcFace normalization - Fixed from <code>x/255</code> to <code>(x-127.5)/128</code> (trainer: <code>ce06344</code>, sidecar: <code>66d9ef8</code>)</li> <li>Flip-averaging - Added horizontal flip embedding averaging (trainer: <code>ce06344</code>, sidecar: <code>66d9ef8</code>)</li> <li>Aspect-ratio resize - Replaced stretching with aspect-ratio-preserving resize + black padding (trainer: <code>ce06344</code>, sidecar: <code>66d9ef8</code>)</li> <li>Manifest updated - Reflects actual pipeline models (trainer: <code>2975e31</code>)</li> <li>Nuke script - <code>python -m api.nuke_enrichment_data --yes</code> to clear all enrichment data (trainer: <code>259ebd3</code>)</li> </ol> <p>Next step: Run nuke script, then re-scrape with high-trust sources first (stashdb, theporndb), followed by reference sites and medium-trust sources. See plan's \"Post-Implementation: Database Rebuild Instructions\" section.</p>"},{"location":"research/face-recognition-quality-investigation/#problem-statement","title":"Problem Statement","text":"<p>Face recognition results are extremely poor. The system fails to correctly identify performers even in controlled conditions (still photos from galleries) where identification should be trivial. Example: Gallery 2139 (Holly Randall shoot featuring Jayden Jaymes) - the system thinks there are multiple different people and doesn't rank Jayden Jaymes highly, despite her having multiple face embeddings in the database.</p>"},{"location":"research/face-recognition-quality-investigation/#investigation-summary","title":"Investigation Summary","text":"<p>Root cause analysis traced the problem to two critical bugs in <code>embeddings.py</code> (shared between trainer and sidecar), plus several missing best practices that competitive systems implement.</p>"},{"location":"research/face-recognition-quality-investigation/#bug-1-no-face-alignment-primary-70-of-quality-loss","title":"Bug 1: No Face Alignment (PRIMARY - ~70% of quality loss)","text":""},{"location":"research/face-recognition-quality-investigation/#whats-happening","title":"What's happening","text":"<p>In both <code>stash-sense-trainer/api/embeddings.py</code> and <code>stash-sense/api/embeddings.py</code>, the <code>detect_faces()</code> method crops faces using a raw bounding box with zero alignment:</p> <pre><code># embeddings.py line 150 (trainer), line 150 (sidecar)\nface_img = image[y1:y2, x1:x2]  # Raw bounding box crop\n</code></pre>"},{"location":"research/face-recognition-quality-investigation/#what-should-happen","title":"What should happen","text":"<p>FaceNet512 and ArcFace were trained on aligned faces. Face alignment is a standard preprocessing step in face recognition where detected faces are rotated so that the eyes are horizontally level before cropping. This normalizes head pose and is critical for embedding consistency.</p> <p>DeepFace's own <code>extract_faces()</code> function does this by default (<code>align=True</code>):</p> <ol> <li>Detect eye landmarks</li> <li>Calculate rotation angle: <code>angle = degrees(arctan2(left_eye_y - right_eye_y, left_eye_x - right_eye_x))</code></li> <li>Apply 2D affine rotation via <code>cv2.warpAffine()</code> to make eyes horizontal</li> <li>Crop the aligned face</li> </ol> <p>Our code already has the landmark data - InsightFace's RetinaFace provides 5-point facial landmarks (<code>face.kps</code> containing left_eye, right_eye, nose, mouth_left, mouth_right). We store them in <code>DetectedFace.landmarks</code> but never use them for alignment.</p>"},{"location":"research/face-recognition-quality-investigation/#why-this-destroys-quality","title":"Why this destroys quality","text":"<p>Without alignment, a 15-degree head tilt produces embeddings that are significantly different from the same person with a level head. The models never learned to be invariant to rotation because their training data was always pre-aligned. This means:</p> <ul> <li>Different photos of the same person at different head angles produce widely scattered embeddings</li> <li>The \"centroid\" of a performer's embedding cluster is noisy and unstable</li> <li>Query-time embeddings from natural photos rarely land near the correct cluster</li> </ul>"},{"location":"research/face-recognition-quality-investigation/#evidence","title":"Evidence","text":"<p>This is a well-established requirement in face recognition literature. Every competitive system does alignment: - StashFace (cc1234): Uses DeepFace's <code>extract_faces()</code> which has <code>align=True</code> by default - FaceStash (kozobot): Uses the <code>face_recognition</code> library which internally uses dlib's alignment - DeepFace itself: Alignment is ON by default in all pipelines - InsightFace's own recognition pipeline: Uses landmark-based alignment before embedding</p>"},{"location":"research/face-recognition-quality-investigation/#bug-2-wrong-arcface-normalization-secondary-30-of-quality-loss","title":"Bug 2: Wrong ArcFace Normalization (SECONDARY - ~30% of quality loss)","text":""},{"location":"research/face-recognition-quality-investigation/#whats-happening_1","title":"What's happening","text":"<pre><code># embeddings.py line 206 (trainer), line 206 (sidecar)\narcface_input = arcface_input / 255.0  # Produces [0, 1] range - WRONG\n</code></pre>"},{"location":"research/face-recognition-quality-investigation/#what-should-happen_1","title":"What should happen","text":"<p>Per the ArcFace paper, DeepFace's own preprocessing code (<code>preprocessing.py:66-71</code>), and StashFace's implementation:</p> <pre><code>arcface_input = (arcface_input - 127.5) / 128  # Produces ~[-1, 1] range - CORRECT\n</code></pre> <p>The ArcFace model expects input pixels normalized to approximately <code>[-0.996, 0.996]</code> via <code>(x - 127.5) / 128</code>. Feeding <code>[0, 1]</code> range input shifts the entire activation distribution. The model still produces embeddings, but they're degraded - lower discriminative power.</p>"},{"location":"research/face-recognition-quality-investigation/#evidence_1","title":"Evidence","text":"<ul> <li>DeepFace source <code>preprocessing.py</code> line 66-71: <code>img -= 127.5; img /= 128</code> for ArcFace normalization</li> <li>ArcFace paper: \"each pixel in RGB images is normalised by subtracting 127.5 then divided by 128\"</li> <li>StashFace uses <code>preprocessing.normalize_input(resized, \"ArcFace\")</code> which applies this correct normalization</li> </ul>"},{"location":"research/face-recognition-quality-investigation/#additional-facenet-normalization-concern","title":"Additional FaceNet normalization concern","text":"<p>Our FaceNet normalization: <pre><code>facenet_input = (facenet_input - 127.5) / 127.5  # Our code\n</code></pre></p> <p>StashFace uses <code>\"Facenet2018\"</code> normalization which is: <pre><code>img /= 127.5  # Then img -= 1\n# Equivalent to: (img - 127.5) / 127.5\n</code></pre></p> <p>These are mathematically equivalent, so our FaceNet normalization is correct.</p>"},{"location":"research/face-recognition-quality-investigation/#additional-issues-found","title":"Additional Issues Found","text":""},{"location":"research/face-recognition-quality-investigation/#issue-3-no-aspect-ratio-preserving-resize","title":"Issue 3: No Aspect-Ratio-Preserving Resize","text":"<p>Our code resizes faces by stretching to the target size:</p> <pre><code>pil_img = pil_img.resize(target_size, Image.Resampling.BILINEAR)  # Non-preserving stretch\n</code></pre> <p>DeepFace and StashFace use aspect-ratio-preserving resize with black padding:</p> <pre><code># DeepFace preprocessing.resize_image()\nfactor = min(target_h / img_h, target_w / img_w)\nresized = cv2.resize(img, (int(img_w * factor), int(img_h * factor)))\n# Then pad with black to reach target_size\n</code></pre> <p>Impact: Moderate. Stretching distorts facial proportions slightly, adding noise to embeddings.</p>"},{"location":"research/face-recognition-quality-investigation/#issue-4-no-horizontal-flip-augmentation-at-query-time","title":"Issue 4: No Horizontal Flip Augmentation at Query Time","text":"<p>StashFace's key technique: For every detected face, it generates embeddings for BOTH the original and a horizontally flipped version, then averages them:</p> <pre><code>face_batch = np.stack([face, face[:, ::-1, :]], axis=0)  # Original + flipped\nembeddings_batch = ensemble.get_face_embeddings_batch(face_batch)\nfacenet = np.mean(embeddings_batch['facenet'], axis=0)  # Average\narc = np.mean(embeddings_batch['arc'], axis=0)           # Average\n</code></pre> <p>Why this helps: Faces are roughly symmetric. Averaging original + mirror embeddings: - Cancels out left-right asymmetries from lighting, expression, and slight pose - Produces a more \"canonical\" embedding closer to the center of the person's embedding cluster - Reduces the impact of minor alignment imperfections - Is essentially a free accuracy boost at the cost of 2x inference time</p>"},{"location":"research/face-recognition-quality-investigation/#issue-5-e4m3-quantized-storage-stashface-optimization","title":"Issue 5: E4M3 Quantized Storage (StashFace optimization)","text":"<p>StashFace uses <code>StorageDataType.E4M3</code> (8-bit float) for Voyager indices, reducing memory by 4x with minimal accuracy loss for cosine similarity. We use full 32-bit float. This is an optimization, not a bug - but worth considering for a 553K+ face database.</p>"},{"location":"research/face-recognition-quality-investigation/#issue-6-manifest-inconsistency","title":"Issue 6: Manifest Inconsistency","text":"<p>The manifest says <code>\"detector\": \"yolov8\"</code> but the code uses <code>buffalo_sc</code> (InsightFace RetinaFace). Either the manifest is stale from a prior version, or a different detector was used for the current DB build. This should be clarified and fixed during the rebuild.</p>"},{"location":"research/face-recognition-quality-investigation/#competitive-analysis-stashface-cc1234","title":"Competitive Analysis: StashFace (cc1234)","text":"<p>StashFace is the primary competitive reference. Here's what it does right that we should match or exceed:</p> Feature StashFace Our System Gap Face alignment DeepFace <code>extract_faces(align=True)</code> None (raw crop) CRITICAL ArcFace normalization <code>(x-127.5)/128</code> via DeepFace <code>x/255</code> CRITICAL FaceNet normalization <code>\"Facenet2018\"</code> via DeepFace <code>(x-127.5)/127.5</code> OK (equivalent) Horizontal flip averaging Yes (original + flipped, averaged) No HIGH Aspect-ratio resize Yes (DeepFace <code>resize_image</code>) No (stretch) Medium Index space Cosine Cosine OK Index storage E4M3 (8-bit) Float32 Low (optimization) Embedding dimensions 512 (both models) 512 (both models) OK Model weights Equal (1.0 each) FaceNet 0.6, ArcFace 0.4 Tuning difference Face detection YOLOv8 + MediaPipe InsightFace RetinaFace Different but OK Database size ~130K performers ~70K performers Different scope Confidence scoring Softmax with temperature + boost Linear (1 - distance) Different approach"},{"location":"research/face-recognition-quality-investigation/#proposed-fix-plan","title":"Proposed Fix Plan","text":""},{"location":"research/face-recognition-quality-investigation/#phase-1-fix-embeddingspy-both-trainer-and-sidecar","title":"Phase 1: Fix embeddings.py (both trainer and sidecar)","text":""},{"location":"research/face-recognition-quality-investigation/#1a-add-face-alignment-using-insightface-landmarks","title":"1a. Add face alignment using InsightFace landmarks","text":"<p>Use the 5-point landmarks already available from RetinaFace to align faces before cropping. The alignment should:</p> <ol> <li>Get left_eye and right_eye coordinates from <code>face.kps[0]</code> and <code>face.kps[1]</code></li> <li>Calculate rotation angle to make eyes horizontal</li> <li>Apply affine rotation via <code>cv2.warpAffine()</code></li> <li>Crop the aligned face</li> </ol> <p>This approach is simpler and faster than DeepFace's alignment because we already have reliable landmarks from RetinaFace (no need for a separate eye detector).</p> <p>For even better alignment, consider using InsightFace's built-in <code>norm_crop()</code> which does a 5-point affine alignment (not just rotation but also scaling and translation to place landmarks at standard positions). This is what InsightFace's own ArcFace model was trained with and would produce the best-aligned faces.</p>"},{"location":"research/face-recognition-quality-investigation/#1b-fix-arcface-normalization","title":"1b. Fix ArcFace normalization","text":"<p>Change from <code>/ 255.0</code> to <code>(x - 127.5) / 128</code>.</p>"},{"location":"research/face-recognition-quality-investigation/#1c-add-aspect-ratio-preserving-resize","title":"1c. Add aspect-ratio-preserving resize","text":"<p>Use either DeepFace's <code>preprocessing.resize_image()</code> directly, or implement equivalent logic with OpenCV/PIL that resizes maintaining aspect ratio and pads with black.</p>"},{"location":"research/face-recognition-quality-investigation/#1d-add-horizontal-flip-averaging","title":"1d. Add horizontal flip averaging","text":"<p>Generate embeddings for both the original face and its horizontal mirror, then average both vectors. This provides a free accuracy boost.</p>"},{"location":"research/face-recognition-quality-investigation/#phase-2-rebuild-the-face-database","title":"Phase 2: Rebuild the face database","text":"<p>Since embeddings.py is shared between trainer and sidecar:</p> <ol> <li>Keep all performer metadata - names, aliases, stashbox IDs, countries, etc. are all correct</li> <li>Keep all face image URLs - we know which images to download for each performer</li> <li>Delete all face embeddings and Voyager index entries</li> <li>Run a new enrichment pass to re-detect faces, align them, and generate correct embeddings</li> <li>Export new faces.json, performers.json, and Voyager indices</li> </ol> <p>The trainer's face URLs are stored in the <code>faces</code> table's <code>image_url</code> column. We can reprocess these same URLs with the corrected pipeline rather than re-scraping from stashbox. This means: - No stashbox API rate limiting concerns - Much faster than original scraping (just downloading images and processing) - Same curated image set, just better embeddings</p>"},{"location":"research/face-recognition-quality-investigation/#phase-3-update-sidecar-embeddingspy","title":"Phase 3: Update sidecar embeddings.py","text":"<p>Copy the corrected <code>embeddings.py</code> from the trainer to the sidecar. Since they're supposed to be identical (the trainer file says \"this file is shared\"), this ensures query-time preprocessing matches DB build-time preprocessing exactly.</p>"},{"location":"research/face-recognition-quality-investigation/#phase-4-update-matchingscoring-optional-improvements","title":"Phase 4: Update matching/scoring (optional improvements)","text":"<p>After the critical fixes, consider: - Tuning fusion weights (StashFace uses equal 1.0/1.0 instead of our 0.6/0.4) - Implementing softmax-based confidence scoring instead of linear - Adding E4M3 quantized storage for memory efficiency - Re-evaluating health detection thresholds (may not trigger as often with correct embeddings)</p>"},{"location":"research/face-recognition-quality-investigation/#confidence-assessment","title":"Confidence Assessment","text":""},{"location":"research/face-recognition-quality-investigation/#will-this-fix-the-problem","title":"Will this fix the problem?","text":"<p>Very high confidence (95%+). The alignment issue alone accounts for most face recognition systems' accuracy. It's the single most important preprocessing step in face recognition, and we completely skip it. Every working system we examined does alignment.</p>"},{"location":"research/face-recognition-quality-investigation/#will-we-match-or-exceed-stashface","title":"Will we match or exceed StashFace?","text":"<p>Yes. After implementing all Phase 1 fixes, our system will use: - The same models (FaceNet512 + ArcFace) - The same distance metric (Cosine) - The same preprocessing (alignment + correct normalization + flip averaging) - A comparable database size (70K performers) - More sophisticated matching (dual-model health detection and adaptive fusion)</p> <p>Our health detection and adaptive fusion strategy is actually more sophisticated than StashFace's simple weighted voting, which should give us an edge in edge cases.</p>"},{"location":"research/face-recognition-quality-investigation/#what-about-insightfaces-built-in-arcface","title":"What about InsightFace's built-in ArcFace?","text":"<p>InsightFace ships with its own ArcFace recognition model (e.g., <code>w600k_r50</code> in the <code>buffalo_l</code> model pack). This is a different ArcFace implementation than DeepFace's, trained on a larger dataset (WebFace600K vs MS1M). We could potentially add it as a third signal or replace DeepFace's ArcFace entirely. However, this would be a larger change and the fixes above should already bring us to competitive quality.</p>"},{"location":"research/face-recognition-quality-investigation/#files-to-modify","title":"Files to Modify","text":""},{"location":"research/face-recognition-quality-investigation/#trainer-stash-sense-trainer","title":"Trainer (stash-sense-trainer)","text":"<ul> <li><code>api/embeddings.py</code> - Add alignment, fix normalization, add flip averaging</li> <li><code>api/face_processor.py</code> - May need updates for new preprocessing</li> <li><code>api/index_manager.py</code> - No changes needed (Cosine space is correct)</li> <li>Database: Delete face rows + re-populate via enrichment run</li> </ul>"},{"location":"research/face-recognition-quality-investigation/#sidecar-stash-sense","title":"Sidecar (stash-sense)","text":"<ul> <li><code>api/embeddings.py</code> - Mirror all changes from trainer</li> <li><code>api/matching.py</code> - Consider weight tuning after rebuild</li> <li><code>api/recognizer.py</code> - No structural changes needed</li> </ul>"},{"location":"research/face-recognition-quality-investigation/#appendix-how-stashfaces-pipeline-works-complete","title":"Appendix: How StashFace's Pipeline Works (Complete)","text":"<p>For reference, here's StashFace's complete pipeline as reverse-engineered from the source:</p> <pre><code>INPUT IMAGE\n    |\n[YOLOv8 Face Detection] or [MediaPipe Face Detection]\n    |\n[DeepFace extract_faces(align=True)]\n    |--- Eye landmark detection\n    |--- 2D affine rotation to level eyes\n    |--- Crop aligned face\n    |\n[For each detected face]\n    |\n    +-- [Create batch: original + horizontal flip]\n    |       face_batch = stack([face, face[:, ::-1, :]])\n    |\n    +-- [FaceNet512 Preprocessing]\n    |       resize_image(face, (160,160))  # aspect-ratio preserving + pad\n    |       normalize_input(img, \"Facenet2018\")  # (x/127.5) - 1\n    |       model(batch, training=False)\n    |\n    +-- [ArcFace Preprocessing]\n    |       resize_image(face, (112,112))  # aspect-ratio preserving + pad\n    |       normalize_input(img, \"ArcFace\")  # (x-127.5)/128\n    |       model(batch, training=False)\n    |\n    +-- [Average original + flipped embeddings per model]\n    |       facenet_emb = mean(facenet_batch, axis=0)\n    |       arcface_emb = mean(arcface_batch, axis=0)\n    |\n    +-- [Query Voyager Indices (Cosine, E4M3)]\n    |       facenet_results = index.query(facenet_emb, k=50)\n    |       arcface_results = index.query(arcface_emb, k=50)\n    |\n    +-- [Ensemble Fusion]\n    |       Weighted voting (1.0 each model)\n    |       Softmax confidence with temperature\n    |       Final score = normalized_votes * avg_confidence * boost\n    |\nOUTPUT: Ranked performer matches with confidence scores\n</code></pre>"},{"location":"research/reeval-2026-02-12/","title":"Face Recognition Re-evaluation Results (2026-02-12)","text":""},{"location":"research/reeval-2026-02-12/#context","title":"Context","text":"<p>New face recognition database imported from stash-sense-trainer with: - 107,759 performers, 277,097 faces from 5 sources (stashdb, fansdb, theporndb, pmvstash, javstash) - Proper face alignment (insightface_norm_crop_5point) - previously raw bbox crops - Correct ArcFace normalization ((x-127.5)/128) - previously /255 - Flip averaging for embeddings</p> <p>Previous testing (Feb 4-6) used garbage data and yielded ~44% accuracy, ~31% precision at best.</p>"},{"location":"research/reeval-2026-02-12/#test-setup","title":"Test Setup","text":"<ul> <li>20 stratified test scenes (15x 1080p, 5x 720p)</li> <li>48 expected performers across all scenes</li> <li>All scenes classified as \"sparse\" coverage tier</li> <li>Benchmark script: <code>api/benchmark/reeval.py</code></li> <li>Full JSON results: <code>api/benchmark_results/reeval_20260212_064023.json</code></li> <li>Runtime: 166 minutes</li> </ul>"},{"location":"research/reeval-2026-02-12/#phase-1-baseline-current-defaults","title":"Phase 1: Baseline (Current Defaults)","text":"Metric Value Accuracy 41.7% Precision 32.8% TP / Expected 20 / 48 False Positives 41 1080p accuracy 39.5% 720p accuracy 50.0% <p>Key finding: Accuracy is roughly the same as old DB (~44%). The alignment/normalization fixes didn't meaningfully improve scene identification. The bottleneck is elsewhere in the pipeline.</p>"},{"location":"research/reeval-2026-02-12/#phase-2-parameter-sweeps","title":"Phase 2: Parameter Sweeps","text":""},{"location":"research/reeval-2026-02-12/#max_distance-no-impact-above-05","title":"max_distance - No impact above 0.5","text":"max_distance Accuracy Precision TP FP FN 0.3 4.2% 100.0% 2 0 46 0.4 33.3% 43.2% 16 21 32 0.5 41.7% 32.8% 20 41 28 0.6 41.7% 32.8% 20 41 28 0.7 41.7% 32.8% 20 41 28 0.8 41.7% 32.8% 20 41 28 0.9 41.7% 32.8% 20 41 28 <p>Insight: Matches either land below 0.5 or don't land at all. Anything above 0.5 is noise. Can safely tighten to 0.5.</p>"},{"location":"research/reeval-2026-02-12/#min_face_size-no-impact-below-80","title":"min_face_size - No impact below 80","text":"min_face_size Accuracy Precision TP FP FN 30 41.7% 32.8% 20 41 28 40 41.7% 32.8% 20 41 28 60 41.7% 32.8% 20 41 28 80 37.5% 36.7% 18 31 30 100 29.2% 32.6% 14 29 34 <p>Insight: Below 80px there are no additional small faces being matched. Above 80 loses faces.</p>"},{"location":"research/reeval-2026-02-12/#fusion_weights-arcface-now-pulling-its-weight","title":"fusion_weights - ArcFace now pulling its weight","text":"Weights (fn/af) Accuracy Precision TP FP FN 0.7/0.3 39.6% 30.6% 19 43 29 0.6/0.4 (old default) 41.7% 32.8% 20 41 28 0.5/0.5 43.8% 33.9% 21 41 27 0.4/0.6 43.8% 32.8% 21 43 27 0.3/0.7 41.7% 31.2% 20 44 28 <p>Insight: Equal weighting is optimal. This confirms the ArcFace normalization fix is working - it's now a reliable signal. The old 0.6/0.4 bias toward FaceNet was compensating for broken ArcFace.</p>"},{"location":"research/reeval-2026-02-12/#matching_mode-frequency-wins","title":"matching_mode - Frequency wins","text":"Mode Accuracy Precision TP FP FN frequency 41.7% 32.8% 20 41 28 hybrid 39.6% 33.9% 19 37 29 <p>Insight: Hybrid has marginally better precision (fewer FPs) but loses 1 TP. Frequency is the better default.</p>"},{"location":"research/reeval-2026-02-12/#min_appearances-no-meaningful-impact","title":"min_appearances - No meaningful impact","text":"min_appearances Accuracy Precision TP FP FN 1 41.7% 32.8% 20 41 28 2 41.7% 32.8% 20 41 28 3 39.6% 47.5% 19 21 29 <p>Insight: 1\u21922 identical. 3 is a strong precision filter (47.5%) at slight accuracy cost. Could be useful in a \"high precision\" mode.</p>"},{"location":"research/reeval-2026-02-12/#min_unique_frames-the-biggest-lever-tied-with-num_frames","title":"min_unique_frames - THE biggest lever (tied with num_frames)","text":"min_unique_frames Accuracy Precision TP FP FN 1 60.4% 30.5% 29 66 19 2 (default) 41.7% 32.8% 20 41 28 3 31.2% 55.6% 15 12 33 <p>Insight: Dropping from 2\u21921 recovers 9 TPs (+45% relative) but adds 25 FPs. The unique frame requirement is the binding constraint for many performers who only appear clearly in one frame. Setting to 3 gives 55.6% precision but kills recall.</p>"},{"location":"research/reeval-2026-02-12/#min_confidence-zero-impact","title":"min_confidence - Zero impact","text":"min_confidence Accuracy Precision 0.2 41.7% 32.8% 0.3 41.7% 32.8% 0.35 41.7% 32.8% 0.4 41.7% 32.8% 0.5 41.7% 32.8% <p>Insight: The confidence threshold is never the binding constraint. All matches that pass the distance filter also pass any confidence threshold we tested.</p>"},{"location":"research/reeval-2026-02-12/#num_frames-strong-positive-correlation-with-accuracy","title":"num_frames - Strong positive correlation with accuracy","text":"num_frames Accuracy Precision TP FP FN 20 35.4% 44.7% 17 21 31 40 (default) 41.7% 32.8% 20 41 28 60 54.2% 36.1% 26 46 22 80 60.4% 30.5% 29 66 19 <p>Insight: Clear linear relationship. Each 20 extra frames adds ~6% accuracy. 60 is the best accuracy/precision/runtime balance: +12.5% accuracy over baseline, precision actually improves to 36.1%, and only 50% more runtime than 40 frames. 80 frames has diminishing returns and drops precision.</p>"},{"location":"research/reeval-2026-02-12/#phase-3-best-of-combination","title":"Phase 3: Best-of Combination","text":"<p>Combined best from each sweep: <pre><code>max_distance=0.5, min_face_size=30, fn/af=0.5/0.5,\nmatching_mode=frequency, min_appearances=1, min_unique_frames=1,\nmin_confidence=0.2, num_frames=80\n</code></pre></p> Config Accuracy Precision TP FP FN Baseline 41.7% 32.8% 20 41 28 Best combo 62.5% 30.0% 30 70 18 Delta +20.8% -2.8% +10 +29 -10 <p>The gains come primarily from more frames + dropping min_unique_frames to 1.</p>"},{"location":"research/reeval-2026-02-12/#phase-4-original-scene-re-validation-8-scenes","title":"Phase 4: Original Scene Re-validation (8 scenes)","text":"Config Accuracy Precision TP FP FN Default params 50.0% 26.7% 8 22 8 Best params 56.2% 22.5% 9 31 7 <p>Per-scene: - Scene 13938 (1080p): 2/2 both configs - Scene 16342 (1080p): 2/2 both configs - Scene 30835 (1080p): 0/2 both configs - stubborn failure - Scene 3283 (720p): 0/2 \u2192 1/2 (best params recovered one) - Others: 1/2 both configs</p>"},{"location":"research/reeval-2026-02-12/#phase-5-gallery-benchmark","title":"Phase 5: Gallery Benchmark","text":"<p>10 galleries tested, 5 distance thresholds. Results were nearly identical across all thresholds:</p> max_distance Accuracy Precision TP FP FN All (0.4-0.8) 78.3% 3.6% 18 483-488 5 <p>Major problem: Gallery 1062 (1,074 images, 2 expected performers) generated 279-283 FPs alone. Excluding it, the other 9 galleries had ~204 FPs for 16 TPs (~7.3% precision).</p> <p>Root cause: The \"2+ appearances OR single &lt; 0.4\" aggregation filter is far too permissive for galleries. Each face in each image produces a top-1 match, and with hundreds of images, random performers easily get 2+ appearances.</p> <p>Suggested fixes for gallery aggregation: 1. Require appearances in minimum % of images (e.g., 10%) not just absolute count 2. Scale minimum appearances with gallery size (e.g., max(2, image_count * 0.05)) 3. Use stricter distance thresholds for gallery mode 4. Require the performer to be the top-1 match (not just any match) in multiple images</p>"},{"location":"research/reeval-2026-02-12/#phase-6-image-benchmark","title":"Phase 6: Image Benchmark","text":"<p>Skipped - no images in the library had tagged performers with StashDB IDs.</p>"},{"location":"research/reeval-2026-02-12/#applied-parameter-changes","title":"Applied Parameter Changes","text":"<p>Based on these findings, the following \"safe\" changes were applied:</p> Parameter Old Default New Default Rationale facenet_weight 0.6 0.5 ArcFace now working correctly, equal weighting optimal arcface_weight 0.4 0.5 Same max_distance 0.7 0.5 Results plateau at 0.5, tighter = less noise num_frames 40 60 +12.5% accuracy, best precision of elevated frame counts <p>Not changed (needs further investigation): - min_unique_frames: kept at 2 (dropping to 1 has huge FP cost) - min_appearances: kept at 2 - min_confidence: kept at 0.35 (no impact either way)</p>"},{"location":"research/reeval-2026-02-12/#post-benchmark-clustering-ui-improvements-same-day","title":"Post-Benchmark: Clustering &amp; UI Improvements (same day)","text":""},{"location":"research/reeval-2026-02-12/#problem-flat-frequency-matching-produces-too-many-persons","title":"Problem: Flat Frequency Matching Produces Too Many \"Persons\"","text":"<p>The frequency matching mode had no concept of face clustering - it threw all face detections into a flat bag and counted performer appearances. A 2-person scene with 38 face detections would produce 34 \"persons\" in the UI, one per unique performer match.</p>"},{"location":"research/reeval-2026-02-12/#root-cause-l2-distance-on-concatenated-embeddings","title":"Root Cause: L2 Distance on Concatenated Embeddings","text":"<p>The existing <code>cluster_faces_by_person</code> used L2 distance on concatenated 1024-dim (FaceNet+ArcFace) embeddings with threshold 0.6. In that high-dimensional space, same-person faces are typically 1.0+ apart in L2 distance, so the 0.6 threshold was far too tight and almost no clustering happened.</p>"},{"location":"research/reeval-2026-02-12/#fix-clustered-frequency-matching","title":"Fix: Clustered Frequency Matching","text":"<p>Implemented a two-stage approach (<code>clustered_frequency_matching</code>):</p> <ol> <li>Stage 1 - Clustering: Group faces by cosine similarity (consistent with Voyager indices) using pre-computed embeddings (avoids recomputing). Threshold 0.6 cosine distance works well.</li> <li>Stage 2 - Identification: Within each cluster, run frequency-style matching to identify who each person is. Alternatives shown per cluster.</li> </ol> <p>Also merged clusters with the same best match (existing <code>merge_clusters_by_match</code>).</p>"},{"location":"research/reeval-2026-02-12/#results","title":"Results","text":"Scene Faces Old Persons New Persons Correct Top Matches 709 38 34 9 (2 dominant: 22 + 5 frames) Rilynn Rae, Keiran Lee 14650 34 ~30 2 Ella Reese, Tim Gottfrid 19759 94 ~80 6 (4 dominant: 27+24+20+17 frames) Karlie Montana, Madison Ivy, Keiran Lee, Voodoo"},{"location":"research/reeval-2026-02-12/#additional-improvements","title":"Additional Improvements","text":"<ul> <li>Tagged-performer awareness: Plugin sends existing scene performer StashDB IDs to the API. A small confidence boost (+0.03) is applied to already-tagged performers, and <code>already_tagged</code> flag is returned for UI display.</li> <li>\"Already tagged\" UI state: Shows \"Already tagged on scene\" with green badge instead of \"Add to Scene\" button for performers already on the scene.</li> <li>Grouped UI: Multi-frame clusters shown prominently; single-frame detections collapsed in a collapsible section.</li> <li>Embedding reuse: <code>RecognitionResult</code> now stores the pre-computed embedding, eliminating redundant embedding generation during clustering.</li> </ul>"},{"location":"research/reeval-2026-02-12/#future-investigation-areas","title":"Future Investigation Areas","text":"<ol> <li> <p>Why is accuracy still ~42% with good embeddings? The database quality improvement didn't help scenes. Investigate whether the problem is face detection (faces not being detected), frame selection (wrong moments), or matching (wrong performers winning).</p> </li> <li> <p>Failure analysis on specific performers: Which of the 28 FN performers are being missed and why? Are they in the database? Are their faces being detected? Are they being matched to wrong performers?</p> </li> <li> <p>Scene 30835: Consistently 0/2 across all parameter configs. What's special about this scene?</p> </li> <li> <p>Gallery aggregation redesign: Current \"2+ appearances\" filter is broken for galleries. Need percentage-based or statistical approach.</p> </li> <li> <p>min_unique_frames=1 with compensating FP filter: Could we drop to 1 frame but add a stricter distance threshold (e.g., require distance &lt; 0.4 for single-frame matches)?</p> </li> <li> <p>Image benchmark: Need to tag some images with performers to enable Phase 6.</p> </li> <li> <p>Larger scene sample: 20 scenes with 48 performers is small. Results can shift by 1-2 TPs. Consider running with 50+ scenes for more statistical power.</p> </li> </ol>"},{"location":"unraid/gpu-passthrough/","title":"GPU Passthrough for Unraid","text":""},{"location":"unraid/gpu-passthrough/#prerequisites","title":"Prerequisites","text":"<ul> <li>NVIDIA GPU (GTX 10-series or newer recommended)</li> <li>Unraid 6.12+</li> </ul>"},{"location":"unraid/gpu-passthrough/#step-1-install-nvidia-driver-plugin","title":"Step 1: Install Nvidia Driver Plugin","text":"<ol> <li>Go to Apps \u2192 Search \"Nvidia Driver\"</li> <li>Install Nvidia-Driver by ich777</li> <li>Reboot Unraid</li> </ol>"},{"location":"unraid/gpu-passthrough/#step-2-verify-gpu-detection","title":"Step 2: Verify GPU Detection","text":"<p>Open Unraid terminal and run:</p> <pre><code>nvidia-smi\n</code></pre> <p>You should see your GPU listed:</p> <pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 550.xx       Driver Version: 550.xx       CUDA Version: 12.x     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:XX:00.0 Off |                  N/A |\n+-------------------------------+----------------------+----------------------+\n</code></pre>"},{"location":"unraid/gpu-passthrough/#step-3-container-configuration","title":"Step 3: Container Configuration","text":"<p>The Stash Sense template automatically includes GPU settings.</p> <p>If configuring manually, ensure these are set:</p> Setting Value Extra Parameters <code>--runtime=nvidia --gpus all</code> NVIDIA_VISIBLE_DEVICES <code>all</code> (or specific GPU UUID)"},{"location":"unraid/gpu-passthrough/#troubleshooting","title":"Troubleshooting","text":""},{"location":"unraid/gpu-passthrough/#nvidia-smi-command-not-found","title":"\"nvidia-smi: command not found\"","text":"<p>Driver plugin not installed or Unraid needs reboot.</p> <pre><code># Check if plugin is installed\nls /boot/config/plugins/nvidia-driver.plg\n# If missing, reinstall from Apps\n</code></pre>"},{"location":"unraid/gpu-passthrough/#no-gpu-detected-in-container","title":"\"No GPU detected in container\"","text":"<p>Check container has GPU access:</p> <pre><code>docker exec stash-sense nvidia-smi\n</code></pre> <p>If this fails, verify:</p> <ol> <li>Extra Parameters includes <code>--runtime=nvidia</code></li> <li>Container was recreated after driver install (not just restarted)</li> </ol>"},{"location":"unraid/gpu-passthrough/#cuda-out-of-memory","title":"\"CUDA out of memory\"","text":"<p>Another process is using the GPU. Check what's using it:</p> <pre><code>nvidia-smi\n</code></pre> <p>Look at \"Processes\" section. Common culprits: - Plex hardware transcoding - Tdarr - Frigate - Other ML containers</p>"},{"location":"unraid/gpu-passthrough/#failed-to-initialize-nvml","title":"\"Failed to initialize NVML\"","text":"<p>The Nvidia container toolkit isn't configured. Re-install the driver plugin and reboot.</p>"},{"location":"unraid/gpu-passthrough/#selecting-a-specific-gpu-multi-gpu-systems","title":"Selecting a Specific GPU (Multi-GPU Systems)","text":"<p>If you have multiple GPUs, specify which one to use:</p> <pre><code># List GPU UUIDs\nnvidia-smi -L\n\n# Use specific GPU\nNVIDIA_VISIBLE_DEVICES=GPU-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n</code></pre> <p>Or by index: <pre><code>NVIDIA_VISIBLE_DEVICES=0  # First GPU\nNVIDIA_VISIBLE_DEVICES=1  # Second GPU\n</code></pre></p>"},{"location":"unraid/setup/","title":"Unraid Setup","text":""},{"location":"unraid/setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Unraid 6.12+</li> <li>Nvidia Driver plugin installed (for GPU acceleration)</li> <li>Stash running on Unraid or accessible from Unraid</li> </ul>"},{"location":"unraid/setup/#option-1-xml-template-recommended","title":"Option 1: XML Template (Recommended)","text":""},{"location":"unraid/setup/#1-download-the-template","title":"1. Download the template","text":"<p>Copy stash-sense.xml to your Unraid flash drive:</p> <pre><code>/boot/config/plugins/dockerMan/templates-user/stash-sense.xml\n</code></pre>"},{"location":"unraid/setup/#2-add-container","title":"2. Add container","text":"<p>Go to Docker \u2192 Add Container \u2192 Template \u2192 User Templates \u2192 stash-sense</p>"},{"location":"unraid/setup/#3-configure","title":"3. Configure","text":"Setting Value Stash URL <code>http://stash:9999</code> or your Stash IP/hostname Stash API Key From Stash: Settings \u2192 Security \u2192 API Key Data Directory Path to extracted database files"},{"location":"unraid/setup/#4-apply","title":"4. Apply","text":"<p>Click Apply. The container will download and start.</p>"},{"location":"unraid/setup/#option-2-docker-compose","title":"Option 2: Docker Compose","text":"<p>If you prefer docker-compose on Unraid:</p>"},{"location":"unraid/setup/#1-create-directory","title":"1. Create directory","text":"<pre><code>mkdir -p /mnt/user/appdata/stash-sense\n</code></pre>"},{"location":"unraid/setup/#2-create-docker-composeyml","title":"2. Create docker-compose.yml","text":"<pre><code>version: \"3.8\"\n\nservices:\n  stash-sense:\n    image: ghcr.io/carrotwaxr/stash-sense:latest\n    container_name: stash-sense\n    runtime: nvidia\n    restart: unless-stopped\n    ports:\n      - \"5787:5000\"\n    environment:\n      - STASH_URL=http://stash:9999\n      - STASH_API_KEY=your-api-key\n      - NVIDIA_VISIBLE_DEVICES=all\n    volumes:\n      - /mnt/user/appdata/stash-sense/data:/data:ro\n      - /mnt/user/appdata/stash-sense/models:/root/.insightface\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n</code></pre>"},{"location":"unraid/setup/#3-start","title":"3. Start","text":"<pre><code>cd /mnt/user/appdata/stash-sense\ndocker compose up -d\n</code></pre>"},{"location":"unraid/setup/#database-setup","title":"Database Setup","text":"<p>Download the database release and extract:</p> <pre><code>cd /mnt/user/appdata/stash-sense/data\nwget https://github.com/carrotwaxr/stash-sense/releases/download/db-latest/stash-sense-db.tar.gz\ntar -xzf stash-sense-db.tar.gz\nrm stash-sense-db.tar.gz\n</code></pre> <p>You should have: <pre><code>data/\n\u251c\u2500\u2500 face_facenet.voy\n\u251c\u2500\u2500 face_arcface.voy\n\u251c\u2500\u2500 performers.json\n\u2514\u2500\u2500 manifest.json\n</code></pre></p>"},{"location":"unraid/setup/#networking","title":"Networking","text":"<p>If Stash runs on the same Unraid server, use Docker's internal networking:</p> <ul> <li>Stash URL: <code>http://stash:9999</code> (if Stash container is named \"stash\")</li> <li>Or use Unraid's IP: <code>http://192.168.1.x:9999</code></li> </ul>"},{"location":"unraid/setup/#next-steps","title":"Next Steps","text":"<ul> <li>GPU Passthrough - ensure GPU is working</li> <li>Plugin Setup - install the Stash plugin</li> <li>Troubleshooting - if something's not working</li> </ul>"}]}